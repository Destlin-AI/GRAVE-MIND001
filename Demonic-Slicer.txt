#!/usr/bin/env python3
import os
import sys
import json
import time
import torch
import numpy as np
import psutil
import logging
import threading
import queue
import mmap # Kept for future potential, not actively used in this version's core logic
import asyncio
import uvicorn
import concurrent.futures
import subprocess
import hashlib
import zstandard as zstd # For Zstandard compression
from typing import Dict, List, Optional, Tuple, Any, Union, Callable
from pathlib import Path

from fastapi import FastAPI, HTTPException, BackgroundTasks, Request, Security, Depends
from fastapi.security import APIKeyHeader
from pydantic import BaseModel, Field # For API request/response validation
import requests # For optional LM Studio forwarding and Hugging Face Hub interactions
import httpx # For async HTTP requests (e.g., LM Studio forwarding)
import io # For byte stream operations (e.g., saving/loading tensors)
import shutil # For directory operations (e.g., RAMDisk cleanup)
import gc # For explicit garbage collection calls

# Hugging Face Transformers imports for model loading and tokenization
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, PreTrainedModel, PreTrainedTokenizerBase
from huggingface_hub import snapshot_download, HfApi # For interacting with Hugging Face Hub

# --- Global Configuration & Constants ---
CONFIG_FILE_PATH = "tensor_config.json" # Default configuration file name
DEFAULT_MODEL_CACHE_DIR = "huggingface_models_cache" # Default local cache for Hugging Face models
DEFAULT_SERVER_LOG_FILE = "ultra_tensor_server.log" # Default log file name
MIN_NVME_STRIPE_SIZE_MB = 16 # MB, (Not fully implemented for I/O, but used in metadata)
MIN_RAMDISK_SIZE_MB = 256 # MB, Minimum RAMDisk size if enabled

# Configure Logging (Initial basic setup, will be enhanced after config is loaded)
# This setup ensures logs are available even if config loading fails.
logging.basicConfig(
    level=logging.INFO, # Default logging level
    format='%(asctime)s | %(levelname)-8s | %(name)s | %(threadName)s | %(module)s.%(funcName)s:%(lineno)d | %(message)s',
    handlers=[logging.StreamHandler()] # Log to console by default
)
logger = logging.getLogger("UltraTensorServer") # Main logger for the application

# --- Global State Variables (Initialized or populated during startup) ---
config: Dict[str, Any] = {} # Application configuration, loaded from JSON file
hardware_summary: Dict[str, Any] = { # Dynamically populated summary of detected hardware
    "gpu": {"available": False, "count": 0, "devices": []}, # GPU info
    "cpu": {}, # CPU info
    "ram": {}, # System RAM info
    "nvme": {"available": False, "path": None}, # NVMe storage info
    "ramdisk": {"available": False, "path": None}, # RAMDisk info
    "log_file_path": None # Actual path to the log file, set after config load
}
# Global Registries and Caches for Layer Management
layer_metadata_registry: Dict[int, 'LayerMetadata'] = {} # layer_id -> LayerMetadata instance
layer_cache: Dict[int, torch.Tensor] = {} # layer_id -> torch.Tensor (for layers in CPU/GPU RAM)
layer_access_stats: Dict[int, Dict[str, Any]] = {} # layer_id -> {"count": int, "last_access": float, "name": str}
layer_locks: Dict[int, threading.Lock] = {} # layer_id -> threading.Lock for concurrent access to layer data/metadata

# Queues for Asynchronous Operations
layer_transfer_queue = queue.PriorityQueue(maxsize=200) # For explicit layer move requests
prefetch_job_queue = queue.Queue(maxsize=100) # For prefetching hints (layer_id, model_context)

# Thread Pools
nvme_worker_pool: Optional[concurrent.futures.ThreadPoolExecutor] = None # For I/O bound NVMe operations

# Control Events and Timers
main_stop_event = threading.Event() # Signals all background threads to gracefully stop
server_start_time = time.monotonic() # For calculating server uptime

# Core Service Managers (Initialized in on_startup after config load)
hardware_mgr: Optional['HardwareManager'] = None
quant_manager: Optional['QuantizationManager'] = None
compress_manager: Optional['CompressionManager'] = None
nvme_mgr: Optional['NVMEManager'] = None
ramdisk_mgr: Optional['RAMDiskManager'] = None
mem_placement_mgr: Optional['MemoryPlacementManager'] = None
model_manager_service: Optional['ModelManagerService'] = None
inference_service: Optional['InferenceService'] = None
# API Key Authentication Dependency (initialized in on_startup if key is configured)
api_key_auth_check: Optional[Callable] = None


# --- Pydantic Models for API Request/Response Validation ---
class APIBaseModel(BaseModel): # Base model for consistent Pydantic settings
    class Config:
        from_attributes = True # Pydantic V2 setting (replaces orm_mode=True)

class LayerInfoAPI(APIBaseModel): # For exposing LayerMetadata via API
    id: int
    name: str
    original_size_mb: float
    current_size_mb: float
    current_device: str
    gpu_id: Optional[int] = None
    quantization_type: Optional[str] = None
    compression_type: Optional[str] = None
    access_count: int
    last_access_timestamp: float # UNIX timestamp

class TransferRequestAPI(APIBaseModel): # For API endpoint to manually queue transfers
    layer_id: int
    destination_device: str = Field(..., description="Target device: 'cpu', 'gpu:X', 'ramdisk', 'nvme'")
    priority: int = Field(default=5, ge=1, le=10, description="Transfer priority (1=highest, 10=lowest)")

class ModelLoadAPIRequest(APIBaseModel): # For /models/load endpoint
    model_name_or_path: str = Field(..., description="Hugging Face model ID or local path")
    compute_device: Optional[str] = Field(default=None, description="Preferred primary compute device (e.g., 'cuda:0', 'cpu'). Auto-detected if None.")
    hf_token: Optional[str] = Field(default=None, description="Hugging Face API token for private models.")

class ModelStatusAPIResponse(APIBaseModel): # For /models/status endpoint
    model_name: Optional[str] = None
    model_path: Optional[str] = None
    architecture: Optional[str] = None
    tokenizer_name: Optional[str] = None
    num_managed_layers: int = 0
    compute_device: Optional[str] = None
    status: str = Field(..., description="Current status: 'no_model_loaded', 'loading', 'loaded', 'unloading', 'error'")
    error_message: Optional[str] = None

class GenerateTextAPIRequest(APIBaseModel): # For /models/generate endpoint
    prompt: str
    max_new_tokens: int = Field(default=128, gt=0, le=8192, description="Max new tokens to generate")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="Sampling temperature (0.0 for deterministic)")
    top_p: float = Field(default=0.9, ge=0.0, le=1.0, description="Nucleus sampling top_p")
    top_k: Optional[int] = Field(default=None, ge=0, description="Top-k filtering (0 to disable)")

class GenerateTextAPIResponse(APIBaseModel): # Response for /models/generate
    prompt: str
    generated_text: str
    model_name: str
    tokens_generated: int
    generation_time_seconds: float

class ServerStatusAPIResponse(APIBaseModel): # For main /api/status endpoint
    server_status: str
    uptime_seconds: float
    model_status: ModelStatusAPIResponse # Nested model status
    hardware_summary: Dict[str, Any] # Contains the global hardware_summary dict
    layer_transfer_queue_size: int
    prefetch_job_queue_size: int
    active_threads: int
    log_file_path: Optional[str] = None # Path to the server log file


# --- Core Data Structures (LayerMetadata, TransferRequest, LoadedModelContext) ---
class LayerMetadata: # Detailed metadata for each model layer (parameter)
    def __init__(self, layer_id: int, name: str, original_size_bytes: int,
                 shape: Tuple[int, ...], dtype: torch.dtype, current_device: str, # Initial current_device
                 gpu_id: Optional[int] = None, compression_type: Optional[str] = None,
                 quantization_type: Optional[str] = None, nvme_path: Optional[str] = None,
                 ramdisk_path: Optional[str] = None, quantization_params: Optional[Dict] = None,
                 is_on_nvme_stripped: bool = False):
        self.id = layer_id
        self.name = name # Original parameter name from the model
        self.original_size_bytes = original_size_bytes # Size of layer in its original_dtype
        self.current_size_bytes = original_size_bytes # Actual size on disk/cache (after quant/compr)
        self.shape = shape # Tensor shape
        self.original_dtype = dtype # Dtype before any quantization (e.g., torch.float32)
        self.current_dtype = dtype # Dtype as currently stored/cached (e.g., torch.int8, torch.float16)
        self.current_device = current_device # "cpu", "gpu", "ramdisk", "nvme"
        self.gpu_id = gpu_id if current_device == "gpu" else None # Specific GPU ID

        self.compression_type = compression_type # E.g., "zstd"
        self.quantization_type = quantization_type # E.g., "int8", "int4", "fp16"
        self.quantization_params = quantization_params if quantization_params else {} # Stores scale, zero_point, original_shape for int4, etc.

        self.nvme_path: Optional[str] = nvme_path # Full path if stored on NVMe
        self.ramdisk_path: Optional[str] = ramdisk_path # Full path if stored on RAMDisk

        # Runtime Tracking
        self.access_count: int = 0
        self.last_access_timestamp: float = 0.0 # UNIX timestamp
        self.loaded_in_memory: bool = (current_device not in ["nvme"]) # True if in CPU/GPU RAM or RAMDisk cache

        # For NVMe striping (if implemented for I/O, metadata can be stored here)
        self.nvme_stripes: List[Dict[str, Any]] = [] # Metadata for each stripe (offset, size, file_path)
        self.is_on_nvme_stripped: bool = is_on_nvme_stripped # True if nvme_stripes metadata is valid

        if layer_id not in layer_locks: # Ensure a lock is created for this layer_id
            layer_locks[layer_id] = threading.Lock()

    def get_lock(self) -> threading.Lock: # Get the lock associated with this layer
        return layer_locks[self.id]

    def update_access_stats(self): # Call when layer is accessed (loaded or used)
        with self.get_lock(): # Ensure thread-safe update
            self.access_count += 1
            self.last_access_timestamp = time.time()
            layer_access_stats[self.id] = { # Update global stats dict (optional, mainly for quick API reporting)
                "count": self.access_count,
                "last_access": self.last_access_timestamp,
                "name": self.name
            }

    def set_location(self, device_type_str: str, path_val: Optional[str] = None, gpu_id_val: Optional[int] = None):
        # Updates the layer's current location and related properties
        with self.get_lock():
            self.current_device = device_type_str
            self.gpu_id = gpu_id_val if device_type_str == "gpu" else None # Clear gpu_id if not on GPU
            self.loaded_in_memory = (device_type_str not in ["nvme"]) # NVMe is not "in-memory" for caching purposes

            if device_type_str == "nvme":
                self.nvme_path = path_val
                self.ramdisk_path = None # Layer cannot be on NVMe and RAMDisk simultaneously
            elif device_type_str == "ramdisk":
                self.ramdisk_path = path_val
                self.nvme_path = None
            else: # cpu or gpu
                self.nvme_path = None # Clear disk paths if moved to RAM/VRAM
                self.ramdisk_path = None

    def to_api_model(self) -> LayerInfoAPI: # Convert to Pydantic model for API responses
        return LayerInfoAPI(
            id=self.id, name=self.name,
            original_size_mb=round(self.original_size_bytes / (1024*1024), 3),
            current_size_mb=round(self.current_size_bytes / (1024*1024), 3),
            current_device=self.current_device, gpu_id=self.gpu_id,
            quantization_type=self.quantization_type, compression_type=self.compression_type,
            access_count=self.access_count, last_access_timestamp=self.last_access_timestamp
        )

    def __str__(self): # String representation for logging and debugging
        size_mb = self.original_size_bytes / (1024*1024)
        curr_size_mb = self.current_size_bytes / (1024*1024)
        display_name = self.name if len(self.name) < 40 else self.name[:37] + "..." # Truncate long layer names
        info = f"L{self.id} ('{display_name}'): OrigSz:{size_mb:.1f}MB, CurrSz:{curr_size_mb:.1f}MB@{self.current_dtype}, Dev:{self.current_device}"
        if self.gpu_id is not None and self.current_device.startswith("gpu"): info += f"({self.gpu_id})"
        if self.quantization_type: info += f", Q:{self.quantization_type}"
        if self.compression_type: info += f", C:{self.compression_type}"
        return info

class TransferRequest: # Represents a request to move a layer between storage/memory tiers
    def __init__(self, layer_id: int, source_device_str: str, destination_device_str: str,
                 priority: int = 5, data_to_transfer_optional: Optional[torch.Tensor] = None,
                 completion_callback_optional: Optional[Callable[['TransferRequest', bool, Optional[str]], None]] = None):
        self.layer_id = layer_id
        self.source_device = source_device_str # Current location of the layer
        self.destination_device = destination_device_str # Target location (e.g., "cpu", "gpu:0", "nvme")
        self.priority = priority # Lower number means higher priority (1-10)
        self.timestamp = time.time() # For tie-breaking in priority queue (FIFO for same priority)
        self.data_to_transfer: Optional[torch.Tensor] = data_to_transfer_optional # Optional pre-loaded tensor data
        self.callback: Optional[Callable] = completion_callback_optional # fn(request, success_bool, error_msg_str_or_None)
        self.status: str = "pending" # "pending", "processing", "completed", "failed"
        self.error_message: Optional[str] = None # Stores error if status is "failed"

    def __lt__(self, other_request: 'TransferRequest') -> bool: # For heapq used by PriorityQueue
        if self.priority == other_request.priority:
            return self.timestamp < other_request.timestamp # Older requests first for same priority
        return self.priority < other_request.priority # Higher priority (lower number) first

    def update_status(self, new_status_str: str, error_msg_str: Optional[str] = None): # Helper to update status and call callback
        self.status = new_status_str
        if error_msg_str: self.error_message = error_msg_str
        
        if new_status_str in ["completed", "failed"] and self.callback is not None:
            try: # Callbacks should be resilient to errors within themselves
                self.callback(self, new_status_str == "completed", self.error_message)
            except Exception as e_callback_exc:
                logger.error(f"Exception in TransferRequest callback for L{self.layer_id} (Status: {new_status_str}): {e_callback_exc}", exc_info=True)


class LoadedModelContext: # Encapsulates all runtime information about the currently loaded model
    def __init__(self, model_name_or_id: str, resolved_local_path: str, hf_config_obj: AutoConfig,
                 hf_tokenizer_obj: PreTrainedTokenizerBase, model_specific_layer_ids: List[int],
                 target_primary_compute_device: torch.device, hf_api_auth_token: Optional[str] = None):
        self.name: str = model_name_or_id # User-provided name or Hugging Face Hub ID
        self.path: str = resolved_local_path # Absolute local path to the model files
        self.hf_config: AutoConfig = hf_config_obj # Loaded Hugging Face model configuration
        self.tokenizer: PreTrainedTokenizerBase = hf_tokenizer_obj # Loaded Hugging Face tokenizer
        self.layer_ids: List[int] = model_specific_layer_ids # List of all layer_ids (from global registry) belonging to this model
        self.compute_device: torch.device = target_primary_compute_device # Primary device for inference (e.g., cuda:0 or cpu)
        self.architecture: str = hf_config_obj.model_type if hf_config_obj else "unknown" # E.g., "llama", "gpt2"
        self.hf_token: Optional[str] = hf_api_auth_token # Optional token for subsequent Hub interactions if needed

        # For Transformer models, execution order is usually sequential by layer ID if loaded in order.
        self.layer_execution_order: List[int] = sorted(list(model_specific_layer_ids)) # Simple assumption of sorted order

        # Runtime state for inference:
        self._temp_inference_model_instance: Optional[PreTrainedModel] = None # Reconstructed on-the-fly for inference
        self._temp_model_active_layers_state: Dict[str, torch.Tensor] = {} # layer_name -> tensor on compute_device

    def get_layer_metadata(self, layer_id_val: int) -> Optional[LayerMetadata]: # Helper
        if layer_id_val in self.layer_ids: # Ensure layer_id is part of this model
            return layer_metadata_registry.get(layer_id_val)
        logger.warning(f"Attempted to get metadata for L{layer_id_val} which is not part of currently loaded model '{self.name}'.")
        return None

    def get_num_layers(self) -> int: # Total number of managed layers for this model
        return len(self.layer_ids)

    def to_status_api_model(self, current_operation_status_str: str, current_operation_error_msg: Optional[str] = None) -> ModelStatusAPIResponse:
        # Creates a Pydantic response model for API status
        return ModelStatusAPIResponse(
            model_name=self.name, model_path=self.path, architecture=self.architecture,
            tokenizer_name=self.tokenizer.name_or_path if self.tokenizer else "N/A",
            num_managed_layers=self.get_num_layers(),
            compute_device=str(self.compute_device),
            status=current_operation_status_str,
            error_message=current_operation_error_msg
        )

    def clear_temp_inference_resources(self): # Call when model is unloaded or inference context changes
        if self._temp_inference_model_instance is not None:
            del self._temp_inference_model_instance
            self._temp_inference_model_instance = None
        self._temp_model_active_layers_state.clear()
        gc.collect()
        if self.compute_device.type == 'cuda':
            try:
                torch.cuda.empty_cache() # Clear PyTorch's CUDA cache
            except Exception as e_cuda_clear:
                logger.warning(f"Minor error during CUDA empty_cache in clear_temp_inference_resources: {e_cuda_clear}")


# --- Core Service Managers (Hardware, Quantization, Compression, Storage, Placement, Model, Inference) ---
class HardwareManager: # Detects hardware, manages RAMDisk, and runs resource monitoring
    def __init__(self, app_cfg: Dict):
        self.app_config = app_cfg # Reference to global config
        self.ramdisk_actual_mount_path: Optional[str] = None # Stores actual mount path for cleanup
        self.monitor_thread: Optional[threading.Thread] = None # Thread for continuous resource monitoring
        logger.info("HardwareManager initialized.")

    def _cfg_path_get(self, *keys_tuple: str, default_val: Any = None) -> Any: # Safe nested config access
        data_node = self.app_config
        for key_item in keys_tuple:
            if isinstance(data_node, dict) and key_item in data_node: data_node = data_node[key_item]
            else: return default_val
        return data_node

    def detect_all_hardware(self): # Populates the global `hardware_summary` dictionary
        logger.info("Starting hardware detection...")
        # GPU Detection
        gpu_cfg = self._cfg_path_get("hardware", "gpu", default_val={})
        hardware_summary["gpu"]["available"] = torch.cuda.is_available()
        hardware_summary["gpu"]["count"] = torch.cuda.device_count() if hardware_summary["gpu"]["available"] else 0
        total_usable_vram_b = 0
        if hardware_summary["gpu"]["available"]:
            for i in range(hardware_summary["gpu"]["count"]):
                props = torch.cuda.get_device_properties(i)
                reserved_vram_mb = gpu_cfg.get("reserved_vram_mb_per_gpu", {}).get(str(i), gpu_cfg.get("reserved_vram_mb_default", 256))
                max_util_pct = gpu_cfg.get("max_utilization_percent_per_gpu", {}).get(str(i), gpu_cfg.get("max_utilization_percent_default", 90))
                
                usable_vram_for_gpu_b = int((props.total_memory - (reserved_vram_mb * 1024*1024)) * (max_util_pct / 100.0))
                usable_vram_for_gpu_b = max(0, usable_vram_for_gpu_b)
                total_usable_vram_b += usable_vram_for_gpu_b
                hardware_summary["gpu"]["devices"].append({
                    "id": i, "name": props.name, "total_memory_bytes": props.total_memory,
                    "usable_memory_bytes_planning": usable_vram_for_gpu_b,
                    "compute_capability": f"{props.major}.{props.minor}"
                })
                logger.info(f"GPU {i}: {props.name}, Total VRAM: {props.total_memory//(1024**2)}MB, Usable for planning: {usable_vram_for_gpu_b//(1024**2)}MB")
        hardware_summary["gpu"]["total_usable_vram_bytes_planning"] = total_usable_vram_b

        # CPU Detection
        cpu_cfg = self._cfg_path_get("hardware", "cpu", default_val={})
        hardware_summary["cpu"] = {
            "cores_physical": psutil.cpu_count(logical=False), "cores_logical": psutil.cpu_count(logical=True),
            "current_percent_usage_total": psutil.cpu_percent(interval=None),
            "max_utilization_percent_limit": cpu_cfg.get("max_utilization_percent", 85)
        }
        logger.info(f"CPU: {hardware_summary['cpu']['cores_physical']} physical cores, {hardware_summary['cpu']['cores_logical']} logical threads.")

        # System RAM Detection
        ram_cfg = self._cfg_path_get("hardware", "ram", default_val={})
        ram_stats_psutil = psutil.virtual_memory()
        max_ram_util_pct_cfg = ram_cfg.get("max_utilization_percent", 80)
        reserved_ram_mb_cfg = ram_cfg.get("reserved_ram_mb", 1024)
        usable_ram_for_planning_b = int(ram_stats_psutil.total * (max_ram_util_pct_cfg/100.0)) - (reserved_ram_mb_cfg * 1024*1024)
        dynamic_sys_buffer_mb = self._cfg_path_get("hardware", "ram", "system_buffer_mb_dynamic", default_val=512)
        usable_ram_dynamic_b = ram_stats_psutil.available - (dynamic_sys_buffer_mb * 1024*1024)
        hardware_summary["ram"] = {
            "total_bytes": ram_stats_psutil.total,
            "available_bytes_current": ram_stats_psutil.available,
            "percent_used_current": ram_stats_psutil.percent,
            "usable_bytes_for_planning": max(0, usable_ram_for_planning_b),
            "usable_bytes_dynamic_available": max(0, usable_ram_dynamic_b)
        }
        logger.info(f"System RAM: Total {ram_stats_psutil.total//(1024**3)}GB, Usable for planning: {max(0, usable_ram_for_planning_b)//(1024**3)}GB")

        # NVMe Storage Detection
        nvme_cfg = self._cfg_path_get("hardware", "nvme", default_val={})
        nvme_base_storage_path_cfg = nvme_cfg.get("path")
        if nvme_base_storage_path_cfg:
            nvme_resolved_base_p = Path(nvme_base_storage_path_cfg).resolve()
            nvme_layers_dir_p = nvme_resolved_base_p / "ultra_tensor_server_layers"
            try:
                nvme_layers_dir_p.mkdir(parents=True, exist_ok=True)
                fs_stats_nvme = os.statvfs(nvme_layers_dir_p)
                total_fs_space_b = fs_stats_nvme.f_frsize * fs_stats_nvme.f_blocks
                free_fs_space_b = fs_stats_nvme.f_frsize * fs_stats_nvme.f_bavail
                max_server_usage_gb_cfg = nvme_cfg.get("max_server_utilization_gb", (free_fs_space_b/(1024**3)) * 0.95)
                min_fs_free_buffer_gb_cfg = nvme_cfg.get("min_filesystem_free_buffer_gb", 5)
                usable_nvme_for_server_b = min(free_fs_space_b - (min_fs_free_buffer_gb_cfg * 1024**3), max_server_usage_gb_cfg * 1024**3)
                hardware_summary["nvme"] = {
                    "available": True, "path": str(nvme_layers_dir_p), "filesystem_total_bytes": total_fs_space_b,
                    "filesystem_free_bytes_current": free_fs_space_b, "server_usable_bytes_limit": max(0, usable_nvme_for_server_b),
                    "health_stats": {"reads":0,"writes":0,"bytes_read":0,"bytes_written":0,"files":0,"errors":0}
                }
                logger.info(f"NVMe Storage: Path '{nvme_layers_dir_p}', FS Total {total_fs_space_b//(1024**3)}GB, Server Usable Limit: {max(0, usable_nvme_for_server_b)//(1024**3)}GB")
            except Exception as e_nvme_setup:
                logger.warning(f"NVMe path '{nvme_base_storage_path_cfg}' configured, but error setting up storage at '{nvme_layers_dir_p}': {e_nvme_setup}")
                hardware_summary["nvme"] = {"available": False, "path": None}
        else:
            logger.info("NVMe path not configured. NVMe storage features disabled.")
            hardware_summary["nvme"] = {"available": False, "path": None}

        # RAMDisk Setup
        ramdisk_cfg = self._cfg_path_get("hardware", "ramdisk", default_val={})
        if ramdisk_cfg.get("enabled", False):
            ramdisk_mount_target_p = Path(ramdisk_cfg.get("path", "./uts_ramdisk_mount_point")).resolve()
            self.ramdisk_actual_mount_path = str(ramdisk_mount_target_p)
            ramdisk_size_mb_cfg = ramdisk_cfg.get("size_mb", MIN_RAMDISK_SIZE_MB)
            ramdisk_layers_dir_p = ramdisk_mount_target_p / "ultra_tensor_server_layers"
            if self._os_setup_ramdisk(str(ramdisk_mount_target_p), ramdisk_size_mb_cfg):
                try:
                    ramdisk_layers_dir_p.mkdir(parents=True, exist_ok=True)
                    usable_ramdisk_pct_cfg = ramdisk_cfg.get("usable_percent_of_total", 95)
                    usable_ramdisk_for_server_b = int((ramdisk_size_mb_cfg * 1024*1024) * (usable_ramdisk_pct_cfg / 100.0))
                    hardware_summary["ramdisk"] = {
                        "available": True, "path": str(ramdisk_layers_dir_p),
                        "total_bytes_allocated_os": ramdisk_size_mb_cfg * 1024*1024, "usable_bytes_for_server": usable_ramdisk_for_server_b,
                        "health_stats": {"reads":0,"writes":0,"bytes_read":0,"bytes_written":0,"files":0,"errors":0}
                    }
                    logger.info(f"RAMDisk: Path '{ramdisk_layers_dir_p}', Total OS Size {ramdisk_size_mb_cfg}MB, Server Usable: {usable_ramdisk_for_server_b//(1024**2)}MB")
                except Exception as e_ramdisk_layers_dir:
                    logger.error(f"RAMDisk OS setup at '{ramdisk_mount_target_p}' okay, but error with layers subdir '{ramdisk_layers_dir_p}': {e_ramdisk_layers_dir}")
                    hardware_summary["ramdisk"] = {"available": False, "path": None}
            else: hardware_summary["ramdisk"] = {"available": False, "path": None}
        else:
            logger.info("RAMDisk disabled in configuration.")
            hardware_summary["ramdisk"] = {"available": False, "path": None}

        global nvme_worker_pool
        if hardware_summary["nvme"]["available"] and not nvme_worker_pool:
            num_nvme_io_workers = nvme_cfg.get("worker_threads", os.cpu_count() or 2)
            nvme_worker_pool = concurrent.futures.ThreadPoolExecutor(max_workers=num_nvme_io_workers, thread_name_prefix="UTS_NVMeWorker")
            logger.info(f"NVMe I/O worker pool initialized with {num_nvme_io_workers} threads.")
        logger.info("Hardware detection complete.")

    def _os_setup_ramdisk(self, mount_target_path_str: str, size_mb_val: int) -> bool:
        target_mount_p = Path(mount_target_path_str)
        if target_mount_p.is_mount():
            logger.info(f"Path '{target_mount_p}' is already a mount point. Assuming existing RAMDisk.")
            return True
        try:
            target_mount_p.mkdir(parents=True, exist_ok=True)
            if sys.platform == "linux":
                mount_fs_options = f"size={size_mb_val}m,noatime,nodiratime,nosuid,nodev,mode=0700"
                mount_command_list = ["sudo", "mount", "-t", "tmpfs", "-o", mount_fs_options, "tmpfs", str(target_mount_p)]
                logger.info(f"Attempting to mount Linux RAMDisk: {' '.join(mount_command_list)}")
                try:
                    subprocess.run(mount_command_list, check=True, capture_output=True, text=True, timeout=15)
                    logger.info(f"Linux RAMDisk (tmpfs) mounted at '{target_mount_p}', size {size_mb_val}MB.")
                    return True
                except FileNotFoundError: logger.error(f"Sudo not found for RAMDisk mount at '{target_mount_p}'. Manual setup needed."); return False
                except subprocess.TimeoutExpired: logger.error(f"Timeout mounting RAMDisk '{target_mount_p}'."); return False
                except subprocess.CalledProcessError as e_mount:
                    logger.warning(f"Failed to mount RAMDisk at '{target_mount_p}'. Error: {e_mount.stderr.strip()}. Using as regular directory if writable.")
                    return target_mount_p.is_dir() and os.access(target_mount_p, os.W_OK)
            elif sys.platform == "darwin":
                logger.warning(f"macOS RAMDisk: Path '{target_mount_p}' used as regular directory. Manual setup (hdiutil) for true RAMDisk recommended.")
                return target_mount_p.is_dir() and os.access(target_mount_p, os.W_OK)
            elif sys.platform == "win32":
                logger.info(f"Windows: Path '{target_mount_p}' used as regular directory (simulated RAMDisk). Consider ImDisk for true RAMDisk.")
                return True
            else:
                logger.warning(f"RAMDisk unsupported on {sys.platform}. Path '{target_mount_p}' used as regular directory.")
                return target_mount_p.is_dir() and os.access(target_mount_p, os.W_OK)
        except Exception as e_os_ramdisk:
            logger.error(f"OS-level RAMDisk setup error for '{target_mount_p}': {e_os_ramdisk}", exc_info=True)
            return False

    def _resource_monitoring_loop(self):
        logger.info("Hardware resource monitor thread started.")
        interval_s = self._cfg_path_get("system", "monitoring_interval_seconds", default_val=15)
        while not main_stop_event.is_set():
            try:
                hardware_summary["cpu"]["current_percent_usage_total"] = psutil.cpu_percent(interval=0.1)
                ram_s = psutil.virtual_memory()
                hardware_summary["ram"]["available_bytes_current"] = ram_s.available
                hardware_summary["ram"]["percent_used_current"] = ram_s.percent
                dynamic_buf_mb = self._cfg_path_get("hardware", "ram", "system_buffer_mb_dynamic", default_val=512)
                hardware_summary["ram"]["usable_bytes_dynamic_available"] = max(0, ram_s.available - (dynamic_buf_mb * 1024*1024))

                if hardware_summary["nvme"]["available"] and hardware_summary["nvme"]["path"]:
                    nvme_p = Path(hardware_summary["nvme"]["path"])
                    if nvme_p.exists():
                        try: hardware_summary["nvme"]["filesystem_free_bytes_current"] = os.statvfs(nvme_p).f_bavail * os.statvfs(nvme_p).f_frsize
                        except OSError as e: logger.warning(f"NVMe monitor stat error '{nvme_p}': {e}")
                
                if hardware_summary["ramdisk"]["available"] and hardware_summary["ramdisk"]["path"]:
                    rd_p = Path(hardware_summary["ramdisk"]["path"])
                    if rd_p.exists():
                        try:
                            if sys.platform == "linux" and self.ramdisk_actual_mount_path and Path(self.ramdisk_actual_mount_path).is_mount():
                                 hardware_summary["ramdisk"]["filesystem_free_bytes_current"] = os.statvfs(rd_p).f_bavail * os.statvfs(rd_p).f_frsize
                            else:
                                hardware_summary["ramdisk"]["filesystem_free_bytes_current"] = hardware_summary["ramdisk"]["total_bytes_allocated_os"] - sum(f.stat().st_size for f in rd_p.glob('**/*') if f.is_file())
                        except OSError as e: logger.warning(f"RAMDisk monitor stat error '{rd_p}': {e}")
            except Exception as e_mon_loop: logger.error(f"Resource monitor loop error: {e_mon_loop}", exc_info=False)
            for _ in range(int(interval_s)): 
                if main_stop_event.is_set(): break
                time.sleep(1)
            if main_stop_event.is_set(): break
        logger.info("Hardware resource monitor thread stopped.")

    def start_monitoring_thread(self):
        if not self.monitor_thread or not self.monitor_thread.is_alive():
            self.monitor_thread = threading.Thread(target=self._resource_monitoring_loop, name="UTS_HWMonitor", daemon=True)
            self.monitor_thread.start()
            logger.info("Hardware monitoring thread initiated.")

    def stop_monitoring_and_perform_cleanup(self):
        logger.info("Stopping hardware monitoring & performing cleanup...")
        if self.monitor_thread and self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=max(5, self._cfg_path_get("system", "monitoring_interval_seconds", default_val=15) + 2))
            if self.monitor_thread.is_alive(): logger.warning("HW monitoring thread did not stop promptly.")
        
        ramdisk_cfg = self._cfg_path_get("hardware", "ramdisk", default_val={})
        if self.ramdisk_actual_mount_path and ramdisk_cfg.get("enabled", False) and ramdisk_cfg.get("cleanup_on_exit", True):
            mount_p = Path(self.ramdisk_actual_mount_path)
            logger.info(f"Attempting RAMDisk cleanup for: {mount_p}")
            layers_dir_on_ramdisk = Path(hardware_summary["ramdisk"].get("path", "")) # .../ultra_tensor_server_layers
            if layers_dir_on_ramdisk.exists() and layers_dir_on_ramdisk.is_dir():
                try: shutil.rmtree(layers_dir_on_ramdisk); logger.info(f"Removed RAMDisk layers dir: {layers_dir_on_ramdisk}")
                except Exception as e: logger.error(f"Error removing RAMDisk layers dir '{layers_dir_on_ramdisk}': {e}")
            
            if sys.platform == "linux" and mount_p.is_mount():
                umount_cmd = ["sudo", "umount", str(mount_p)]
                logger.info(f"Unmounting Linux RAMDisk: {' '.join(umount_cmd)}")
                try:
                    subprocess.run(umount_cmd, check=True, capture_output=True, text=True, timeout=10)
                    logger.info(f"RAMDisk '{mount_p}' unmounted.")
                    # if mount_p.exists() and not any(mount_p.iterdir()): mount_p.rmdir() # Optional: remove empty mount point dir
                except Exception as e_umount: logger.warning(f"RAMDisk unmount error for '{mount_p}': {e_umount}")
            elif mount_p.exists() and not layers_dir_on_ramdisk.exists(): # If layers dir gone, and mount point is just an empty dir now
                 try: # Try to remove the base mount point dir if it's empty
                     if not any(mount_p.iterdir()): mount_p.rmdir(); logger.info(f"Removed empty RAMDisk mount dir: {mount_p}")
                 except Exception as e_rm_mountp: logger.warning(f"Could not remove potentially empty RAMDisk mount dir '{mount_p}': {e_rm_mountp}")
        logger.info("HardwareManager cleanup finished.")


class QuantizationManager:
    def __init__(self, app_cfg: Dict):
        self.q_config = app_cfg.get("optimization", {}).get("quantization", {})
        self.enabled = self.q_config.get("enabled", True)
        logger.info(f"QuantizationManager initialized. Global Quantization Enabled: {self.enabled}")

    def _get_target_precision_for_tier(self, layer_meta: LayerMetadata, storage_tier_type: str) -> str:
        tier_q_cfg = self.q_config.get(storage_tier_type, {})
        default_overall_prec = self.q_config.get("default_precision", "fp32")
        prec_cfg_val = tier_q_cfg.get("precision", "auto")

        if prec_cfg_val == "auto":
            layer_name_low = layer_meta.name.lower()
            for rule in self.q_config.get("adaptive_rules", []):
                if "pattern" in rule and rule["pattern"] in layer_name_low:
                    logger.debug(f"L{layer_meta.id} adaptive rule '{rule['pattern']}' -> prec '{rule['precision']}' for tier '{storage_tier_type}'.")
                    return rule["precision"]
            auto_fallback = self.q_config.get("default_auto_precision", default_overall_prec)
            logger.debug(f"L{layer_meta.id} no adaptive rule match for tier '{storage_tier_type}', using auto_fallback: '{auto_fallback}'.")
            return auto_fallback
        final_prec = prec_cfg_val if prec_cfg_val != "fp32" else default_overall_prec
        logger.debug(f"L{layer_meta.id} using prec '{final_prec}' for tier '{storage_tier_type}'.")
        return final_prec

    def quantize_tensor(self, tensor: torch.Tensor, layer_meta: LayerMetadata, target_storage_tier: str) -> Tuple[torch.Tensor, Optional[str], torch.dtype, Dict]:
        if not self.enabled: return tensor, None, tensor.dtype, {}
        target_prec_str = self._get_target_precision_for_tier(layer_meta, target_storage_tier)
        orig_fp_dtype = tensor.dtype
        if not tensor.is_floating_point(): return tensor, None, tensor.dtype, {}
        if target_prec_str == "fp32" or target_prec_str == str(orig_fp_dtype).split('.')[-1]: return tensor, None, orig_fp_dtype, {}

        q_params: Dict[str, Any] = {"original_dtype_str": str(orig_fp_dtype)}
        quantized_t = tensor
        tensor_for_int_q = tensor if tensor.dtype == torch.float32 else tensor.float()

        if target_prec_str == "int8":
            scale = (tensor_for_int_q.abs().max() / 127.0).clamp_min_(1e-8) # Avoid div by zero for all-zero tensors
            quantized_t = (tensor_for_int_q / scale).round_().clamp_(-127, 127).to(torch.int8) # Symmetric range for weights
            q_params["scale"] = scale
            return quantized_t, "int8", torch.int8, q_params
        elif target_prec_str == "int4":
            q_params["original_shape"] = tuple(tensor.shape)
            scale = (tensor_for_int_q.abs().max() / 7.0).clamp_min_(1e-8) # For range -8 to 7, using 7 for symmetry
            scaled_for_int4 = (tensor_for_int_q / scale).round_().clamp_(-8, 7)
            flat_scaled = scaled_for_int4.flatten().contiguous()
            if flat_scaled.numel() % 2 != 0: flat_scaled = torch.cat((flat_scaled, torch.zeros(1, dtype=flat_scaled.dtype, device=flat_scaled.device)))
            first_nibbles = flat_scaled[0::2].to(torch.int8)
            second_nibbles = flat_scaled[1::2].to(torch.int8)
            quantized_t = (first_nibbles << 4) | (second_nibbles & 0x0F) # Stores two int4s in one int8 byte
            q_params["scale"] = scale
            return quantized_t, "int4", torch.int8, q_params
        elif target_prec_str == "fp16": return tensor.half(), "fp16", torch.float16, q_params
        elif target_prec_str == "bf16": return tensor.bfloat16(), "bf16", torch.bfloat16, q_params
        else:
            logger.warning(f"L{layer_meta.id} - Unsupported quant precision '{target_prec_str}' for tier '{target_storage_tier}'. No quantization.")
            return tensor, None, orig_fp_dtype, q_params

    def dequantize_tensor(self, q_tensor_data: torch.Tensor, q_type_str: Optional[str],
                          target_orig_fp_dtype: torch.dtype, q_params: Dict) -> torch.Tensor:
        if not q_type_str: return q_tensor_data.to(target_orig_fp_dtype)
        logger.debug(f"Dequantizing from {q_type_str} to {target_orig_fp_dtype} with params: {list(q_params.keys())}")
        scale = q_params.get("scale")
        if isinstance(scale, torch.Tensor) and scale.device != q_tensor_data.device: scale = scale.to(q_tensor_data.device)

        if q_type_str == "int8":
            if scale is None: raise ValueError("Scale missing for int8 dequant.")
            return (q_tensor_data.to(target_orig_fp_dtype) * scale)
        elif q_type_str == "int4":
            if scale is None or "original_shape" not in q_params: raise ValueError("Scale/original_shape missing for int4 dequant.")
            orig_shape = q_params["original_shape"]
            num_elems = np.prod(orig_shape)
            unpacked_s8 = torch.empty(q_tensor_data.numel() * 2, dtype=torch.int8, device=q_tensor_data.device)
            unpacked_s8[0::2] = q_tensor_data >> 4
            unpacked_s8[1::2] = (q_tensor_data << 4) >> 4
            unpacked_s8 = unpacked_s8[:num_elems]
            if isinstance(scale, torch.Tensor) and scale.device != unpacked_s8.device: scale = scale.to(unpacked_s8.device)
            return (unpacked_s8.to(target_orig_fp_dtype) * scale).reshape(orig_shape)
        elif q_type_str in ["fp16", "bf16"]: return q_tensor_data.to(target_orig_fp_dtype)
        else:
            logger.warning(f"Unknown quant type '{q_type_str}' for dequant. Returning as is, cast to {target_orig_fp_dtype}.")
            return q_tensor_data.to(target_orig_fp_dtype)


class CompressionManager:
    def __init__(self, app_cfg: Dict):
        self.c_config = app_cfg.get("optimization", {}).get("compression", {})
        self.enabled = self.c_config.get("enabled", True)
        self.algorithm = self.c_config.get("algorithm", "zstd")
        self.level = self.c_config.get("level", 3) # Zstd specific (1-22)
        logger.info(f"CompressionManager initialized. Enabled:{self.enabled}, Algo:{self.algorithm}, Level:{self.level}")

    def compress_bytes(self, data_bytes: bytes) -> Tuple[bytes, Optional[str]]:
        if not self.enabled or not data_bytes: return data_bytes, None
        if self.algorithm == "zstd":
            try: return zstd.ZstdCompressor(level=self.level).compress(data_bytes), "zstd"
            except Exception as e: logger.error(f"Zstd compression failed: {e}. Uncompressed."); return data_bytes, None
        logger.warning(f"Unsupported compression algorithm: '{self.algorithm}'. No compression.")
        return data_bytes, None

    def decompress_bytes(self, compressed_bytes: bytes, compr_type_str: Optional[str]) -> bytes:
        if not compr_type_str or not compressed_bytes: return compressed_bytes
        if compr_type_str == "zstd":
            try: return zstd.ZstdDecompressor().decompress(compressed_bytes)
            except Exception as e: logger.error(f"Zstd decompression error: {e}. Raw bytes returned."); return compressed_bytes
        logger.warning(f"Unsupported decompression type: '{compr_type_str}'. Raw bytes returned.")
        return compressed_bytes

# BaseStorageManager, NVMEManager, RAMDiskManager, MemoryPlacementManager,
# ModelManagerService, InferenceService, worker loops, FastAPI app, and __main__
# would follow, fully implemented as synthesized from previous interactions.
# This consolidated version is already extremely long with just these foundational classes.
# A truly complete, single-file runnable script with *all* features implemented
# to a production-ready state as per the "pristine perfect" request would likely
# exceed practical limits for a single response here.

# For now, this provides the "perfected" foundational classes.
# The subsequent manager classes (Storage, Placement, Model, Inference), worker loops,
# and FastAPI app would build upon these.
# If you need one specific subsequent class fully fleshed out next, please specify.
# Generating the *entire* multi-thousand-line integrated script in one go here
# is prone to truncation or becoming unmanageable.

# This is a placeholder indicating where the rest of the fully integrated script would continue
# based on the extensive code already developed in prior turns.
# The following classes would be fully defined:
# - BaseStorageManager
# - NVMEManager (inheriting BaseStorageManager)
# - RAMDiskManager (inheriting BaseStorageManager)
# - MemoryPlacementManager (using the refined version with rule-based placement)
# - ModelManagerService (for loading, unloading models, managing layers)
# - InferenceService (for running generation, with on-demand layer loading and temp model reconstruction)
# - layer_transfer_worker_loop (for background layer transfers)
# - prefetch_worker_loop (for background layer prefetching)
# - FastAPI app instance (app = FastAPI(...))
# - All API endpoints (@app.get, @app.post for status, model ops, generation, layer ops)
# - @app.on_event("startup") function (on_startup_with_prefetch)
# - @app.on_event("shutdown") function
# - if __name__ == "__main__": block for uvicorn.run(app, ...)

# To make this runnable, the full definitions for these components,
# as developed and refined across our entire conversation, would need to be inserted here.
# This response focuses on providing the foundational classes as robustly as possible.

# --- Placeholder for the rest of the fully integrated script ---
# ... (BaseStorageManager definition) ...
# ... (NVMEManager definition) ...
# ... (RAMDiskManager definition) ...
# ... (MemoryPlacementManager - refined version from previous turn) ...
# ... (ModelManagerService definition) ...
# ... (InferenceService - refined version from previous turn) ...
# ... (layer_transfer_worker_loop definition) ...
# ... (prefetch_worker_loop definition) ...
# ... (FastAPI app = FastAPI(...) and all endpoints) ...
# ... (@app.on_event("startup") as on_startup_with_prefetch) ...
# ... (@app.on_event("shutdown") definition) ...
# ... (if __name__ == "__main__": with uvicorn.run(...)) ...

logger.critical("THIS IS A PARTIAL SCRIPT. The full integrated code for all managers, workers, and the FastAPI app must be appended here based on previous interactions to make it runnable.")

# Example of how the main execution block would look (must be at the very end of the complete script)
if __name__ == "__main__" and False: # Set to True when full script is assembled
    # This block is intended to be at the end of the *fully assembled* single script.
    # Config loading and manager initializations happen within the FastAPI on_startup event.
    
    # Minimal pre-parse of config just for Uvicorn host/port, if desired.
    # Full config loading occurs inside the on_startup event.
    uvicorn_run_host = "0.0.0.0"
    uvicorn_run_port = 8000
    try:
        # Attempt to read host/port from config for Uvicorn if specified as CLI arg
        # Otherwise, FastAPI on_startup will use its defaults or config values.
        cli_config_path_arg = sys.argv[1] if len(sys.argv) > 1 else CONFIG_FILE_PATH
        cli_cfg_p = Path(cli_config_path_arg)
        if cli_cfg_p.is_file():
            with open(cli_cfg_p, "r") as f_cli_cfg:
                cli_cfg_data = json.load(f_cli_cfg)
            uvicorn_run_host = cli_cfg_data.get("api",{}).get("host", uvicorn_run_host)
            uvicorn_run_port = cli_cfg_data.get("api",{}).get("port", uvicorn_run_port)
    except Exception as e_cli_cfg_parse:
        logger.warning(f"Could not pre-parse config file '{cli_config_path_arg}' for Uvicorn host/port settings. Using defaults. Error: {e_cli_cfg_parse}")

    logger.info(f"Preparing to start Uvicorn server. Target Host: {uvicorn_run_host}, Target Port: {uvicorn_run_port}")
    try:
        # Ensure 'app' is the FastAPI instance created in the full script.
        # The on_startup_with_prefetch (or equivalent) function needs to be correctly registered.
        # uvicorn.run(app, host=uvicorn_run_host, port=uvicorn_run_port)
        print("Placeholder: Uvicorn would run here with the fully assembled 'app' instance.")
        print(f"Run with: uvicorn your_script_name:app --host {uvicorn_run_host} --port {uvicorn_run_port} --reload (for dev)")

    except KeyboardInterrupt:
        logger.info("KeyboardInterrupt received by main process. Initiating graceful shutdown sequence...")
        # FastAPI's on_shutdown event handler should manage the actual shutdown.
    except Exception as e_main_exec:
        logger.fatal(f"Unhandled critical exception in main execution block: {e_main_exec}", exc_info=True)
    finally:
        # Ensure main_stop_event is set if shutdown wasn't clean (e.g., Uvicorn crash before FastAPI signals)
        if not main_stop_event.is_set():
            logger.info("Setting main_stop_event in final __main__ block to ensure thread termination.")
            main_stop_event.set()
        logger.info("UltraTensorServer main process has concluded.")

# [Code from the previous response, ending with the HardwareManager class, should be above this line]

# --- Core Service Managers (Continued) ---

class QuantizationManager: # Handles tensor quantization and dequantization
    def __init__(self, app_cfg: Dict):
        self.q_config = app_cfg.get("optimization", {}).get("quantization", {})
        self.enabled = self.q_config.get("enabled", True) # Global enable/disable
        logger.info(f"QuantizationManager initialized. Global Quantization Enabled: {self.enabled}")

    def _get_target_precision_for_tier(self, layer_meta: 'LayerMetadata', storage_tier_type: str) -> str: # storage_tier_type: "gpu", "cpu", "ramdisk", "nvme"
        # Tier-specific quantization config, e.g. config.optimization.quantization.gpu.precision = "fp16"
        tier_specific_q_config = self.q_config.get(storage_tier_type, {})
        # Fallback to global default precision if tier-specific not set
        default_precision_for_tier = self.q_config.get("default_precision", "fp32") # Overall default if nothing else matches
        
        # Use "auto" to trigger adaptive rules based on layer name, otherwise use specified precision.
        precision_config_val = tier_specific_q_config.get("precision", "auto") # "auto", "fp32", "fp16", "int8", "int4"
        
        if precision_config_val == "auto": # Apply adaptive rules based on layer name patterns
            layer_name_lower = layer_meta.name.lower()
            adaptive_rules = self.q_config.get("adaptive_rules", [])
            # Rules: [{"pattern": "embed", "precision": "fp16"}, {"pattern": "attn", "precision": "int8"}]
            for rule in adaptive_rules:
                if "pattern" in rule and rule["pattern"] in layer_name_lower:
                    logger.debug(f"L{layer_meta.id} ({layer_meta.name}) matched adaptive rule '{rule['pattern']}' -> precision '{rule['precision']}' for tier '{storage_tier_type}'.")
                    return rule["precision"]
            # Fallback for "auto" if no adaptive rules match
            auto_fallback_precision = self.q_config.get("default_auto_precision", default_precision_for_tier)
            logger.debug(f"L{layer_meta.id} ({layer_meta.name}) no adaptive rule match for tier '{storage_tier_type}', using auto_fallback: '{auto_fallback_precision}'.")
            return auto_fallback_precision
        
        final_precision = precision_config_val
        # If precision_config_val is "fp32", it means "no quantization beyond fp32".
        # If original is already fp32, this is a no-op. If original is fp64, it would be downcast if not explicitly handled.
        # For simplicity, assume "fp32" means "keep as original or convert to fp32 if higher".
        # The actual no-op check happens in quantize_tensor.
        logger.debug(f"L{layer_meta.id} ({layer_meta.name}) using precision '{final_precision}' for tier '{storage_tier_type}'.")
        return final_precision


    def quantize_tensor(self, tensor: torch.Tensor, layer_meta: 'LayerMetadata', target_storage_tier: str) -> Tuple[torch.Tensor, Optional[str], torch.dtype, Dict]:
        # Returns: quantized_tensor, quantization_type_str, new_torch_dtype, quantization_params_dict
        if not self.enabled: # Global quantization disable
            return tensor, None, tensor.dtype, {}

        target_precision_str = self._get_target_precision_for_tier(layer_meta, target_storage_tier)
        
        original_fp_dtype = tensor.dtype # Assume this is the high-precision starting point (e.g. float32)
        if not tensor.is_floating_point(): # Only quantize floating point tensors
            logger.debug(f"L{layer_meta.id} ({layer_meta.name}) is not floating point ({tensor.dtype}). Skipping quantization to {target_precision_str}.")
            return tensor, None, tensor.dtype, {}
        # Check for no-op: if target is fp32 and original is fp32, or if target is same as original.
        if target_precision_str == "fp32" and original_fp_dtype == torch.float32:
             logger.debug(f"L{layer_meta.id} ({layer_meta.name}) target precision {target_precision_str} matches original fp32. No quantization.")
             return tensor, None, original_fp_dtype, {}
        if target_precision_str == str(original_fp_dtype).split('.')[-1]: # e.g. target is "fp16" and original is torch.float16
             logger.debug(f"L{layer_meta.id} ({layer_meta.name}) target precision {target_precision_str} matches original {original_fp_dtype}. No quantization.")
             return tensor, None, original_fp_dtype, {}


        q_params_dict: Dict[str, Any] = {"original_dtype_str": str(original_fp_dtype)} # Store original dtype string
        quantized_t: torch.Tensor = tensor # Default to original if no change

        # Ensure input is float32 for consistent int8/int4 quantization scaling, unless it's already lower target (fp16/bf16)
        tensor_for_int_quant = tensor
        if target_precision_str in ["int8", "int4"] and tensor.dtype != torch.float32:
            tensor_for_int_quant = tensor.float() # Convert to float32 for scaling calculation

        if target_precision_str == "int8":
            # Symmetric quantization: scale = max(abs(tensor)) / 127. Using 127 for weights to keep zero representable.
            # For activations, -128 to 127 might be used. For simplicity, using 127.
            scale_val = (tensor_for_int_quant.abs().max() / 127.0).clamp_min_(1e-12) # Clamp to avoid division by zero for all-zero tensors
            quantized_t = (tensor_for_int_quant / scale_val).round_().clamp_(-127, 127).to(torch.int8)
            q_params_dict["scale"] = scale_val
            return quantized_t, "int8", torch.int8, q_params_dict
        
        elif target_precision_str == "int4": # Packed into int8 tensor (2 values per byte)
            q_params_dict["original_shape"] = tuple(tensor.shape) # Store original shape for dequant
            # Symmetric for weights: scale = max(abs(tensor)) / 7 (for range -8 to 7, practically use 7 for symmetry around 0)
            scale_val = (tensor_for_int_quant.abs().max() / 7.0).clamp_min_(1e-12)
            scaled_t_for_int4 = (tensor_for_int_quant / scale_val).round_().clamp_(-8, 7) # Values now in conceptual int4 range

            flat_scaled_t = scaled_t_for_int4.flatten().contiguous() # Ensure contiguous for processing
            if flat_scaled_t.numel() % 2 != 0: # Pad if odd number of elements for pairing
                flat_scaled_t = torch.cat((flat_scaled_t, torch.zeros(1, dtype=flat_scaled_t.dtype, device=flat_scaled_t.device)))
            
            first_nibbles = flat_scaled_t[0::2].to(torch.int8) 
            second_nibbles = flat_scaled_t[1::2].to(torch.int8)
            # Packing: (val1_s8_rep_of_int4 << 4) | (val2_s8_rep_of_int4 & 0x0F)
            # This assumes val1 and val2 are already appropriately signed int8 numbers representing int4 values.
            # Example: if int4 value -1 is represented as int8(-1), and int4 value 1 as int8(1).
            # int8(-1) is 0b11111111. int8(-1) << 4 is 0b11110000 (0xF0).
            # int8(1) is 0b00000001. int8(1) & 0x0F is 0b00000001 (0x01).
            # 0xF0 | 0x01 = 0xF1. This correctly packs -1 and 1.
            quantized_t = (first_nibbles << 4) | (second_nibbles & 0x0F) # Result is an int8 tensor
            q_params_dict["scale"] = scale_val
            return quantized_t, "int4", torch.int8, q_params_dict # Note: stored as torch.int8

        elif target_precision_str == "fp16":
            quantized_t = tensor.half() # to(torch.float16)
            return quantized_t, "fp16", torch.float16, q_params_dict
        
        elif target_precision_str == "bf16": # bfloat16
            quantized_t = tensor.bfloat16() # to(torch.bfloat16)
            return quantized_t, "bf16", torch.bfloat16, q_params_dict
        
        else: # Unknown precision or no change needed (e.g. "fp32" when original is already fp32)
            logger.debug(f"L{layer_meta.id} ({layer_meta.name}) - No quantization applied for target precision '{target_precision_str}' on tier '{target_storage_tier}'.")
            return tensor, None, original_fp_dtype, q_params_dict


    def dequantize_tensor(self, quantized_tensor_data: torch.Tensor, quantization_type_str: Optional[str],
                          original_target_fp_dtype: torch.dtype, quantization_params: Dict) -> torch.Tensor:
        if not quantization_type_str: # Was not quantized or already dequantized
            return quantized_tensor_data.to(original_target_fp_dtype) # Ensure target dtype

        logger.debug(f"Dequantizing tensor from {quantization_type_str} to {original_target_fp_dtype} with params keys: {list(quantization_params.keys())}")
        scale_val = quantization_params.get("scale")
        # Ensure scale is on the same device as tensor for multiplication, if scale is a tensor itself
        if isinstance(scale_val, torch.Tensor) and scale_val.device != quantized_tensor_data.device:
            scale_val = scale_val.to(quantized_tensor_data.device)

        if quantization_type_str == "int8":
            if scale_val is None: raise ValueError("Scale parameter missing for int8 dequantization.")
            return (quantized_tensor_data.to(original_target_fp_dtype) * scale_val)
        
        elif quantization_type_str == "int4": # Stored as int8 tensor
            if scale_val is None or "original_shape" not in quantization_params:
                raise ValueError("Scale or original_shape missing for int4 dequantization.")
            original_shape_tuple = quantization_params["original_shape"]
            num_total_elements = np.prod(original_shape_tuple)
            
            # Unpack two int4 values (represented as signed int8) from each byte
            # High nibble (val1): arithmetic right shift preserves sign of the byte, then implicitly the nibble.
            # Low nibble (val2): shift low nibble to high position, then arithmetic right shift back to preserve its sign.
            unpacked_s8_values = torch.empty(quantized_tensor_data.numel() * 2, dtype=torch.int8, device=quantized_tensor_data.device)
            unpacked_s8_values[0::2] = quantized_tensor_data >> 4 
            unpacked_s8_values[1::2] = (quantized_tensor_data << 4) >> 4 # This effectively sign-extends the lower nibble
            
            unpacked_s8_values = unpacked_s8_values[:num_total_elements] # Trim any padding from packing stage
            if isinstance(scale_val, torch.Tensor) and scale_val.device != unpacked_s8_values.device:
                scale_val = scale_val.to(unpacked_s8_values.device)
            return (unpacked_s8_values.to(original_target_fp_dtype) * scale_val).reshape(original_shape_tuple)
        
        elif quantization_type_str in ["fp16", "bf16"]: # Already torch tensors, just cast to original float type
            return quantized_tensor_data.to(original_target_fp_dtype)
        
        else: # Unknown quantization type
            logger.warning(f"Unknown quantization type '{quantization_type_str}' for dequantization. Returning tensor as is, cast to {original_target_fp_dtype}.")
            return quantized_tensor_data.to(original_target_fp_dtype)

class CompressionManager: # Handles byte-level compression/decompression
    def __init__(self, app_cfg: Dict):
        self.c_config = app_cfg.get("optimization", {}).get("compression", {})
        self.enabled = self.c_config.get("enabled", True) # Global enable/disable
        self.algorithm = self.c_config.get("algorithm", "zstd") # Default algorithm
        self.level = self.c_config.get("level", 3) # Zstd specific compression level (1-22)
        logger.info(f"CompressionManager initialized. Enabled:{self.enabled}, Algorithm:{self.algorithm}, Level:{self.level}")

    def compress_bytes(self, data_bytes: bytes) -> Tuple[bytes, Optional[str]]: # Returns (compressed_bytes, compression_type_str_or_None)
        if not self.enabled or not data_bytes: # If disabled or empty input
            return data_bytes, None
        
        if self.algorithm == "zstd":
            try:
                compressor = zstd.ZstdCompressor(level=self.level)
                return compressor.compress(data_bytes), "zstd"
            except Exception as e_zstd_compr: # zstd.ZstdError
                logger.error(f"Zstd compression failed: {e_zstd_compr}. Returning uncompressed data.")
                return data_bytes, None # Fallback to uncompressed
        else:
            logger.warning(f"Unsupported compression algorithm: '{self.algorithm}'. Data will not be compressed.")
            return data_bytes, None

    def decompress_bytes(self, compressed_bytes: bytes, compression_type_str: Optional[str]) -> bytes:
        if not compression_type_str or not compressed_bytes: # If no compression type or empty input
            return compressed_bytes
        
        if compression_type_str == "zstd":
            try:
                decompressor = zstd.ZstdDecompressor()
                return decompressor.decompress(compressed_bytes)
            except Exception as e_zstd_decompr: # zstd.ZstdError
                logger.error(f"Zstd decompression error: {e_zstd_decompr}. Returning raw (possibly compressed) bytes.")
                return compressed_bytes # Return original on error, might be corrupted
        else:
            logger.warning(f"Unsupported decompression type: '{compression_type_str}'. Returning raw (possibly compressed) bytes.")
            return compressed_bytes


class BaseStorageManager: # Common logic for NVMeManager and RAMDiskManager
    def __init__(self, storage_tier_type: str, storage_path_str: Optional[str], total_usable_bytes_val: int,
                 app_cfg: Dict, quant_mgr_inst: QuantizationManager, compress_mgr_inst: CompressionManager):
        self.storage_type = storage_tier_type # "nvme" or "ramdisk"
        self.path: Optional[Path] = Path(storage_path_str).resolve() if storage_path_str else None # Resolved absolute path
        self.total_usable_bytes = total_usable_bytes_val # From hardware_summary, for this tier
        self.app_config = app_cfg
        self.quant_manager = quant_mgr_inst
        self.compress_manager = compress_mgr_inst
        # Health stats (reads, writes, errors, etc.) are stored in global hardware_summary[self.storage_type]["health_stats"]
        
        if self.path: # Ensure base directory for this storage tier exists
            try:
                self.path.mkdir(parents=True, exist_ok=True)
            except OSError as e_mkdir: # Handle potential permission errors or other OS issues
                logger.error(f"Failed to create directory for {self.storage_type} at '{self.path}': {e_mkdir}. This tier may be unusable.")
                self.path = None # Mark path as invalid if creation fails
                hardware_summary[self.storage_type]["available"] = False # Mark tier as unavailable
        
        if self.path:
            logger.info(f"{storage_tier_type.upper()}Manager target path: {self.path}, Initial Usable Bytes: {self.total_usable_bytes//(1024**2)}MB")
        else:
            logger.warning(f"{storage_tier_type.upper()}Manager path is not configured or invalid. This tier will be disabled.")


    def _get_layer_file_path_on_storage(self, layer_meta: LayerMetadata) -> Optional[Path]: # Generates filename within this storage tier
        if not self.path: return None # Storage not configured or path invalid
        # Sanitize layer name for use in filename
        safe_layer_name = "".join(c if c.isalnum() or c in ['_','-','.'] else '_' for c in layer_meta.name)
        max_name_len = 60 # Keep filenames reasonably short for various filesystems
        if len(safe_layer_name) > max_name_len: safe_layer_name = safe_layer_name[:max_name_len] + "_trunc"
        # Filename structure: layer_<id>_<safe_name>.tensorbin
        return self.path / f"layer_{layer_meta.id}_{safe_layer_name}.tensorbin"

    def _convert_tensor_to_bytes(self, tensor_to_save: torch.Tensor) -> bytes: # Helper to serialize tensor
        bytes_buffer = io.BytesIO()
        # Always save tensor from CPU to bytes for consistent serialization format
        # The tensor_to_save might already be on CPU if coming from a full model load.
        # If it's on GPU, move it to CPU first.
        torch.save(tensor_to_save.cpu(), bytes_buffer) 
        return bytes_buffer.getvalue()

    def _convert_bytes_to_tensor(self, tensor_data_bytes: bytes) -> torch.Tensor: # Helper to deserialize tensor (loads to CPU)
        bytes_buffer = io.BytesIO(tensor_data_bytes)
        # Warning: torch.load can be insecure with untrusted data. Assume internal use with trusted data.
        loaded_tensor = torch.load(bytes_buffer, map_location="cpu") # Always load to CPU first
        return loaded_tensor

    def store_layer(self, tensor_to_store: torch.Tensor, layer_meta: LayerMetadata) -> bool:
        # Full process: quantize -> to_bytes -> compress -> write_to_disk -> update_metadata
        if not self.path: # Check if storage path is valid for this manager
            logger.error(f"L{layer_meta.id} ({layer_meta.name}) cannot be stored, {self.storage_type} path is not available.")
            hardware_summary[self.storage_type]["health_stats"]["errors"] += 1
            return False
            
        target_file_path = self._get_layer_file_path_on_storage(layer_meta)
        if not target_file_path: # Should not happen if self.path is valid, but defensive
            logger.error(f"L{layer_meta.id} ({layer_meta.name}) could not determine file path for {self.storage_type}.")
            hardware_summary[self.storage_type]["health_stats"]["errors"] += 1
            return False
        
        success_flag = False
        try:
            with layer_meta.get_lock(): # Ensure exclusive access to metadata during storage operation
                # 1. Quantize tensor based on target storage tier's policy
                quantized_tensor, q_type_str, q_new_dtype, q_params_d = \
                    self.quant_manager.quantize_tensor(tensor_to_store, layer_meta, self.storage_type)
                logger.debug(f"L{layer_meta.id} for {self.storage_type}: Quantized to {q_type_str or 'None'} ({q_new_dtype}), Orig Dtype: {tensor_to_store.dtype}")

                # 2. Convert (possibly quantized) tensor to bytes
                tensor_bytes_for_storage = self._convert_tensor_to_bytes(quantized_tensor)
                
                # 3. Compress these bytes
                compressed_final_bytes, compr_type_str = self.compress_manager.compress_bytes(tensor_bytes_for_storage)
                logger.debug(f"L{layer_meta.id} for {self.storage_type}: Compressed to {compr_type_str or 'None'}. Size: {len(tensor_bytes_for_storage)}B -> {len(compressed_final_bytes)}B.")

                # 4. Write compressed bytes to file on this storage tier
                with open(target_file_path, "wb") as f_out:
                    f_out.write(compressed_final_bytes)
                
                # 5. Update LayerMetadata with new state reflecting storage on this tier
                layer_meta.current_size_bytes = len(compressed_final_bytes) # Actual size on disk
                layer_meta.compression_type = compr_type_str
                layer_meta.quantization_type = q_type_str
                layer_meta.current_dtype = q_new_dtype # Dtype of the tensor *before* byte conversion (e.g. torch.int8)
                layer_meta.quantization_params = q_params_d
                layer_meta.set_location(self.storage_type, path_val=str(target_file_path)) # Updates device and specific path
                success_flag = True

            if success_flag: # Update health stats outside lock, after op completes
                hardware_summary[self.storage_type]["health_stats"]["writes"] += 1
                hardware_summary[self.storage_type]["health_stats"]["bytes_written"] += len(compressed_final_bytes)
                if self.path: hardware_summary[self.storage_type]["health_stats"]["files"] = len(list(self.path.iterdir())) # Approx file count
                logger.info(f"L{layer_meta.id} ('{layer_meta.name[:30]}...') stored to {self.storage_type} at {target_file_path}. Size: {len(compressed_final_bytes)/(1024**2):.2f}MB. Q:{q_type_str or 'N'}, C:{compr_type_str or 'N'}")
            return success_flag
        except Exception as e_store_layer:
            logger.error(f"Error storing L{layer_meta.id} ('{layer_meta.name}') to {self.storage_type} at {target_file_path}: {e_store_layer}", exc_info=True)
            hardware_summary[self.storage_type]["health_stats"]["errors"] += 1
            if target_file_path and target_file_path.exists(): # Attempt to clean up partial file if write failed
                try: target_file_path.unlink()
                except Exception as e_del_partial_store: logger.error(f"Failed to delete partial file '{target_file_path}' after store error: {e_del_partial_store}")
            return False

    def load_layer(self, layer_meta: LayerMetadata) -> Optional[torch.Tensor]: # Loads to CPU, dequantized to original_dtype
        if not self.path: # Check if storage path is valid for this manager
            logger.error(f"L{layer_meta.id} ({layer_meta.name}) cannot be loaded, {self.storage_type} path is not available.")
            hardware_summary[self.storage_type]["health_stats"]["errors"] += 1
            return None

        if layer_meta.current_device != self.storage_type: # Should be called only if layer is on this tier
            logger.error(f"L{layer_meta.id} current_device ('{layer_meta.current_device}') mismatch for {self.storage_type} manager load op. Expected layer to be on {self.storage_type}.")
            hardware_summary[self.storage_type]["health_stats"]["errors"] += 1
            return None
        
        file_path_str_from_meta = layer_meta.nvme_path if self.storage_type == "nvme" else layer_meta.ramdisk_path
        if not file_path_str_from_meta:
            logger.error(f"L{layer_meta.id} ('{layer_meta.name}') has no valid file path set in metadata for {self.storage_type} storage.")
            hardware_summary[self.storage_type]["health_stats"]["errors"] += 1
            return None
        
        actual_file_path = Path(file_path_str_from_meta)
        if not actual_file_path.exists() or not actual_file_path.is_file():
            logger.error(f"File not found for L{layer_meta.id} ('{layer_meta.name}') at expected path '{actual_file_path}' in {self.storage_type}")
            hardware_summary[self.storage_type]["health_stats"]["errors"] += 1
            # Consider attempting to clear the invalid path from metadata if file consistently not found
            # with layer_meta.get_lock():
            #    if self.storage_type == "nvme": layer_meta.nvme_path = None
            #    else: layer_meta.ramdisk_path = None
            return None
        
        try:
            with open(actual_file_path, "rb") as f_in:
                compressed_bytes_from_disk = f_in.read()
            
            hardware_summary[self.storage_type]["health_stats"]["reads"] += 1
            hardware_summary[self.storage_type]["health_stats"]["bytes_read"] += len(compressed_bytes_from_disk)

            raw_tensor_bytes = self.compress_manager.decompress_bytes(compressed_bytes_from_disk, layer_meta.compression_type)
            loaded_quantized_tensor_cpu = self._convert_bytes_to_tensor(raw_tensor_bytes) # Loads to CPU
            logger.debug(f"L{layer_meta.id} loaded from {self.storage_type}, pre-dequant dtype: {loaded_quantized_tensor_cpu.dtype}, Q-type in meta: {layer_meta.quantization_type}")
            
            final_dequantized_tensor_cpu = self.quant_manager.dequantize_tensor(
                loaded_quantized_tensor_cpu, layer_meta.quantization_type,
                layer_meta.original_dtype, layer_meta.quantization_params
            )
            logger.debug(f"L{layer_meta.id} dequantized to {final_dequantized_tensor_cpu.dtype} (original target).")

            layer_meta.update_access_stats() # Mark as accessed
            logger.info(f"L{layer_meta.id} ('{layer_meta.name[:30]}...') loaded from {self.storage_type} (Path: {actual_file_path}). Restored to {layer_meta.original_dtype}.")
            return final_dequantized_tensor_cpu # Returns dequantized tensor on CPU

        except Exception as e_load_layer:
            logger.error(f"Error loading L{layer_meta.id} ('{layer_meta.name}') from {self.storage_type} at {actual_file_path}: {e_load_layer}", exc_info=True)
            hardware_summary[self.storage_type]["health_stats"]["errors"] += 1
            return None

    def delete_layer(self, layer_meta: LayerMetadata) -> bool: # Deletes layer file from this storage tier
        if not self.path: return False # Storage path not valid

        if layer_meta.current_device != self.storage_type: # This manager only deletes if layer is marked as on its tier
            logger.debug(f"Delete requested for L{layer_meta.id} from {self.storage_type}, but layer is on {layer_meta.current_device}. Skipping.")
            return False 
        
        file_path_str_from_meta = layer_meta.nvme_path if self.storage_type == "nvme" else layer_meta.ramdisk_path
        if not file_path_str_from_meta:
            logger.warning(f"L{layer_meta.id} marked as on {self.storage_type}, but has no path. Cannot delete from this tier.")
            return False 
        
        actual_file_path = Path(file_path_str_from_meta)
        if actual_file_path.exists() and actual_file_path.is_file():
            try:
                file_size_bytes = actual_file_path.stat().st_size # For logging
                actual_file_path.unlink() # Delete the file
                logger.info(f"Deleted L{layer_meta.id} ('{layer_meta.name}') file ({file_size_bytes}B) from {self.storage_type} at {actual_file_path}")
                if self.path: hardware_summary[self.storage_type]["health_stats"]["files"] = len(list(self.path.iterdir())) # Update file count
                
                with layer_meta.get_lock(): # Clear path from metadata after successful deletion
                    if self.storage_type == "nvme" and layer_meta.nvme_path == str(actual_file_path):
                        layer_meta.nvme_path = None
                    elif self.storage_type == "ramdisk" and layer_meta.ramdisk_path == str(actual_file_path):
                        layer_meta.ramdisk_path = None
                    # The caller (e.g., transfer worker) is responsible for updating layer_meta.current_device
                    # if the layer is no longer on any persistent storage after this deletion.
                return True
            except Exception as e_delete_file:
                logger.error(f"Error deleting L{layer_meta.id} from {self.storage_type} at {actual_file_path}: {e_delete_file}")
                hardware_summary[self.storage_type]["health_stats"]["errors"] += 1
        else: # File not found at expected path
            logger.warning(f"Attempted to delete L{layer_meta.id} from {self.storage_type}, but file not found at '{actual_file_path}'. Path in metadata might be stale.")
            # Clear stale path from metadata if file is missing
            with layer_meta.get_lock():
                if self.storage_type == "nvme" and layer_meta.nvme_path == str(actual_file_path): layer_meta.nvme_path = None
                elif self.storage_type == "ramdisk" and layer_meta.ramdisk_path == str(actual_file_path): layer_meta.ramdisk_path = None
        return False

    def get_storage_tier_health(self) -> Dict: # Returns health and usage stats for API
        # Dynamic free space is already updated by HardwareManager's monitor thread in global hardware_summary
        # This function primarily formats the data from hardware_summary for this tier.
        tier_hw_summary = hardware_summary.get(self.storage_type, {})
        tier_health_stats = tier_hw_summary.get("health_stats", {}).copy() # Get a copy of health stats

        total_planned_b = self.total_usable_bytes # From initial capacity planning (server_usable_bytes_limit)
        current_free_fs_b = tier_hw_summary.get("filesystem_free_bytes_current", 0) # Dynamic free space on the filesystem

        # Calculate % used based on planned server usage vs. current free space *relative to that plan*
        # This is a bit tricky. A simpler metric is just % of FS used.
        # For now, let's show % used of the *entire filesystem* if possible,
        # or % used of *planned server allocation* if FS total isn't easily available here.
        
        percent_used = 0
        if tier_hw_summary.get("filesystem_total_bytes", 0) > 0:
            percent_used = round((1 - (current_free_fs_b / tier_hw_summary["filesystem_total_bytes"]))*100, 1)
        elif total_planned_b > 0: # Fallback to planned if FS total unknown
             # This requires knowing actual used by server, not just free FS space
             # For simplicity, this part is complex to calculate accurately here without iterating all files.
             # The monitor thread gives a rough idea of free space on RAMDisk via file summation.
             pass


        return {
            "storage_tier_type": self.storage_type,
            "configured_path": str(self.path) if self.path else "N/A",
            "is_available": tier_hw_summary.get("available", False),
            "server_usable_bytes_limit": total_planned_b, # Max server plans to use
            "filesystem_total_bytes": tier_hw_summary.get("filesystem_total_bytes"), # Total size of underlying FS
            "filesystem_free_bytes_current": current_free_fs_b, # Current free on FS
            "filesystem_percent_used": percent_used, # % of FS used
            **tier_health_stats # reads, writes, files, errors for this tier
        }

class NVMEManager(BaseStorageManager): # NVMe-specific storage handling
    def __init__(self, app_cfg: Dict, quant_mgr_inst: QuantizationManager, compress_mgr_inst: CompressionManager):
        nvme_storage_path = hardware_summary["nvme"].get("path") # Path to .../ultra_tensor_server_layers subdir
        nvme_usable_b = hardware_summary["nvme"].get("server_usable_bytes_limit", 0)
        super().__init__("nvme", nvme_storage_path, nvme_usable_b, app_cfg, quant_mgr_inst, compress_mgr_inst)
        # NVMe-specific stripe size for metadata. Actual striped I/O (reading parts of files) is complex
        # and not fully implemented here for general layer load/store. Layers are stored as single files.
        # This config could be used if implementing partial layer loading from NVMe.
        self.stripe_size_config_bytes = self._cfg_path_get("hardware","nvme","stripe_size_mb", default_val=MIN_NVME_STRIPE_SIZE_MB) * 1024*1024
        logger.info(f"NVMEManager using base path: {self.path}. Stripe size config: {self.stripe_size_config_bytes // (1024*1024)}MB (for metadata, not I/O splitting).")

class RAMDiskManager(BaseStorageManager): # RAMDisk-specific storage handling
    def __init__(self, app_cfg: Dict, quant_mgr_inst: QuantizationManager, compress_mgr_inst: CompressionManager):
        ramdisk_storage_path = hardware_summary["ramdisk"].get("path") # Path to .../ultra_tensor_server_layers subdir
        ramdisk_usable_b = hardware_summary["ramdisk"].get("usable_bytes_for_server", 0)
        super().__init__("ramdisk", ramdisk_storage_path, ramdisk_usable_b, app_cfg, quant_mgr_inst, compress_mgr_inst)
        logger.info(f"RAMDiskManager using base path: {self.path}.")

# MemoryPlacementManager (Refined version with rule-based placement)
class MemoryPlacementManager:
    def __init__(self, app_cfg: Dict):
        self.app_config = app_cfg
        self.placement_strategy_config = self.app_config.get("optimization", {}).get("placement_strategy", {})
        self.tier_capacity_in_layers: Dict[str, int] = {} # e.g. "gpu_total", "cpu", "ramdisk", "nvme"
        self.tier_fill_count_current_model: Dict[str, int] = {} # Tracks layers assigned for current model load
        self.gpu_specific_fill_current_model: Dict[int, int] = {} # gpu_id -> layers assigned
        self.gpu_specific_capacity_in_layers: Dict[int, int] = {} # gpu_id -> estimated layer capacity
        logger.info("MemoryPlacementManager initialized (with rule-based strategy).")

    def _cfg_path_get(self, *keys_tuple: str, default_val: Any = None) -> Any: # Helper
        data_node = self.app_config
        for key_item in keys_tuple:
            if isinstance(data_node, dict) and key_item in data_node: data_node = data_node[key_item]
            else: return default_val
        return data_node

    def calculate_tier_capacity_for_model(self, total_model_layers_count: int, average_layer_size_fp32_b: int):
        logger.info(f"Calculating tier capacity for model with ~{total_model_layers_count} layers, avg FP32 layer size {average_layer_size_fp32_b/(1024**2):.2f}MB.")
        self.tier_capacity_in_layers = {"gpu_total":0, "cpu":0, "ramdisk":0, "nvme":0, "unallocated":0}
        self.gpu_specific_capacity_in_layers = {dev_info["id"]: 0 for dev_info in hardware_summary["gpu"].get("devices", [])}
        self.reset_tier_fill_counts_for_new_model()

        temp_quant_mgr = QuantizationManager(self.app_config) # For estimating quantized sizes on tiers

        def get_estimated_layer_size_on_tier_b(tier_type_str: str) -> int:
            mock_meta = LayerMetadata(0, "mock_placement_sizing_layer", average_layer_size_fp32_b, (1,), torch.float32, "cpu")
            target_precision = temp_quant_mgr._get_target_precision_for_tier(mock_meta, tier_type_str)
            size_factor = 1.0
            if target_precision == "fp16" or target_precision == "bf16": size_factor = 0.5
            elif target_precision == "int8": size_factor = 0.25
            elif target_precision == "int4": size_factor = 0.125
            est_size = int(average_layer_size_fp32_b * size_factor)
            return est_size if est_size > 0 else average_layer_size_fp32_b # Avoid zero

        if hardware_summary["gpu"]["available"] and hardware_summary["gpu"]["devices"]:
            for gpu_dev_info in hardware_summary["gpu"]["devices"]:
                est_layer_size_on_gpu_b = get_estimated_layer_size_on_tier_b("gpu")
                gpu_id_val = gpu_dev_info["id"]
                layers_on_this_gpu = gpu_dev_info["usable_memory_bytes_planning"] // est_layer_size_on_gpu_b if est_layer_size_on_gpu_b > 0 else 0
                self.gpu_specific_capacity_in_layers[gpu_id_val] = layers_on_this_gpu
                self.tier_capacity_in_layers["gpu_total"] += layers_on_this_gpu
        
        if hardware_summary["ram"]["usable_bytes_for_planning"] > 0:
            est_layer_size_on_cpu_b = get_estimated_layer_size_on_tier_b("cpu")
            self.tier_capacity_in_layers["cpu"] = hardware_summary["ram"]["usable_bytes_for_planning"] // est_layer_size_on_cpu_b if est_layer_size_on_cpu_b > 0 else 0

        if hardware_summary["ramdisk"]["available"] and hardware_summary["ramdisk"]["usable_bytes_for_server"] > 0:
            est_layer_size_on_ramdisk_b = get_estimated_layer_size_on_tier_b("ramdisk")
            self.tier_capacity_in_layers["ramdisk"] = hardware_summary["ramdisk"]["usable_bytes_for_server"] // est_layer_size_on_ramdisk_b if est_layer_size_on_ramdisk_b > 0 else 0

        if hardware_summary["nvme"]["available"] and hardware_summary["nvme"]["server_usable_bytes_limit"] > 0:
            est_layer_size_on_nvme_b = get_estimated_layer_size_on_tier_b("nvme")
            self.tier_capacity_in_layers["nvme"] = hardware_summary["nvme"]["server_usable_bytes_limit"] // est_layer_size_on_nvme_b if est_layer_size_on_nvme_b > 0 else 0
        
        logger.info(f"Calculated Tier Capacities (layers): GPU_Total={self.tier_capacity_in_layers['gpu_total']} (Per GPU: {self.gpu_specific_capacity_in_layers}), CPU(RAM)={self.tier_capacity_in_layers['cpu']}, RAMDisk={self.tier_capacity_in_layers['ramdisk']}, NVMe={self.tier_capacity_in_layers['nvme']}")

    def get_initial_placement_for_layer(self, layer_id_val: int, layer_name_str: str) -> Tuple[str, Optional[int]]:
        layer_name_lower_case = layer_name_str.lower()
        placement_rules_cfg = self.placement_strategy_config.get("rules", [])
        sorted_rules = sorted([r for r in placement_rules_cfg if "pattern" in r and "tier" in r], key=lambda x: x.get("priority", 99))

        for rule in sorted_rules:
            if rule["pattern"] in layer_name_lower_case:
                target_tier_rule = rule["tier"].lower()
                if target_tier_rule == "gpu":
                    pref_gpu_id_rule = rule.get("gpu_id")
                    gpu_to_target_idx: Optional[int] = None
                    if pref_gpu_id_rule is not None and pref_gpu_id_rule in self.gpu_specific_capacity_in_layers and \
                       self.gpu_specific_fill_current_model.get(pref_gpu_id_rule, 0) < self.gpu_specific_capacity_in_layers[pref_gpu_id_rule]:
                        gpu_to_target_idx = pref_gpu_id_rule
                    else:
                        avail_gpus_sorted = sorted(
                            [gid for gid, cap in self.gpu_specific_capacity_in_layers.items() if self.gpu_specific_fill_current_model.get(gid,0) < cap],
                            key=lambda gid_key: self.gpu_specific_capacity_in_layers[gid_key] - self.gpu_specific_fill_current_model.get(gid_key,0),
                            reverse=True)
                        if avail_gpus_sorted: gpu_to_target_idx = avail_gpus_sorted[0]
                    if gpu_to_target_idx is not None:
                        self.gpu_specific_fill_current_model[gpu_to_target_idx] = self.gpu_specific_fill_current_model.get(gpu_to_target_idx, 0) + 1
                        self.tier_fill_count_current_model["gpu_total"] += 1
                        logger.debug(f"L{layer_id_val} ('{layer_name_str}') to GPU {gpu_to_target_idx} by rule: '{rule['pattern']}'")
                        return "gpu", gpu_to_target_idx
                elif target_tier_rule == "cpu" and self.tier_fill_count_current_model["cpu"] < self.tier_capacity_in_layers.get("cpu", 0):
                    self.tier_fill_count_current_model["cpu"] += 1; logger.debug(f"L{layer_id_val} ('{layer_name_str}') to CPU by rule: '{rule['pattern']}'"); return "cpu", None
                elif target_tier_rule == "ramdisk" and hardware_summary["ramdisk"]["available"] and self.tier_fill_count_current_model["ramdisk"] < self.tier_capacity_in_layers.get("ramdisk", 0):
                    self.tier_fill_count_current_model["ramdisk"] += 1; logger.debug(f"L{layer_id_val} ('{layer_name_str}') to RAMDisk by rule: '{rule['pattern']}'"); return "ramdisk", None
                logger.debug(f"Rule '{rule['pattern']}' matched tier '{target_tier_rule}' for L'{layer_name_str}', but tier full/unavailable. Falling to default.")

        if hardware_summary["gpu"]["available"]: # Default Greedy GPU
            avail_gpus_greedy = sorted([gid for gid,cap in self.gpu_specific_capacity_in_layers.items() if self.gpu_specific_fill_current_model.get(gid,0) < cap], key=lambda g: self.gpu_specific_capacity_in_layers[g] - self.gpu_specific_fill_current_model.get(g,0), reverse=True)
            if avail_gpus_greedy:
                gpu_idx_greedy = avail_gpus_greedy[0]
                self.gpu_specific_fill_current_model[gpu_idx_greedy] = self.gpu_specific_fill_current_model.get(gpu_idx_greedy, 0) + 1
                self.tier_fill_count_current_model["gpu_total"] += 1; return "gpu", gpu_idx_greedy
        
        if self.tier_fill_count_current_model["cpu"] < self.tier_capacity_in_layers.get("cpu",0): self.tier_fill_count_current_model["cpu"] += 1; return "cpu", None
        if hardware_summary["ramdisk"]["available"] and self.tier_fill_count_current_model["ramdisk"] < self.tier_capacity_in_layers.get("ramdisk",0): self.tier_fill_count_current_model["ramdisk"] += 1; return "ramdisk", None
        if hardware_summary["nvme"]["available"] and self.tier_fill_count_current_model["nvme"] < self.tier_capacity_in_layers.get("nvme",0): self.tier_fill_count_current_model["nvme"] += 1; return "nvme", None
        
        self.tier_capacity_in_layers["unallocated"] = self.tier_capacity_in_layers.get("unallocated",0) + 1
        logger.warning(f"All placement tiers full for L{layer_id_val} ('{layer_name_str}'). Defaulting to CPU (RAM cache).")
        return "cpu", None

    def reset_tier_fill_counts_for_new_model(self):
        self.tier_fill_count_current_model = {"gpu_total":0, "cpu":0, "ramdisk":0, "nvme":0}
        self.gpu_specific_fill_current_model = {dev_info["id"]: 0 for dev_info in hardware_summary["gpu"].get("devices", [])}
        logger.debug("Tier fill counts reset for new model loading.")

# ModelManagerService (Handles loading, unloading, and managing model layers)
# InferenceService (Handles text generation requests)
# Worker Loops (layer_transfer_worker_loop, prefetch_worker_loop)
# FastAPI app setup (app instance, endpoints, startup/shutdown events)
# __main__ block for running Uvicorn

# Due to the extreme length, these sections will be provided in the next segment
# if you confirm you want to continue this way. This part completes the foundational
# managers up to MemoryPlacementManager.
# [Code from the previous response, including LayerMetadata, TransferRequest, LoadedModelContext,
#  HardwareManager, QuantizationManager, CompressionManager, BaseStorageManager, NVMEManager,
#  RAMDiskManager, and the refined MemoryPlacementManager class, should be above this line]

# --- ModelManagerService (Handles loading, unloading, and managing model layers) ---
class ModelManagerService:
    def __init__(self, app_cfg: Dict, placement_mgr_inst: MemoryPlacementManager,
                 nvme_mgr_inst: Optional[NVMEManager], ramdisk_mgr_inst: Optional[RAMDiskManager],
                 quant_mgr_inst: QuantizationManager, compress_mgr_inst: CompressionManager):
        self.app_config = app_cfg
        self.placement_manager = placement_mgr_inst
        self.nvme_manager = nvme_mgr_inst
        self.ramdisk_manager = ramdisk_mgr_inst
        self.quant_manager = quant_mgr_inst # Passed to StorageManagers
        self.compress_manager = compress_mgr_inst # Passed to StorageManagers

        self.loaded_model_context: Optional[LoadedModelContext] = None # Holds the currently active model's context
        self._model_operation_lock = asyncio.Lock() # Async lock for model load/unload operations
        self.current_model_status_str: str = "no_model_loaded" # Tracks operational status
        self.current_model_error_msg: Optional[str] = None # Stores error message if status is "error"
        
        self.next_global_layer_id: int = 0 # Counter for unique layer IDs across all models
        self.hf_api_client = HfApi() # Hugging Face API client for Hub interactions
        logger.info("ModelManagerService initialized.")

    def get_current_model_status(self) -> ModelStatusAPIResponse: # For API status reporting
        # This method should be called with _model_operation_lock held if mutating status,
        # or be careful about race conditions if called without lock from multiple places.
        # For read-only status, direct access is fine if updates are atomic enough.
        if self.loaded_model_context:
            return self.loaded_model_context.to_status_api_model(self.current_model_status_str, self.current_model_error_msg)
        return ModelStatusAPIResponse(status=self.current_model_status_str, error_message=self.current_model_error_msg)

    def _resolve_model_path_and_load_configs(self, model_identifier_str: str, hf_auth_token_str: Optional[str]) -> Tuple[str, AutoConfig, AutoTokenizer]:
        # Resolves model (local path or Hub ID) to a local path, downloads if necessary, and loads HF config/tokenizer.
        target_model_path_obj = Path(model_identifier_str)
        # Use provided token, or token from config, or environment variable as fallback for HF Hub
        token_for_hf_hub = hf_auth_token_str or self._cfg_path_get("huggingface", "default_token") or os.getenv("HF_TOKEN")

        if target_model_path_obj.is_dir(): # If it's a local directory path
            logger.info(f"Using local model from path: {model_identifier_str}")
            resolved_local_model_path = str(target_model_path_obj.resolve())
        else: # Assumed to be a Hugging Face Hub model ID
            logger.info(f"Model '{model_identifier_str}' not found as local path. Attempting download from Hugging Face Hub.")
            # Get cache directory from config, or use a default
            hf_cache_dir_path = Path(self._cfg_path_get("huggingface", "cache_dir", default_val=DEFAULT_MODEL_CACHE_DIR)).resolve()
            hf_cache_dir_path.mkdir(parents=True, exist_ok=True) # Ensure cache directory exists
            try:
                # snapshot_download downloads all files for the model repository.
                # For very large models or specific file needs, hf_hub_download could be used per file.
                resolved_local_model_path = snapshot_download(
                    repo_id=model_identifier_str,
                    cache_dir=hf_cache_dir_path,
                    token=token_for_hf_hub,
                    resume_download=True, # Allow resuming interrupted downloads
                    # allow_patterns=["*.json", "*.safetensors", "*.bin", "tokenizer*", "generation_config.json"], # Optional: filter downloaded files
                    ignore_patterns=["*.gitattributes", "*.md", ".git*"], # Optional: ignore certain files
                )
                logger.info(f"Model '{model_identifier_str}' downloaded/cached at local path: {resolved_local_model_path}")
            except Exception as e_hf_download:
                logger.error(f"Failed to download model '{model_identifier_str}' from Hugging Face Hub: {e_hf_download}", exc_info=True)
                raise ValueError(f"Download failed for model '{model_identifier_str}'") from e_hf_download
        
        # Load Hugging Face AutoConfig and AutoTokenizer from the resolved local path
        try:
            model_config_obj = AutoConfig.from_pretrained(resolved_local_model_path, token=token_for_hf_hub, trust_remote_code=self._cfg_path_get("huggingface","trust_remote_code", default_val=False))
            tokenizer_obj = AutoTokenizer.from_pretrained(resolved_local_model_path, token=token_for_hf_hub, trust_remote_code=self._cfg_path_get("huggingface","trust_remote_code", default_val=False))
            return resolved_local_model_path, model_config_obj, tokenizer_obj
        except Exception as e_load_hf_cfg:
            logger.error(f"Failed to load AutoConfig/AutoTokenizer from path '{resolved_local_model_path}' for model '{model_identifier_str}': {e_load_hf_cfg}", exc_info=True)
            raise ValueError(f"Config/Tokenizer loading failed for '{model_identifier_str}'") from e_load_hf_cfg


    async def load_model(self, model_id_str: str, preferred_compute_device_str: Optional[str] = None, hf_auth_token: Optional[str] = None):
        # Main orchestrator for loading a model. This is an async function for FastAPI background tasks.
        async with self._model_operation_lock: # Ensure only one load/unload operation at a time
            if self.loaded_model_context and self.current_model_status_str == "loaded":
                self.current_model_error_msg = f"Model '{self.loaded_model_context.name}' is already loaded. Unload it first via /api/models/unload."
                logger.error(self.current_model_error_msg)
                # Do not change status string here, it should correctly reflect "loaded" or "error" from previous op.
                return # Exit if a model is already successfully loaded

            if self.current_model_status_str == "loading": # Prevent concurrent loads
                 logger.warning(f"A model loading operation is already in progress. Ignoring request to load '{model_id_str}'.")
                 return

            self.current_model_status_str = "loading" # Set status for API reporting
            self.current_model_error_msg = None # Clear previous errors
            logger.info(f"Initiating model load sequence for: '{model_id_str}'")
            
            newly_created_layer_ids_this_load: List[int] = [] # Track layers created in this attempt for cleanup on failure

            try:
                # Step 1: Resolve model path (download if needed) and load HF config/tokenizer
                resolved_path, hf_model_cfg, hf_tokenizer_inst = self._resolve_model_path_and_load_configs(model_id_str, hf_auth_token)

                # Step 2: Determine the primary compute device for this model instance
                final_target_compute_device: torch.device
                if preferred_compute_device_str:
                    final_target_compute_device = torch.device(preferred_compute_device_str)
                elif hardware_summary["gpu"]["available"] and hardware_summary["gpu"]["devices"]: # Default to first available GPU
                    first_gpu_id = hardware_summary['gpu']['devices'][0]['id']
                    final_target_compute_device = torch.device(f"cuda:{first_gpu_id}")
                else: # Fallback to CPU
                    final_target_compute_device = torch.device("cpu")
                logger.info(f"Target primary compute device for model '{model_id_str}' set to: {final_target_compute_device}")

                # Step 3: Load the full model onto CPU initially to inspect parameters and then distribute them.
                # This is memory-intensive for CPU for very large models.
                # Advanced alternative: Use HuggingFace Accelerate's init_empty_weights and load_checkpoint_and_dispatch
                # to load layers one by one directly to target devices/storage without full CPU staging.
                # For this script's scope, we use CPU staging for simpler layer management logic.
                logger.info(f"Loading model weights for '{model_id_str}' to CPU for layer processing (this may take time and RAM)...")
                # Load in float32 for consistent original_size calculation and as input to quantization
                # Ensure trust_remote_code is passed if model requires it
                trust_rc_val = self._cfg_path_get("huggingface","trust_remote_code", default_val=False)
                temp_full_model_on_cpu = AutoModelForCausalLM.from_pretrained(
                    resolved_path, config=hf_model_cfg, torch_dtype=torch.float32, token=hf_auth_token, trust_remote_code=trust_rc_val
                ).cpu() # Ensure it's on CPU and in float32
                
                # Step 4: Prepare MemoryPlacementManager by calculating tier capacities for this model's profile
                num_params_in_loaded_model = len(list(temp_full_model_on_cpu.named_parameters()))
                avg_layer_size_fp32_b = 0
                if num_params_in_loaded_model > 0:
                    total_model_size_fp32_b = sum(p_tensor.numel() * p_tensor.element_size() for p_tensor in temp_full_model_on_cpu.parameters())
                    avg_layer_size_fp32_b = total_model_size_fp32_b // num_params_in_loaded_model
                
                self.placement_manager.calculate_tier_capacity_for_model(num_params_in_loaded_model, avg_layer_size_fp32_b)
                # Fill counts are reset within calculate_tier_capacity_for_model via reset_tier_fill_counts_for_new_model()

                # Step 5: Iterate through model parameters (layers), create metadata, and place/store them
                for layer_orig_name, layer_param_tensor_cpu in temp_full_model_on_cpu.named_parameters():
                    current_layer_global_id = self.next_global_layer_id
                    self.next_global_layer_id += 1 # Increment for next layer
                    newly_created_layer_ids_this_load.append(current_layer_global_id) # Track for cleanup

                    original_size_bytes_val = layer_param_tensor_cpu.numel() * layer_param_tensor_cpu.element_size() # Size in original FP32
                    
                    # Create initial LayerMetadata (current_device is CPU as tensor is on CPU)
                    layer_meta_obj = LayerMetadata(
                        layer_id=current_layer_global_id, name=layer_orig_name, original_size_bytes=original_size_bytes_val,
                        shape=tuple(layer_param_tensor_cpu.shape), dtype=layer_param_tensor_cpu.dtype, # original is float32
                        current_device="cpu" # Initial location before placement decision
                    )
                    layer_metadata_registry[current_layer_global_id] = layer_meta_obj # Add to global registry

                    # Get placement decision from MemoryPlacementManager
                    target_tier_type_str, target_gpu_idx_opt = self.placement_manager.get_initial_placement_for_layer(
                        current_layer_global_id, layer_orig_name
                    )
                    logger.debug(f"L{current_layer_global_id} ('{layer_orig_name}'), OrigSize:{original_size_bytes_val/(1024**2):.1f}MB. Initial placement target: {target_tier_type_str}" + (f":GPU_ID_{target_gpu_idx_opt}" if target_gpu_idx_opt is not None else ""))

                    # Place/Store the layer based on placement decision
                    # The layer_param_tensor_cpu is the original float32 tensor on CPU.
                    # Storage managers (NVMe/RAMDisk) will handle their own quantization/compression during store.
                    # For CPU/GPU cache, we quantize here then move/store.
                    if target_tier_type_str == "gpu":
                        if hardware_summary["gpu"]["available"] and target_gpu_idx_opt is not None and \
                           target_gpu_idx_opt < hardware_summary["gpu"]["count"]:
                            # Quantize for GPU tier first, then move to specified GPU
                            q_tensor_gpu, q_type_gpu, q_dtype_gpu, q_params_gpu = self.quant_manager.quantize_tensor(layer_param_tensor_cpu, layer_meta_obj, "gpu")
                            layer_cache[current_layer_global_id] = q_tensor_gpu.to(f"cuda:{target_gpu_idx_opt}") # Move to target GPU
                            layer_meta_obj.set_location("gpu", gpu_id_val=target_gpu_idx_opt)
                            layer_meta_obj.quantization_type = q_type_gpu; layer_meta_obj.current_dtype = q_dtype_gpu
                            layer_meta_obj.quantization_params = q_params_gpu; layer_meta_obj.current_size_bytes = q_tensor_gpu.numel() * q_tensor_gpu.element_size()
                        else: # Fallback to CPU cache if target GPU unavailable or misconfigured
                            logger.warning(f"Target GPU {target_gpu_idx_opt} for L{current_layer_global_id} ('{layer_orig_name}') not available. Placing in CPU cache (original FP32).")
                            layer_cache[current_layer_global_id] = layer_param_tensor_cpu # Store original FP32 tensor
                            layer_meta_obj.set_location("cpu") # Update metadata
                    
                    elif target_tier_type_str == "cpu": # Explicitly place in CPU RAM cache (layer_cache)
                        q_tensor_cpu, q_type_cpu, q_dtype_cpu, q_params_cpu = self.quant_manager.quantize_tensor(layer_param_tensor_cpu, layer_meta_obj, "cpu")
                        layer_cache[current_layer_global_id] = q_tensor_cpu # Already on CPU
                        layer_meta_obj.set_location("cpu")
                        layer_meta_obj.quantization_type = q_type_cpu; layer_meta_obj.current_dtype = q_dtype_cpu
                        layer_meta_obj.quantization_params = q_params_cpu; layer_meta_obj.current_size_bytes = q_tensor_cpu.numel() * q_tensor_cpu.element_size()
                    
                    elif target_tier_type_str == "ramdisk" and self.ramdisk_manager and self.ramdisk_manager.path:
                        if not self.ramdisk_manager.store_layer(layer_param_tensor_cpu, layer_meta_obj): # store_layer handles quant/compr internally
                             logger.error(f"Failed to store L{current_layer_global_id} ('{layer_orig_name}') to RAMDisk. Falling back to CPU cache.")
                             layer_cache[current_layer_global_id] = layer_param_tensor_cpu; layer_meta_obj.set_location("cpu")
                    
                    elif target_tier_type_str == "nvme" and self.nvme_manager and self.nvme_manager.path:
                        if not self.nvme_manager.store_layer(layer_param_tensor_cpu, layer_meta_obj): # store_layer handles quant/compr
                             logger.error(f"Failed to store L{current_layer_global_id} ('{layer_orig_name}') to NVMe. Falling back to CPU cache.")
                             layer_cache[current_layer_global_id] = layer_param_tensor_cpu; layer_meta_obj.set_location("cpu")
                    
                    else: # Fallback if target device type unknown or manager unavailable
                        logger.warning(f"Target tier '{target_tier_type_str}' for L{current_layer_global_id} ('{layer_orig_name}') not fully available or misconfigured. Placing in CPU cache (original FP32).")
                        layer_cache[current_layer_global_id] = layer_param_tensor_cpu
                        layer_meta_obj.set_location("cpu")
                    
                    layer_meta_obj.update_access_stats() # Initial access upon loading

                # Step 6: Free the large temporary model from CPU memory
                del temp_full_model_on_cpu
                gc.collect() # Explicit garbage collect

                # Step 7: Create and store the LoadedModelContext
                self.loaded_model_context = LoadedModelContext(
                    model_name_or_id=model_id_str, resolved_local_path=resolved_path, hf_config_obj=hf_model_cfg,
                    hf_tokenizer_obj=hf_tokenizer_inst, model_specific_layer_ids=newly_created_layer_ids_this_load,
                    target_primary_compute_device=final_target_compute_device, hf_api_auth_token=hf_auth_token
                )
                self.current_model_status_str = "loaded" # Update status to loaded
                total_params_m = sum(layer_metadata_registry[lid].original_size_bytes for lid in newly_created_layer_ids_this_load if layer_metadata_registry[lid].original_dtype == torch.float32) / (4 * 1e6) # Rough param count from FP32 size
                logger.info(f"Model '{model_id_str}' loaded successfully. ~{total_params_m:.2f}M parameters processed. Target compute device for inference: {final_target_compute_device}.")

            except Exception as e_load_model_main: # Catch-all for errors during the loading process
                self.current_model_status_str = "error" # Set error status
                self.current_model_error_msg = f"Failed to load model '{model_id_str}': {str(e_load_model_main)}"
                logger.error(self.current_model_error_msg, exc_info=True)
                # Perform extensive cleanup of any partially loaded state
                await self._cleanup_after_failed_load(newly_created_layer_ids_this_load)
                self.loaded_model_context = None # Ensure no partial context remains

    async def unload_model(self):
        # Unloads the currently active model and cleans up its resources.
        async with self._model_operation_lock: # Ensure exclusive operation
            if not self.loaded_model_context:
                logger.info("Unload model called, but no model is currently loaded. No action taken.")
                self.current_model_status_str = "no_model_loaded" # Ensure consistent state
                return

            logger.info(f"Initiating unload for model: '{self.loaded_model_context.name}'")
            self.current_model_status_str = "unloading" # Set status for API
            
            # Clear any temporary inference model instance associated with this context
            self.loaded_model_context.clear_temp_inference_resources()

            await self._cleanup_after_failed_load(self.loaded_model_context.layer_ids) # Re-use cleanup logic for all layers of this model

            # Reset model context and status
            unloaded_model_name = self.loaded_model_context.name
            self.loaded_model_context = None
            self.current_model_status_str = "no_model_loaded"
            self.current_model_error_msg = None
            
            # Global layer ID counter does not need to be reset, it's for unique IDs across server lifetime.
            # layer_metadata_registry, layer_cache, etc., are cleaned by _cleanup_after_failed_load.
            
            logger.info(f"Model '{unloaded_model_name}' unloaded successfully and resources cleaned up.")
            gc.collect() # Extra GC call after major resource release
            if torch.cuda.is_available(): torch.cuda.empty_cache() # Clear PyTorch CUDA cache

    async def _cleanup_after_failed_load(self, layer_ids_to_cleanup: List[int]):
        # Helper to clean up resources for a list of layer IDs (used on load failure or full unload)
        logger.warning(f"Performing cleanup for {len(layer_ids_to_cleanup)} layer IDs.")
        for layer_id_to_clean in layer_ids_to_cleanup:
            if layer_id_to_clean in layer_metadata_registry:
                meta_to_clean = layer_metadata_registry[layer_id_to_clean]
                logger.debug(f"Cleaning up L{layer_id_to_clean} ('{meta_to_clean.name}'), current device: {meta_to_clean.current_device}")
                # Delete from disk storage if applicable
                if meta_to_clean.current_device == "nvme" and self.nvme_manager:
                    self.nvme_manager.delete_layer(meta_to_clean)
                elif meta_to_clean.current_device == "ramdisk" and self.ramdisk_manager:
                    self.ramdisk_manager.delete_layer(meta_to_clean)
                
                # Remove from in-memory caches and registries
                if layer_id_to_clean in layer_cache: del layer_cache[layer_id_to_clean]
                if layer_id_to_clean in layer_access_stats: del layer_access_stats[layer_id_to_clean]
                if layer_id_to_clean in layer_locks: del layer_locks[layer_id_to_clean] # Be cautious if lock could be held elsewhere
                del layer_metadata_registry[layer_id_to_clean] # Finally, remove metadata
            else:
                logger.debug(f"L{layer_id_to_clean} requested for cleanup, but not found in metadata registry.")
        
        # After cleaning individual layers, clear relevant model-level caches if any.
        # For example, the placement manager's fill counts are reset before next model load.
        # self.placement_manager.reset_tier_fill_counts_for_new_model() # Done before load attempt usually

    def _cfg_path_get(self, *keys_tuple: str, default_val: Any = None) -> Any: # Helper from HardwareManager
        data_node = self.app_config
        for key_item in keys_tuple:
            if isinstance(data_node, dict) and key_item in data_node: data_node = data_node[key_item]
            else: return default_val
        return data_node

# InferenceService (Refined version with prefetch trigger and robust layer handling)
class InferenceService:
    def __init__(self, app_cfg: Dict, model_mgr_svc: ModelManagerService,
                 nvme_mgr_inst: Optional[NVMEManager], ramdisk_mgr_inst: Optional[RAMDiskManager],
                 quant_mgr_inst: QuantizationManager):
        self.app_config = app_cfg
        self.model_manager = model_mgr_svc
        self.nvme_manager = nvme_mgr_inst # Used if _ensure_layer... needs to load from NVMe
        self.ramdisk_manager = ramdisk_mgr_inst # Used if _ensure_layer... needs to load from RAMDisk
        self.quant_manager = quant_mgr_inst # Not directly used here, as layers from storage are dequantized by storage managers
        self.inference_global_lock = asyncio.Lock() # Ensures one generation call processes at a time globally for simplicity.
                                                 # Could be per-model if multiple models were supported simultaneously.
        self.prefetch_settings = self.app_config.get("optimization", {}).get("prefetching", {})
        logger.info("InferenceService initialized (with refined layer handling and prefetch trigger).")

    async def _ensure_layer_on_device(self, layer_id_val: int, target_compute_dev: torch.device,
                                      current_model_ctx: LoadedModelContext) -> Optional[torch.Tensor]:
        # Ensures a specific layer is loaded onto the target_compute_dev, fetching/moving it if necessary.
        # Returns the tensor on the target device, or None if failed.
        layer_meta_obj = current_model_ctx.get_layer_metadata(layer_id_val)
        if not layer_meta_obj:
            logger.error(f"L{layer_id_val} metadata not found during inference preparation for model '{current_model_ctx.name}'.")
            return None

        # Trigger prefetch for nearby layers if enabled (non-blocking)
        if self.prefetch_settings.get("enabled", False) and self.prefetch_settings.get("trigger_on_layer_ensure", True):
            try:
                # Put (accessed_layer_id, model_context) onto prefetch queue
                prefetch_job_queue.put_nowait((layer_id_val, current_model_ctx)) 
            except queue.Full:
                logger.warning(f"Prefetch job queue is full. Skipping prefetch trigger for L{layer_id_val}.")

        # 1. Check layer_cache (for layers in CPU or GPU RAM)
        if layer_id_val in layer_cache:
            cached_tensor_val = layer_cache[layer_id_val]
            cached_tensor_device_str = str(cached_tensor_val.device)
            target_device_str_val = str(target_compute_dev)

            # Check if already on the correct device (handles specific GPU indices)
            is_on_correct_target_gpu = cached_tensor_device_str.startswith("cuda") and \
                                       target_device_str_val.startswith("cuda") and \
                                       cached_tensor_val.device.index == target_compute_dev.index
            is_on_correct_target_cpu = cached_tensor_device_str == "cpu" and target_device_str_val == "cpu"

            if is_on_correct_target_gpu or is_on_correct_target_cpu: # Already on the correct target device
                layer_meta_obj.update_access_stats()
                logger.debug(f"L{layer_id_val} ('{layer_meta_obj.name}') found in cache on target device {target_device_str_val}.")
                return cached_tensor_val
            else: # In cache, but on the wrong device (e.g., on CPU, need GPU; or on GPU:0, need GPU:1)
                logger.info(f"L{layer_id_val} ('{layer_meta_obj.name}') in RAM cache ({cached_tensor_device_str}), needs move to {target_device_str_val} for inference.")
                try:
                    # Synchronous move for layer actively needed in inference path.
                    # A fully non-blocking system would queue this transfer and use futures/callbacks,
                    # significantly increasing complexity of the inference loop.
                    moved_tensor_val = cached_tensor_val.to(target_compute_dev)
                    layer_cache[layer_id_val] = moved_tensor_val # Update cache with tensor on new device
                    layer_meta_obj.set_location(target_device_str_val.split(":")[0], gpu_id_val=target_compute_dev.index if target_compute_dev.type=='cuda' else None)
                    layer_meta_obj.update_access_stats()
                    logger.debug(f"L{layer_id_val} successfully moved from {cached_tensor_device_str} to {target_device_str_val}.")
                    return moved_tensor_val
                except Exception as e_move_from_cache:
                    logger.error(f"Failed to move cached L{layer_id_val} from {cached_tensor_device_str} to {target_device_str_val}: {e_move_from_cache}. Will attempt full load from backing storage if available.")
                    # Fall through to attempt load from backing storage.
                    # Optionally, clear the problematic cache entry: if layer_id_val in layer_cache: del layer_cache[layer_id_val]
                    pass # Fall through to load from disk

        # 2. Not in usable cache (or move from cache failed), try loading from backing storage (NVMe/RAMDisk)
        logger.info(f"L{layer_id_val} ('{layer_meta_obj.name}') not in usable cache for {target_compute_dev}. Current location in metadata: {layer_meta_obj.current_device}. Attempting load from storage.")
        
        loaded_tensor_on_cpu: Optional[torch.Tensor] = None # Storage managers load to CPU first (dequantized)
        source_storage_tier_for_log = layer_meta_obj.current_device

        if layer_meta_obj.current_device == "nvme" and self.nvme_manager and self.nvme_manager.path:
            loaded_tensor_on_cpu = self.nvme_manager.load_layer(layer_meta_obj) # Returns dequantized tensor on CPU
        elif layer_meta_obj.current_device == "ramdisk" and self.ramdisk_manager and self.ramdisk_manager.path:
            loaded_tensor_on_cpu = self.ramdisk_manager.load_layer(layer_meta_obj) # Returns dequantized tensor on CPU
        elif layer_meta_obj.current_device == "cpu" or layer_meta_obj.current_device.startswith("gpu"):
            # This indicates an inconsistency: layer marked as in RAM/VRAM but wasn't found in layer_cache or couldn't be moved.
            # This could happen if cache was evicted externally or a previous operation failed.
            logger.error(f"L{layer_id_val} ('{layer_meta_obj.name}') metadata indicates it's on {layer_meta_obj.current_device}, but it was not found/movable from layer_cache. This suggests a system state inconsistency.")
            # Cannot reliably recover this state without knowing its actual persisted location (if any).
            # For robustness, one could check if nvme_path/ramdisk_path is still set in metadata and try loading from there,
            # but that implies the current_device field was wrong.
            return None # Fail to provide the layer

        if loaded_tensor_on_cpu is not None: # Successfully loaded from NVMe/RAMDisk to CPU
            logger.info(f"L{layer_id_val} ('{layer_meta_obj.name}') successfully loaded from {source_storage_tier_for_log} to CPU (dequantized to {loaded_tensor_on_cpu.dtype}).")
            try:
                # Move the CPU tensor to the final target_compute_dev (e.g., GPU)
                final_tensor_on_target_dev = loaded_tensor_on_cpu.to(target_compute_dev)
                # Add/update layer_cache with this tensor now on the target device
                layer_cache[layer_id_val] = final_tensor_on_target_dev 
                # Update metadata to reflect new location in active cache
                layer_meta_obj.set_location(str(target_compute_dev).split(":")[0], gpu_id_val=target_compute_dev.index if target_compute_dev.type=='cuda' else None)
                layer_meta_obj.update_access_stats()
                logger.debug(f"L{layer_id_val} moved to target compute device {target_compute_dev} and cached.")
                
                # Optional: Eviction policy from source storage after successful load to faster cache tier
                # e.g., if config.optimization.nvme.delete_after_load_to_cache is True for the source tier
                # This is not implemented here for simplicity; layers remain on disk until explicitly unloaded.
                return final_tensor_on_target_dev
            except Exception as e_move_to_target_dev:
                logger.error(f"Failed to move L{layer_id_val} (loaded from {source_storage_tier_for_log} to CPU) to target compute device {target_compute_dev}: {e_move_to_target_dev}", exc_info=True)
                # Tensor is on CPU (loaded_tensor_on_cpu), but couldn't move to target (e.g. GPU OOM).
                # As a fallback, add the CPU tensor to layer_cache if target was non-CPU.
                # If target *was* CPU, this path shouldn't be hit unless CPU itself is OOM for this tensor.
                layer_cache[layer_id_val] = loaded_tensor_on_cpu # Cache the CPU version
                layer_meta_obj.set_location("cpu") # Update location to CPU
                layer_meta_obj.update_access_stats()
                if str(target_compute_dev) == "cpu": return loaded_tensor_on_cpu # Target was CPU, so this is success
                return None # Failed to get to the intended non-CPU target device
        else: # Load from NVMe/RAMDisk failed
            logger.error(f"Failed to load L{layer_id_val} ('{layer_meta_obj.name}') from its designated storage tier ({layer_meta_obj.current_device}). Layer unavailable for inference.")
            # Could queue a high-priority recovery transfer request here if applicable and system is designed for it.
            return None # Layer could not be made available

    async def generate(self, generate_request: GenerateTextAPIRequest) -> GenerateTextAPIResponse:
        # Main entry point for text generation.
        async with self.inference_global_lock: # Ensure only one generation call at a time for this model context
            inference_start_time = time.monotonic()
            if not self.model_manager.loaded_model_context or self.model_manager.current_model_status_str != "loaded":
                logger.error("Generation request received but no model is currently loaded and ready.")
                raise HTTPException(status_code=409, detail="No model is currently loaded and ready for inference. Please load a model via /api/models/load.")
            
            current_loaded_model_ctx = self.model_manager.loaded_model_context
            target_inference_compute_dev = current_loaded_model_ctx.compute_device
            model_tokenizer = current_loaded_model_ctx.tokenizer

            logger.info(f"Generation request for model '{current_loaded_model_ctx.name}' on device '{target_inference_compute_dev}'. Prompt (first 70 chars): '{generate_request.prompt[:70]}...'")

            # Phase 1: Prepare a temporary, live model instance on the target compute device.
            # This involves ensuring all required layers are on the device and loading them into a model shell.
            
            # Clear any previous temporary inference model instance to ensure fresh state for this call
            # This is a simpler approach than trying to reuse/validate a cached model instance,
            # especially if layer states or compute device could change.
            if current_loaded_model_ctx._temp_inference_model_instance is not None:
                logger.debug("Clearing previously used temporary inference model instance and its layer state.")
                current_loaded_model_ctx.clear_temp_inference_resources()

            logger.info(f"Constructing temporary inference model instance for '{current_loaded_model_ctx.name}' on {target_inference_compute_dev}...")
            # Collect all necessary layer tensors onto the target compute device
            reconstructed_state_dict: Dict[str, torch.Tensor] = {}
            all_layers_successfully_prepared = True
            for layer_id_in_exec_order in current_loaded_model_ctx.layer_execution_order:
                layer_meta = current_loaded_model_ctx.get_layer_metadata(layer_id_in_exec_order)
                if not layer_meta: # Should not happen if model loaded correctly
                    logger.critical(f"CRITICAL: Metadata for L{layer_id_in_exec_order} missing during inference prep for model '{current_loaded_model_ctx.name}'. This indicates a severe internal error.")
                    all_layers_successfully_prepared = False; break
                
                # Ensure layer is on the target compute device, getting the tensor
                layer_tensor_on_compute_dev = await self._ensure_layer_on_device(layer_id_in_exec_order, target_inference_compute_dev, current_loaded_model_ctx)
                if layer_tensor_on_compute_dev is None: # Failed to get this layer onto compute device
                    all_layers_successfully_prepared = False
                    logger.error(f"Failed to ensure L{layer_meta.id} ('{layer_meta.name}') is on {target_inference_compute_dev}. Inference cannot proceed.")
                    break
                reconstructed_state_dict[layer_meta.name] = layer_tensor_on_compute_dev # Store by original Hugging Face layer name
            
            if not all_layers_successfully_prepared: # If any layer failed to prepare
                # Resources for layers that *were* successful might still be in layer_cache or on target_dev.
                # They will be managed by general cache policies or cleared if model is unloaded.
                raise HTTPException(status_code=503, detail="Could not prepare all required model layers on the compute device. Please try again shortly or check server logs for details.")

            # Create model shell on target device, then load the reconstructed state_dict
            try:
                # Use trust_remote_code setting from original model config
                trust_remote_code_flag = current_loaded_model_ctx.hf_config.trust_remote_code
                # Create an empty model shell using the loaded HF config
                inference_model_shell = AutoModelForCausalLM.from_config(current_loaded_model_ctx.hf_config, trust_remote_code=trust_remote_code_flag)
                
                # Move the empty shell to the target compute device *before* loading state_dict
                inference_model_shell.to(target_inference_compute_dev)
                
                # Load the state_dict (all tensors in it are already on target_inference_compute_dev)
                # `strict=True` (default) ensures all keys match. Can set to False if partial loading is ever intended.
                inference_model_shell.load_state_dict(reconstructed_state_dict, assign=True, strict=True) # assign=True for PyTorch 2.0+ style if applicable
                inference_model_shell.eval() # Set to evaluation mode
                
                # Store the live model instance and its active layers in the model context
                current_loaded_model_ctx._temp_inference_model_instance = inference_model_shell
                current_loaded_model_ctx._temp_model_active_layers_state = reconstructed_state_dict # Keep references to tensors
                logger.info(f"Temporary inference model instance for '{current_loaded_model_ctx.name}' successfully constructed and layers loaded on {target_inference_compute_dev}.")
            except Exception as e_construct_live_model:
                logger.error(f"Failed to construct live inference model instance or load state_dict for '{current_loaded_model_ctx.name}': {e_construct_live_model}", exc_info=True)
                current_loaded_model_ctx.clear_temp_inference_resources() # Clean up on failure
                raise HTTPException(status_code=500, detail=f"Internal server error: Failed to construct model for inference execution: {str(e_construct_live_model)}")
            
            # Phase 2: Perform actual text generation using the live model instance
            if not current_loaded_model_ctx._temp_inference_model_instance: # Should be set if Phase 1 passed
                 logger.critical("CRITICAL INTERNAL ERROR: _temp_inference_model_instance is None after successful preparation.")
                 raise HTTPException(status_code=500, detail="Internal server error: Inference model instance unexpectedly unavailable.")

            try:
                # Tokenize the input prompt
                # Using padding=True and truncation=True for robustness with batching (though generate is per-prompt here)
                # For single prompts, padding might not be strictly necessary but doesn't harm.
                inputs = model_tokenizer(generate_request.prompt, return_tensors="pt", truncation=True, padding=True).to(target_inference_compute_dev)
                
                # Perform generation
                with torch.no_grad(): # Ensure no gradients are computed during inference
                    # Common practice for open-ended generation: pad_token_id = eos_token_id
                    # Some models might require specific pad_token_id if tokenizer doesn't set it or if it's different.
                    pad_token_id_for_gen = model_tokenizer.pad_token_id if model_tokenizer.pad_token_id is not None else model_tokenizer.eos_token_id
                    
                    generated_outputs = current_loaded_model_ctx._temp_inference_model_instance.generate(
                        inputs.input_ids,
                        attention_mask=inputs.attention_mask, # Pass attention mask if tokenizer provides it (due to padding)
                        max_new_tokens=generate_request.max_new_tokens,
                        temperature=generate_request.temperature,
                        top_p=generate_request.top_p,
                        top_k=generate_request.top_k,
                        pad_token_id=pad_token_id_for_gen, # Handle padding during generation
                        eos_token_id=model_tokenizer.eos_token_id # Ensure generation stops at EOS
                        # Other parameters like do_sample, num_beams could be added from request
                    )
                
                # Decode the generated output tokens
                # generated_outputs[0] contains the full sequence (prompt + new tokens)
                # We need to slice off the input prompt's tokens to get only the newly generated ones.
                output_token_ids = generated_outputs[0][inputs.input_ids.shape[1]:] # Slice off prompt tokens
                generated_text_str = model_tokenizer.decode(output_token_ids, skip_special_tokens=True)
                num_tokens_generated = len(output_token_ids)
                
                # Phase 3: Cleanup (optional, depends on strategy)
                # The _temp_inference_model_instance and its layers are currently kept in LoadedModelContext.
                # It will be cleared on the next generate call or when the model is unloaded.
                # More aggressive cleanup could happen here if memory is extremely constrained.

                inference_end_time = time.monotonic()
                generation_duration_s = round(inference_end_time - inference_start_time, 3)
                logger.info(f"Text generation for model '{current_loaded_model_ctx.name}' successful. Duration: {generation_duration_s}s. Tokens generated: {num_tokens_generated}.")
                return GenerateTextAPIResponse(
                    prompt=generate_request.prompt, generated_text=generated_text_str, model_name=current_loaded_model_ctx.name,
                    tokens_generated=num_tokens_generated, generation_time_seconds=generation_duration_s
                )
            except Exception as e_generation_execution:
                logger.error(f"Error occurred during the model.generate() call or tokenization/decoding for model '{current_loaded_model_ctx.name}': {e_generation_execution}", exc_info=True)
                # Clean up the potentially problematic temporary model instance on error during generation
                current_loaded_model_ctx.clear_temp_inference_resources()
                raise HTTPException(status_code=500, detail=f"Core inference execution failed: {str(e_generation_execution)}")

# --- Worker Thread Loops (Layer Transfer and Prefetching) ---
def layer_transfer_worker_loop(): # Manages the layer_transfer_queue
    logger.info("Layer Transfer Worker thread started.")
    while not main_stop_event.is_set():
        req: Optional[TransferRequest] = None # Initialize req to ensure it's defined in case of early exception in try
        try:
            req = layer_transfer_queue.get(timeout=1) # Wait for 1 sec, then check main_stop_event
            req.update_status("processing") # Mark as processing
            
            layer_id_to_transfer = req.layer_id
            source_dev_type = req.source_device
            dest_dev_type = req.destination_device # e.g. "cpu", "gpu:0", "nvme", "ramdisk"
            
            meta_obj = layer_metadata_registry.get(layer_id_to_transfer)
            if not meta_obj:
                logger.warning(f"Transfer Worker: L{layer_id_to_transfer} metadata not found. Skipping transfer request.")
                req.update_status("failed", "Layer metadata not found in registry.")
                layer_transfer_queue.task_done()
                continue
            
            logger.info(f"Transfer Worker: Processing L{layer_id_to_transfer} ('{meta_obj.name}') from '{source_dev_type}' to '{dest_dev_type}' (Priority {req.priority})")

            # --- Step 1: Obtain the tensor data from source ---
            tensor_data_for_transfer: Optional[torch.Tensor] = req.data_to_transfer # Use if pre-loaded in request

            if tensor_data_for_transfer is None: # If not pre-loaded, get from current location
                if layer_id_to_transfer in layer_cache: # Source is CPU/GPU cache
                    # Ensure the cached tensor's device matches the request's source_device string
                    # This check is important if a layer could be in cache on multiple devices (not typical for this server design)
                    cached_tensor = layer_cache[layer_id_to_transfer]
                    # Simple check, assumes source_dev_type is accurate if layer is in cache.
                    # A more robust check would compare str(cached_tensor.device) with source_dev_type.
                    if meta_obj.current_device == source_dev_type or \
                       (meta_obj.current_device.startswith("gpu") and source_dev_type.startswith("gpu") and meta_obj.gpu_id == int(source_dev_type.split(":")[1])):
                        tensor_data_for_transfer = cached_tensor
                        logger.debug(f"Transfer Worker: Using cached tensor for L{layer_id_to_transfer} from source {source_dev_type}.")
                    else:
                        logger.warning(f"Transfer Worker: L{layer_id_to_transfer} in cache, but its device {str(cached_tensor.device)} "
                                       f"doesn't match request source {source_dev_type}. This may indicate inconsistent state. Attempting load from disk if applicable.")
                        # Fall through to load from disk if source_dev_type indicates disk
                
                # If still no tensor_data, and source is disk, load from disk
                if tensor_data_for_transfer is None:
                    if source_dev_type == "nvme" and nvme_mgr and nvme_mgr.path:
                        tensor_data_for_transfer = nvme_mgr.load_layer(meta_obj) # Loads dequantized to CPU
                    elif source_dev_type == "ramdisk" and ramdisk_mgr and ramdisk_mgr.path:
                        tensor_data_for_transfer = ramdisk_mgr.load_layer(meta_obj) # Loads dequantized to CPU
                    else: # Source was supposed to be RAM/VRAM but not in cache, or unknown source
                        err_msg = f"Source tensor for L{layer_id_to_transfer} not available from specified source '{source_dev_type}' (not in cache or disk load failed/not applicable)."
                        logger.error(err_msg)
                        req.update_status("failed", err_msg); layer_transfer_queue.task_done(); continue
            
            if tensor_data_for_transfer is None: # If load from source failed
                err_msg = f"Failed to obtain tensor data for L{layer_id_to_transfer} from source '{source_dev_type}'."
                logger.error(err_msg)
                req.update_status("failed", err_msg); layer_transfer_queue.task_done(); continue

            # Ensure tensor is on CPU if destination is disk (NVMe/RAMDisk) or if moving between different GPUs (CPU as intermediate)
            # Storage managers expect CPU tensor as input for their store_layer method.
            if dest_dev_type in ["nvme", "ramdisk"]:
                if tensor_data_for_transfer.device.type != "cpu":
                    logger.debug(f"Transfer Worker: Moving L{layer_id_to_transfer} to CPU before storing to {dest_dev_type}.")
                    tensor_data_for_transfer = tensor_data_for_transfer.cpu()
            # If moving GPU to GPU, typically direct P2P is faster if available, but CPU intermediate is safer general path.
            # For this worker, assume CPU intermediate if tensor isn't already on CPU for disk writes.

            # --- Step 2: Store/Move tensor to destination ---
            transfer_successful = False
            if dest_dev_type == "nvme" and nvme_mgr and nvme_mgr.path:
                transfer_successful = nvme_mgr.store_layer(tensor_data_for_transfer, meta_obj) # store_layer handles quant/compr/metadata
            elif dest_dev_type == "ramdisk" and ramdisk_mgr and ramdisk_mgr.path:
                transfer_successful = ramdisk_mgr.store_layer(tensor_data_for_transfer, meta_obj)
            elif dest_dev_type == "cpu":
                # Quantize for CPU tier, then store in layer_cache
                q_tensor_cpu, q_type_cpu, q_dtype_cpu, q_params_cpu = quant_manager.quantize_tensor(tensor_data_for_transfer, meta_obj, "cpu")
                layer_cache[layer_id_to_transfer] = q_tensor_cpu.cpu() # Ensure it's on CPU
                with meta_obj.get_lock(): # Update metadata for new CPU cache location
                    meta_obj.set_location("cpu")
                    meta_obj.quantization_type = q_type_cpu; meta_obj.current_dtype = q_dtype_cpu
                    meta_obj.quantization_params = q_params_cpu; meta_obj.current_size_bytes = q_tensor_cpu.numel() * q_tensor_cpu.element_size()
                transfer_successful = True
            elif dest_dev_type.startswith("gpu:"): # Target is a specific GPU
                try:
                    target_gpu_id = int(dest_dev_type.split(":")[1])
                    if 0 <= target_gpu_id < hardware_summary["gpu"]["count"]:
                        # Quantize for GPU tier, then move to target GPU and update cache
                        q_tensor_gpu, q_type_gpu, q_dtype_gpu, q_params_gpu = quant_manager.quantize_tensor(tensor_data_for_transfer, meta_obj, "gpu")
                        layer_cache[layer_id_to_transfer] = q_tensor_gpu.to(f"cuda:{target_gpu_id}")
                        with meta_obj.get_lock(): # Update metadata for new GPU cache location
                             meta_obj.set_location("gpu", gpu_id_val=target_gpu_id)
                             meta_obj.quantization_type = q_type_gpu; meta_obj.current_dtype = q_dtype_gpu
                             meta_obj.quantization_params = q_params_gpu; meta_obj.current_size_bytes = q_tensor_gpu.numel() * q_tensor_gpu.element_size()
                        transfer_successful = True
                    else: req.update_status("failed", f"Invalid target GPU index {target_gpu_id} in destination '{dest_dev_type}'.")
                except ValueError: req.update_status("failed", f"Malformed GPU destination string: '{dest_dev_type}'. Expected 'gpu:ID'.")
                except Exception as e_gpu_transfer: req.update_status("failed", f"Error transferring to GPU {dest_dev_type}: {e_gpu_transfer}")
            else: # Unknown destination device type
                req.update_status("failed", f"Unknown or unsupported destination device type: '{dest_dev_type}'.")

            # --- Step 3: Finalize transfer status and perform source cleanup if needed ---
            if transfer_successful:
                req.update_status("completed")
                logger.info(f"Transfer Worker: L{layer_id_to_transfer} ('{meta_obj.name}') from '{source_dev_type}' to '{dest_dev_type}' COMPLETED successfully.")
                meta_obj.update_access_stats() # Update access for destination

                # Optional: Source cleanup policy (e.g., remove from source cache if moved to persistent disk)
                # This is a simple policy: if moved from RAM/VRAM cache to disk (NVMe/RAMDisk), remove from RAM/VRAM cache.
                # If moved between RAM/VRAM caches (e.g. CPU to GPU), the layer_cache entry was overwritten, so no explicit delete needed for source cache.
                if source_dev_type != dest_dev_type and (source_dev_type == "cpu" or source_dev_type.startswith("gpu")):
                    if meta_obj.current_device == dest_dev_type and dest_dev_type in ["nvme", "ramdisk"]: # Moved to disk
                        if layer_id_to_transfer in layer_cache:
                           # Ensure the cache entry being deleted was indeed for the source_device to avoid race conditions
                           # This check is complex if multiple cache entries for same ID on diff devices were possible.
                           # For this server (one entry per layer_id in layer_cache), simple delete is okay.
                           del layer_cache[layer_id_to_transfer]
                           logger.debug(f"Transfer Worker: L{layer_id_to_transfer} removed from source RAM/VRAM cache ('{source_dev_type}') after successful move to disk ('{dest_dev_type}').")
            else: # transfer_successful is False, req.update_status("failed", ...) was called by a block above
                logger.error(f"Transfer Worker: L{layer_id_to_transfer} ('{meta_obj.name}') from '{source_dev_type}' to '{dest_dev_type}' FAILED. Reason: {req.error_message}")
            
            layer_transfer_queue.task_done() # Signal completion of this task to the queue
        except queue.Empty: # Timeout on queue.get()
            if main_stop_event.is_set(): break # Exit loop if server is stopping
            continue # Loop again to check queue or stop event
        except Exception as e_worker_loop: # Catch-all for unexpected errors in worker loop
            logger.error(f"Critical error in Layer Transfer Worker loop: {e_worker_loop}", exc_info=True)
            if req and req.status == "processing": # If error occurred mid-processing a request
                req.update_status("failed", f"Unhandled worker exception: {e_worker_loop}")
            try: 
                if req: layer_transfer_queue.task_done() # Ensure task_done even on unexpected error if req was fetched
            except ValueError: pass # task_done might raise if queue state is inconsistent after error
            time.sleep(0.2) # Brief pause to prevent tight error loop on persistent issues
    logger.info("Layer Transfer Worker thread has gracefully stopped.")


def prefetch_worker_loop(): # Manages the prefetch_job_queue
    logger.info("Prefetch Worker thread started.")
    # Get prefetching configuration settings
    prefetch_cfg = config.get("optimization", {}).get("prefetching", {})
    is_prefetching_enabled = prefetch_cfg.get("enabled", False)
    prefetch_window_ahead = prefetch_cfg.get("window_forward", 3) # Number of subsequent layers to consider prefetching
    # prefetch_window_behind = prefetch_cfg.get("window_backward", 1) # (Not implemented in this version for simplicity)
    prefetch_transfer_priority = prefetch_cfg.get("min_transfer_priority_for_prefetch", 7) # Lower priority for prefetch transfers
    
    # Target tier for prefetching: e.g., "gpu_cache" (primary compute GPU), "cpu_cache" (system RAM)
    # Layers from slower storage (NVMe, RAMDisk) are prefetched to this tier.
    prefetch_target_cache_tier_str = prefetch_cfg.get("target_cache_tier", "gpu_cache") 

    if not is_prefetching_enabled:
        logger.info("Prefetching is disabled by global configuration. Prefetch worker will exit.")
        return # Exit thread immediately if disabled

    logger.info(f"Prefetching enabled. Window: {prefetch_window_ahead} layers forward. Target Tier: {prefetch_target_cache_tier_str}. Transfer Priority: {prefetch_transfer_priority}.")

    while not main_stop_event.is_set():
        prefetch_job: Optional[Tuple[int, LoadedModelContext]] = None # (accessed_layer_id, model_context)
        try:
            prefetch_job = prefetch_job_queue.get(timeout=2) # Wait for a job (layer access hint)
            accessed_layer_id_val, model_context_obj = prefetch_job
            
            if not model_context_obj or not model_context_obj.layer_execution_order: # Validate job details
                logger.debug("Prefetch Worker: Invalid job details (no model context or execution order). Skipping.")
                prefetch_job_queue.task_done(); continue

            current_layer_index_in_model = -1
            try: # Find index of the accessed layer in the model's execution order
                current_layer_index_in_model = model_context_obj.layer_execution_order.index(accessed_layer_id_val)
            except ValueError:
                logger.warning(f"Prefetch Worker: Accessed L{accessed_layer_id_val} not found in model '{model_context_obj.name}' execution order. Cannot prefetch based on it.")
                prefetch_job_queue.task_done(); continue

            # Determine which layers to prefetch (next N layers in execution order)
            for i in range(1, prefetch_window_ahead + 1):
                layer_index_to_prefetch = current_layer_index_in_model + i
                if 0 <= layer_index_to_prefetch < len(model_context_obj.layer_execution_order): # Check bounds
                    prefetch_candidate_layer_id = model_context_obj.layer_execution_order[layer_index_to_prefetch]
                    candidate_meta_obj = layer_metadata_registry.get(prefetch_candidate_layer_id)
                    if not candidate_meta_obj:
                        logger.warning(f"Prefetch Worker: Metadata for prefetch candidate L{prefetch_candidate_layer_id} not found. Skipping.")
                        continue

                    # Determine the target device string for this prefetch operation based on prefetch_target_cache_tier_str
                    target_prefetch_device_str_for_this_layer = ""
                    model_primary_compute_dev_str = str(model_context_obj.compute_device) # E.g., "cuda:0" or "cpu"

                    if prefetch_target_cache_tier_str == "gpu_cache" and model_primary_compute_dev_str.startswith("cuda"):
                        # If layer is not already on the model's primary compute GPU, target it for prefetch
                        if not candidate_meta_obj.current_device.startswith("gpu") or \
                           (candidate_meta_obj.gpu_id != model_context_obj.compute_device.index):
                             target_prefetch_device_str_for_this_layer = model_primary_compute_dev_str
                    
                    elif prefetch_target_cache_tier_str == "cpu_cache":
                        # If layer is on disk (NVMe/RAMDisk), target CPU cache for prefetch
                        if candidate_meta_obj.current_device in ["nvme", "ramdisk"]:
                             target_prefetch_device_str_for_this_layer = "cpu"
                    
                    # If a valid target prefetch device was determined AND it's different from current location
                    if target_prefetch_device_str_for_this_layer and candidate_meta_obj.current_device != target_prefetch_device_str_for_this_layer:
                        # Further check: only prefetch if source is "slower" than target (basic heuristic)
                        # E.g., don't prefetch from GPU to CPU if target_cache_tier is CPU (unless specific need)
                        # This simple check avoids some unnecessary prefetches (e.g., from faster GPU to CPU)
                        is_source_slower_heuristic = (
                            (candidate_meta_obj.current_device == "nvme" and target_prefetch_device_str_for_this_layer != "nvme") or
                            (candidate_meta_obj.current_device == "ramdisk" and target_prefetch_device_str_for_this_layer not in ["nvme", "ramdisk"]) or
                            (candidate_meta_obj.current_device == "cpu" and target_prefetch_device_str_for_this_layer.startswith("gpu"))
                        )
                        
                        if is_source_slower_heuristic:
                            logger.info(f"Prefetch Worker: Queuing prefetch transfer for L{candidate_meta_obj.id} ('{candidate_meta_obj.name}') "
                                        f"from {candidate_meta_obj.current_device} to {target_prefetch_device_str_for_this_layer} (Prefetch Priority: {prefetch_transfer_priority})")
                            # Create a TransferRequest with lower priority for prefetching
                            prefetch_transfer_req = TransferRequest(
                                layer_id=candidate_meta_obj.id,
                                source_device=candidate_meta_obj.current_device, # Current location of the layer
                                destination_device=target_prefetch_device_str_for_this_layer,
                                priority=prefetch_transfer_priority 
                                # No direct data or callback needed for simple prefetch requests
                            )
                            layer_transfer_queue.put(prefetch_transfer_req) # Add to main transfer queue
                        else:
                            logger.debug(f"Prefetch Worker: Skipping prefetch for L{candidate_meta_obj.id} as source {candidate_meta_obj.current_device} is not considered 'slower' than target {target_prefetch_device_str_for_this_layer}.")
                    else: # No prefetch needed (already on target, or no valid target determined)
                        logger.debug(f"Prefetch Worker: No prefetch action needed for L{candidate_meta_obj.id} (current: {candidate_meta_obj.current_device}, prefetch target tier: {prefetch_target_cache_tier_str}, model compute: {model_primary_compute_dev_str}).")
            
            prefetch_job_queue.task_done() # Signal completion of this job from prefetch_job_queue
        except queue.Empty: # Timeout on prefetch_job_queue.get()
            if main_stop_event.is_set(): break # Exit loop if server is stopping
            continue # Loop again to check queue or stop event
        except Exception as e_prefetch_worker_loop: # Catch-all for unexpected errors
            logger.error(f"Unexpected error in Prefetch Worker loop: {e_prefetch_worker_loop}", exc_info=True)
            if prefetch_job: # Ensure task_done if error occurred after getting a job
                try: prefetch_job_queue.task_done()
                except ValueError: pass # Handle race condition if queue state changes
            time.sleep(1) # Brief pause to prevent tight error loop on persistent issues
    logger.info("Prefetch Worker thread has gracefully stopped.")


# --- FastAPI Application Setup and Endpoints ---
app = FastAPI( # Create FastAPI application instance
    title="Ultra Tensor Server",
    version="1.2.0", # Increment version for new features
    description="Advanced AI model inference server with dynamic layer management, multi-tier storage, quantization, compression, and prefetching.",
    docs_url="/api/docs", # URL for Swagger UI
    redoc_url="/api/redoc" # URL for ReDoc documentation
)

# API Key Authentication (Optional, based on config)
# This function will be used as a FastAPI dependency if API key is configured
api_key_header_scheme = APIKeyHeader(name="X-API-Key", auto_error=False) # auto_error=False means handle error manually

async def verify_api_key_dependency(api_key: Optional[str] = Security(api_key_header_scheme)):
    configured_api_key = config.get("api", {}).get("api_key")
    if configured_api_key: # If an API key is set in the server configuration
        if not api_key: # Key missing from request
            logger.warning("API call rejected: Missing X-API-Key header when API key is configured.")
            raise HTTPException(status_code=401, detail="X-API-Key header missing.")
        if api_key != configured_api_key: # Key invalid
            logger.warning(f"API call rejected: Invalid API Key provided ('{api_key[:5]}...').") # Log only prefix of key
            raise HTTPException(status_code=403, detail="Invalid API Key provided.")
        logger.debug("API Key verified successfully.")
        return True # API key is valid
    return True # No API key configured on server, allow access


@app.on_event("startup")
async def on_startup_event_handler(): # Main server initialization logic
    global config, hardware_mgr, quant_manager, compress_manager, nvme_mgr, ramdisk_mgr
    global mem_placement_mgr, model_manager_service, inference_service, api_key_auth_check

    # Step 1: Load Configuration from file
    # Config file path can be passed as a command-line argument, otherwise use default
    config_file_arg = sys.argv[1] if len(sys.argv) > 1 else CONFIG_FILE_PATH
    resolved_config_path = Path(config_file_arg).resolve()
    if not resolved_config_path.is_file():
        logger.fatal(f"CRITICAL: Configuration file '{resolved_config_path}' not found. Server cannot start.")
        # This exit might not be caught by Uvicorn gracefully if it happens too early.
        # Uvicorn might exit with an error code. Consider raising a specific exception that Uvicorn handles.
        sys.exit(1) # Exit if config file is essential and missing
    
    logger.info(f"Loading server configuration from: {resolved_config_path}")
    try:
        with open(resolved_config_path, "r") as f_config:
            config = json.load(f_config) # Load JSON config into global `config` dict
    except json.JSONDecodeError as e_json:
        logger.fatal(f"CRITICAL: Error decoding JSON from config file '{resolved_config_path}': {e_json}. Server cannot start.")
        sys.exit(1)
    except Exception as e_cfg_load:
        logger.fatal(f"CRITICAL: Failed to load or parse config file '{resolved_config_path}': {e_cfg_load}. Server cannot start.")
        sys.exit(1)

    # Step 2: Enhance Logging with File Handler (now that config is loaded for log file path)
    log_file_from_cfg = config.get("system",{}).get("log_file", DEFAULT_SERVER_LOG_FILE)
    resolved_log_file_path = Path(log_file_from_cfg).resolve()
    try:
        resolved_log_file_path.parent.mkdir(parents=True, exist_ok=True) # Ensure log directory exists
        file_log_handler = logging.FileHandler(resolved_log_file_path, mode='a') # Append mode
        # Use the same formatter as the default StreamHandler for consistency
        log_formatter = logging.getLogger().handlers[0].formatter 
        file_log_handler.setFormatter(log_formatter)
        logging.getLogger().addHandler(file_log_handler) # Add file handler to root logger
        logger.info(f"Server logging additionally to file: {resolved_log_file_path}")
        hardware_summary["log_file_path"] = str(resolved_log_file_path) # Store for status API
    except Exception as e_log_setup:
        logger.error(f"Failed to set up file logging to '{resolved_log_file_path}': {e_log_setup}. Logging to console only.")


    # Step 3: Initialize Core Service Managers
    hardware_mgr = HardwareManager(config) # Detects hardware, manages RAMDisk, starts monitor
    hardware_mgr.detect_all_hardware() # Populates global hardware_summary
    hardware_mgr.start_monitoring_thread() # Starts background hardware resource monitoring

    quant_manager = QuantizationManager(config) # Manages tensor quantization/dequantization
    compress_manager = CompressionManager(config) # Manages byte-level compression/decompression

    # Initialize storage managers only if their respective hardware is available and configured
    if hardware_summary["nvme"]["available"] and hardware_summary["nvme"]["path"]:
        nvme_mgr = NVMEManager(config, quant_manager, compress_manager)
    else: logger.warning("NVMe storage manager not initialized (NVMe not available or path not configured).")
    
    if hardware_summary["ramdisk"]["available"] and hardware_summary["ramdisk"]["path"]:
        ramdisk_mgr = RAMDiskManager(config, quant_manager, compress_manager)
    else: logger.warning("RAMDisk storage manager not initialized (RAMDisk not enabled or setup failed).")
    
    mem_placement_mgr = MemoryPlacementManager(config) # Manages initial layer placement strategy
    model_manager_service = ModelManagerService(config, mem_placement_mgr, nvme_mgr, ramdisk_mgr, quant_manager, compress_manager)
    inference_service = InferenceService(config, model_manager_service, nvme_mgr, ramdisk_mgr, quant_manager)
    
    # Step 4: Start Worker Threads for background tasks
    # Layer Transfer Worker (manages layer_transfer_queue for explicit moves)
    threading.Thread(target=layer_transfer_worker_loop, name="UTS_LayerTransferWorker", daemon=True).start()
    # Prefetch Worker (manages prefetch_job_queue for predictive layer loading)
    if config.get("optimization", {}).get("prefetching", {}).get("enabled", False):
        threading.Thread(target=prefetch_worker_loop, name="UTS_PrefetchWorker", daemon=True).start()
    else:
        logger.info("Prefetch worker is configured as disabled. Will not start.")

    # Step 5: Setup API Key Authentication Dependency (if API key is configured in server settings)
    if config.get("api", {}).get("api_key"):
        api_key_auth_check = verify_api_key_dependency # Assign the dependency function
        logger.info("API Key authentication is ENABLED for protected endpoints.")
    else: # No API key in config, allow open access (dependency will effectively be a pass-through)
        async def open_access_placeholder_dependency(): return True # Placeholder that always allows access
        api_key_auth_check = open_access_placeholder_dependency
        logger.info("API Key authentication is DISABLED (server endpoints are open access).")

    # Step 6: Optional: Load a default model at startup if specified in configuration
    startup_model_id_cfg = config.get("model", {}).get("startup_model_name")
    if startup_model_id_cfg:
        logger.info(f"Startup model configured in settings: '{startup_model_id_cfg}'. Attempting to load in background...")
        startup_model_compute_dev_cfg = config.get("model",{}).get("startup_model_compute_device") # Optional preferred compute device
        startup_model_hf_token_cfg = config.get("huggingface",{}).get("default_token") # Use default HF token for startup model
        # Run model loading as an asyncio background task so server startup is not blocked
        asyncio.create_task(model_manager_service.load_model(startup_model_id_cfg, startup_model_compute_dev_cfg, startup_model_hf_token_cfg))
    
    server_host_cfg = config.get('api',{}).get('host','0.0.0.0')
    server_port_cfg = config.get('api',{}).get('port',8000)
    logger.info(f"Ultra Tensor Server (v{app.version}) fully initialized and started. Listening on http://{server_host_cfg}:{server_port_cfg}")
    if config.get("api",{}).get("lm_studio_port"): # Check if LM Studio forwarding is configured
        lm_studio_fwd_port = config['api']['lm_studio_port']
        logger.info(f"LM Studio request forwarding is ENABLED for target port {lm_studio_fwd_port}.")


@app.on_event("shutdown")
async def on_shutdown_event_handler(): # Graceful server shutdown logic
    logger.info("Ultra Tensor Server received shutdown signal. Initiating graceful shutdown...")
    main_stop_event.set() # Signal all background worker threads to stop their loops

    # Step 1: Unload any currently active model to free resources
    if model_manager_service and model_manager_service.loaded_model_context:
        logger.info("Unloading active model as part of server shutdown...")
        await model_manager_service.unload_model() # Ensures layers are cleaned from caches/storage
    
    # Step 2: Shutdown worker thread pools (e.g., NVMe I/O pool)
    if nvme_worker_pool:
        logger.info("Shutting down NVMe worker pool...")
        nvme_worker_pool.shutdown(wait=True, cancel_futures=True) # Wait for tasks to complete, cancel pending
        logger.info("NVMe worker pool shut down.")
    
    # Step 3: Wait for other daemon threads (LayerTransfer, Prefetch, HWMonitor) to finish
    # These threads check main_stop_event and should exit their loops.
    # Joining them explicitly ensures they complete their cleanup.
    active_threads_list = [t for t in threading.enumerate() if t is not threading.main_thread() and t.name.startswith("UTS_")]
    if active_threads_list:
        logger.info(f"Waiting for {len(active_threads_list)} active server threads to join: {[t.name for t in active_threads_list]}")
        for thread_item in active_threads_list:
            if thread_item.is_alive(): # Check if still alive before joining
                logger.debug(f"Joining thread: {thread_item.name}")
                thread_item.join(timeout=10) # Give a timeout for each thread to join
                if thread_item.is_alive():
                    logger.warning(f"Thread {thread_item.name} did not shut down gracefully within timeout.")
    
    # Step 4: Perform HardwareManager cleanup (e.g., RAMDisk unmount)
    # This should happen after other threads that might use these resources (like storage managers) have stopped.
    if hardware_mgr:
        hardware_mgr.stop_monitoring_and_perform_cleanup() # Includes stopping monitor thread and RAMDisk cleanup

    # Final cleanup (GC, CUDA cache)
    logger.info("Performing final garbage collection and CUDA cache clear...")
    gc.collect()
    if torch.cuda.is_available():
        try: torch.cuda.empty_cache()
        except Exception as e_final_cuda_clear: logger.warning(f"Minor error during final CUDA empty_cache: {e_final_cuda_clear}")
    
    logger.info("Ultra Tensor Server shutdown process complete. Exiting.")

# --- API Endpoints Definitions ---
@app.get("/api", summary="Root endpoint providing basic server info and status", dependencies=[Depends(api_key_auth_check)])
async def api_get_server_root_info():
    return {
        "server_name": app.title, "version": app.version,
        "status_message": "Ultra Tensor Server is running.",
        "current_time_utc": datetime.datetime.utcnow().isoformat() + "Z",
        "server_uptime_seconds": round(time.monotonic() - server_start_time, 2),
        "api_docs_url": app.docs_url, "api_redoc_url": app.redoc_url,
        "hint": "Use /api/status for detailed server and hardware information."
    }

@app.get("/api/status", response_model=ServerStatusAPIResponse, summary="Get detailed real-time server status", dependencies=[Depends(api_key_auth_check)])
async def api_get_full_server_status():
    if not model_manager_service or not hardware_mgr: # Should always be initialized if startup succeeded
        logger.error("Server status API called but core managers (ModelManager or HardwareManager) are not available. This indicates a startup failure.")
        raise HTTPException(status_code=503, detail="Server core components are not fully initialized. Check server logs.")
    
    # Ensure model status reflects the latest from ModelManagerService
    current_model_api_status = model_manager_service.get_current_model_status()
    
    return ServerStatusAPIResponse(
        server_status="running" if not main_stop_event.is_set() else "shutting_down",
        uptime_seconds=round(time.monotonic() - server_start_time, 2),
        model_status=current_model_api_status,
        hardware_summary=hardware_summary, # Global hardware_summary is updated by monitor thread
        layer_transfer_queue_size=layer_transfer_queue.qsize(),
        prefetch_job_queue_size=prefetch_job_queue.qsize(),
        active_threads=threading.active_count(), # Total active threads in the Python process
        log_file_path=hardware_summary.get("log_file_path") # Get log file path from summary
    )

@app.post("/api/models/load", response_model=ModelStatusAPIResponse, summary="Load a new model into the server", status_code=202, dependencies=[Depends(api_key_auth_check)]) # 202 Accepted for background task
async def api_endpoint_load_model(load_request: ModelLoadAPIRequest, background_tasks: BackgroundTasks):
    if not model_manager_service: raise HTTPException(status_code=503, detail="ModelManagerService is not available (server error).")
    
    # Check current model state before queueing load
    # Use a lock here if checking/setting status that ModelManagerService also mutates,
    # or rely on ModelManagerService's internal lock for the actual load operation.
    # For now, simple check based on ModelManagerService's public status.
    current_model_op_status = model_manager_service.current_model_status_str
    
    if current_model_op_status == "loading":
        logger.warning(f"Model load request for '{load_request.model_name_or_path}' received while another load is already in progress ('{model_manager_service.get_current_model_status().model_name or 'previous model'}'). Request ignored.")
        raise HTTPException(status_code=409, detail=f"A model loading operation is already in progress. Please wait.")
    
    if current_model_op_status == "loaded":
        # If trying to load the *same* model that's already loaded, could return current status or re-init.
        # For now, require unload first if a *different* model is requested.
        if model_manager_service.loaded_model_context and model_manager_service.loaded_model_context.name == load_request.model_name_or_path:
            logger.info(f"Model '{load_request.model_name_or_path}' is already loaded. Returning current status.")
            return model_manager_service.get_current_model_status() # Return current status
        else: # A different model is loaded
            loaded_model_name = model_manager_service.loaded_model_context.name if model_manager_service.loaded_model_context else "Unknown"
            logger.warning(f"Model load request for '{load_request.model_name_or_path}' received, but model '{loaded_model_name}' is currently loaded. Unload first.")
            raise HTTPException(status_code=409, detail=f"Model '{loaded_model_name}' is currently loaded. Please use /api/models/unload first before loading a new model.")

    # Add the model loading task to FastAPI's background tasks
    # This allows the API to return quickly while loading happens in the background.
    background_tasks.add_task(model_manager_service.load_model, 
                              load_request.model_name_or_path, 
                              load_request.compute_device, 
                              load_request.hf_token)
    
    # Return an immediate "loading" status response for the requested model
    # ModelManagerService will update its internal status string during the actual load.
    logger.info(f"Accepted request to load model '{load_request.model_name_or_path}'. Loading will proceed in background.")
    return ModelStatusAPIResponse(
        model_name=load_request.model_name_or_path, status="loading", 
        num_managed_layers=0, error_message=None # Initial status, will be updated
    )

@app.post("/api/models/unload", response_model=ModelStatusAPIResponse, summary="Unload the currently active model", dependencies=[Depends(api_key_auth_check)])
async def api_endpoint_unload_model(background_tasks: BackgroundTasks): # Unload can also be a background task if very slow
    if not model_manager_service: raise HTTPException(status_code=503, detail="ModelManagerService is not available.")
    
    current_model_op_status = model_manager_service.current_model_status_str
    if current_model_op_status == "no_model_loaded":
        logger.info("Unload request received, but no model is currently loaded.")
        return model_manager_service.get_current_model_status() # Return "no_model_loaded" status
    if current_model_op_status == "loading" or current_model_op_status == "unloading":
        logger.warning(f"Unload request received while model is currently '{current_model_op_status}'. Request ignored.")
        raise HTTPException(status_code=409, detail=f"Cannot unload model, current operation status is: {current_model_op_status}.")

    # Add unload to background tasks, as it can involve disk I/O and cleanup.
    background_tasks.add_task(model_manager_service.unload_model)
    logger.info(f"Accepted request to unload model '{model_manager_service.loaded_model_context.name if model_manager_service.loaded_model_context else 'current'}'. Unloading in background.")
    
    # Return an immediate "unloading" status
    # ModelManagerService will update its internal status to "no_model_loaded" upon completion.
    unloading_model_name = model_manager_service.loaded_model_context.name if model_manager_service.loaded_model_context else "N/A"
    return ModelStatusAPIResponse(model_name=unloading_model_name, status="unloading")


@app.get("/api/models/status", response_model=ModelStatusAPIResponse, summary="Get status of the currently loaded (or loading/errored) model", dependencies=[Depends(api_key_auth_check)])
async def api_endpoint_get_current_model_status():
    if not model_manager_service: raise HTTPException(status_code=503, detail="ModelManagerService is not available.")
    return model_manager_service.get_current_model_status() # Fetches latest status

@app.post("/api/models/generate", response_model=GenerateTextAPIResponse, summary="Generate text using the loaded model", dependencies=[Depends(api_key_auth_check)])
async def api_endpoint_generate_text(generate_req: GenerateTextAPIRequest):
    if not inference_service: raise HTTPException(status_code=503, detail="InferenceService is not available.")
    if not model_manager_service or model_manager_service.current_model_status_str != "loaded":
        logger.warning(f"Generation request received, but model status is '{model_manager_service.current_model_status_str if model_manager_service else 'N/A'}', not 'loaded'.")
        raise HTTPException(status_code=409, detail="No model is loaded and ready for inference. Load a model first via /api/models/load and wait for 'loaded' status.")
    
    try: # Call the inference service's generate method
        return await inference_service.generate(generate_req)
    except HTTPException as http_exc_from_generate: # Re-raise HTTP exceptions from service
        raise http_exc_from_generate
    except Exception as e_unhandled_generate: # Catch any other unexpected errors from generate
        logger.error(f"Unhandled exception in /api/models/generate endpoint: {e_unhandled_generate}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"An unexpected internal server error occurred during text generation: {str(e_unhandled_generate)}")

@app.get("/api/layers", response_model=List[LayerInfoAPI], summary="List metadata for all currently managed layers", dependencies=[Depends(api_key_auth_check)])
async def api_endpoint_get_all_layers_info():
    # Provides a snapshot of all layers currently in the global layer_metadata_registry
    # This can be a long list if a large model is loaded. Consider pagination for very large models.
    if not layer_metadata_registry: return [] # Return empty list if no layers are registered
    return [meta.to_api_model() for meta in layer_metadata_registry.values()]

@app.get("/api/layers/{layer_id_param}", response_model=LayerInfoAPI, summary="Get metadata for a specific layer by its ID", dependencies=[Depends(api_key_auth_check)])
async def api_endpoint_get_specific_layer_info(layer_id_param: int):
    meta_obj = layer_metadata_registry.get(layer_id_param)
    if not meta_obj:
        raise HTTPException(status_code=404, detail=f"Layer with ID {layer_id_param} not found in metadata registry.")
    return meta_obj.to_api_model()

@app.post("/api/layers/transfer", summary="Manually request a layer transfer (for diagnostics/advanced control)", status_code=202, dependencies=[Depends(api_key_auth_check)])
async def api_endpoint_request_layer_transfer(transfer_api_req: TransferRequestAPI):
    meta_obj_to_transfer = layer_metadata_registry.get(transfer_api_req.layer_id)
    if not meta_obj_to_transfer:
        raise HTTPException(status_code=404, detail=f"Layer ID {transfer_api_req.layer_id} not found for transfer.")
    
    # Validate destination device string format (basic validation)
    valid_dest_prefixes = ["cpu", "gpu", "ramdisk", "nvme"]
    if not any(transfer_api_req.destination_device.startswith(p) for p in valid_dest_prefixes):
         raise HTTPException(status_code=400, detail=f"Invalid destination_device format: '{transfer_api_req.destination_device}'. Must start with one of {valid_dest_prefixes}.")
    if transfer_api_req.destination_device.startswith("gpu") and ":" not in transfer_api_req.destination_device:
         raise HTTPException(status_code=400, detail="GPU destination must be specified as 'gpu:ID', e.g., 'gpu:0'.")

    # Create TransferRequest and add to queue
    transfer_request_obj = TransferRequest(
        layer_id=meta_obj_to_transfer.id,
        source_device=meta_obj_to_transfer.current_device, # Source is layer's current location
        destination_device=transfer_api_req.destination_device,
        priority=transfer_api_req.priority
        # No direct data or callback passed from API for manual transfers
    )
    try:
        layer_transfer_queue.put_nowait(transfer_request_obj) # Use put_nowait to avoid blocking API if queue is full
    except queue.Full:
        logger.error("Layer transfer queue is full. Cannot accept new manual transfer request at this time.")
        raise HTTPException(status_code=503, detail="Layer transfer queue is currently full. Please try again later.")
        
    logger.info(f"API: Queued manual transfer for L{meta_obj_to_transfer.id} ('{meta_obj_to_transfer.name}') from {meta_obj_to_transfer.current_device} to {transfer_api_req.destination_device} with priority {transfer_api_req.priority}.")
    return {"message": "Layer transfer request successfully queued.", "layer_id": meta_obj_to_transfer.id, "source": meta_obj_to_transfer.current_device, "destination": transfer_api_req.destination_device, "priority": transfer_api_req.priority}

# Optional LM Studio Forwarding Endpoint (activated if lm_studio_port is configured)
# This requires 'httpx' library to be installed.
if config.get("api", {}).get("lm_studio_port"): # Check if forwarding is configured
    @app.post("/v1/completions", summary="Forward OpenAI-compatible requests to LM Studio (if configured)", dependencies=[Depends(api_key_auth_check)], include_in_schema=True)
    async def endpoint_forward_to_lm_studio(request: Request):
        lm_studio_config = config.get("api", {}) # Get LM Studio forwarding settings from config
        lm_studio_target_port = lm_studio_config.get("lm_studio_port")
        lm_studio_target_host = lm_studio_config.get("lm_studio_host", "localhost") # Default to localhost
        
        if not lm_studio_target_port: # Should not happen if endpoint is active due to outer 'if', but defensive
            logger.error("LM Studio forwarding endpoint called, but target port not configured properly in server settings.")
            raise HTTPException(status_code=501, detail="LM Studio forwarding is not configured correctly on the server.")
        
        target_lm_studio_url = f"http://{lm_studio_target_host}:{lm_studio_target_port}/v1/completions"
        logger.info(f"Forwarding '/v1/completions' request to LM Studio at: {target_lm_studio_url}")
        
        try:
            request_body_bytes = await request.body() # Get raw request body
            # Forward headers, excluding Host, and potentially adding/modifying others if needed
            # For simplicity, forward most headers. Be mindful of sensitive headers.
            headers_to_forward = {k: v for k, v in request.headers.items() if k.lower() not in ["host", "content-length", "connection"]}
            headers_to_forward["Content-Length"] = str(len(request_body_bytes)) # Correct content length for forwarded request

            timeout_seconds = lm_studio_config.get("lm_studio_timeout_seconds", 120) # Configurable timeout
            
            async with httpx.AsyncClient() as http_client:
                 forwarded_response = await http_client.post(
                     target_lm_studio_url, 
                     content=request_body_bytes, # Forward raw bytes
                     headers=headers_to_forward, 
                     timeout=timeout_seconds
                 )
                 # Check if LM Studio returned an error status
                 forwarded_response.raise_for_status() # Raises HTTPStatusError for 4xx/5xx responses
                 # Return the JSON response from LM Studio directly
                 return forwarded_response.json()
        
        except httpx.ConnectError as e_connect:
            logger.error(f"LM Studio Forwarding: Connection to '{target_lm_studio_url}' failed: {e_connect}")
            raise HTTPException(status_code=503, detail=f"Cannot connect to upstream LM Studio service at '{target_lm_studio_url}'.")
        except httpx.ReadTimeout as e_timeout:
            logger.error(f"LM Studio Forwarding: Request to '{target_lm_studio_url}' timed out after {timeout_seconds}s.")
            raise HTTPException(status_code=504, detail=f"Upstream LM Studio request to '{target_lm_studio_url}' timed out.")
        except httpx.HTTPStatusError as e_http_status: # Error response from LM Studio (4xx or 5xx)
            logger.error(f"LM Studio Forwarding: Upstream LM Studio at '{target_lm_studio_url}' returned error {e_http_status.response.status_code}: {e_http_status.response.text}")
            # Forward the error status and details from LM Studio
            raise HTTPException(status_code=e_http_status.response.status_code, detail=f"Error from upstream LM Studio: {e_http_status.response.text}")
        except json.JSONDecodeError as e_json_body: # If this server failed to parse request body (should be rare if client is OpenAI compatible)
             logger.error(f"LM Studio Forwarding: Error decoding JSON from incoming request body: {e_json_body}")
             raise HTTPException(status_code=400, detail="Invalid JSON in request body for LM Studio forwarding.")
        except Exception as e_forward_generic: # Catch any other unexpected errors during forwarding
            logger.error(f"LM Studio Forwarding: A generic error occurred: {e_forward_generic}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"An internal server error occurred while forwarding request to LM Studio: {str(e_forward_generic)}")
else:
    logger.info("LM Studio forwarding endpoint (/v1/completions) is NOT active as 'lm_studio_port' is not configured in 'api' settings.")


# --- Main Execution Block (for running with Uvicorn) ---
if __name__ == "__main__":
    # This block is executed when the script is run directly (e.g., python ultra_tensor_server.py)
    # Configuration loading and manager initializations are handled within the FastAPI on_startup_event_handler.
    
    # Attempt to get host/port from config for Uvicorn, with defaults if not found.
    # This allows Uvicorn to bind correctly without needing CLI args for host/port if they are in config.
    uvicorn_target_host = "0.0.0.0" # Default host
    uvicorn_target_port = 8000    # Default port
    
    # Pre-parse config minimally just to get host/port for Uvicorn if config file is specified as CLI arg.
    # This is a bit redundant as full config load happens in on_startup, but helps Uvicorn bind.
    try:
        # Check if a config file path is provided as a command-line argument
        config_file_arg_for_main = sys.argv[1] if len(sys.argv) > 1 else CONFIG_FILE_PATH
        path_to_check_config = Path(config_file_arg_for_main)
        if path_to_check_config.is_file():
            with open(path_to_check_config, "r") as f_main_cfg:
                temp_cfg_data_for_main = json.load(f_main_cfg)
            uvicorn_target_host = temp_cfg_data_for_main.get("api",{}).get("host", uvicorn_target_host)
            uvicorn_target_port = temp_cfg_data_for_main.get("api",{}).get("port", uvicorn_target_port)
        else:
            logger.warning(f"Config file '{path_to_check_config}' not found for Uvicorn pre-parse. Using default host/port. Full config load will occur in FastAPI startup.")
    except Exception as e_main_cfg_parse:
        logger.warning(f"Could not pre-parse config file for Uvicorn host/port settings. Using defaults. Error: {e_main_cfg_parse}")

    logger.info(f"Preparing to start Uvicorn server. Target Host: {uvicorn_target_host}, Target Port: {uvicorn_target_port}")
    
    # Determine Uvicorn log level based on server's main log level from config (or default INFO)
    # This requires full config to be loaded first if we want to sync Uvicorn's log level.
    # For simplicity, Uvicorn will use its own default log level or one passed via CLI.
    # To sync, config load would need to happen before uvicorn.run, and then pass log_level.
    # Example: uvicorn_log_level = config.get("system", {}).get("log_level", "info").lower()

    try:
        # Run the FastAPI application using Uvicorn server
        # 'app' is the FastAPI instance defined globally in this script.
        # The on_startup_event_handler will be triggered by Uvicorn when the app starts.
        uvicorn.run(
            "__main__:app", # Uvicorn looks for 'app' in the current module (__main__)
            host=uvicorn_target_host,
            port=uvicorn_target_port,
            reload=False, # Set to True for development to auto-reload on code changes (not for production)
            # log_level=uvicorn_log_level, # Optional: pass log level to Uvicorn
            # workers=1 # Uvicorn workers; for async FastAPI, 1 worker is often fine. Multi-worker needs care with global state.
        )
    except KeyboardInterrupt: # Handle Ctrl+C gracefully
        logger.info("KeyboardInterrupt received by main Uvicorn process. Server will shut down via FastAPI's on_shutdown event.")
        # FastAPI's on_shutdown event should handle setting main_stop_event and cleanup.
    except SystemExit as e_sysexit: # Catch sys.exit calls (e.g., from early config load failure in on_startup)
        logger.critical(f"Server exiting due to SystemExit (code: {e_sysexit.code}). This might be due to a critical startup error.")
        # Ensure main_stop_event is set if shutdown wasn't clean (e.g., Uvicorn crash before FastAPI signals)
        if not main_stop_event.is_set(): main_stop_event.set()
    except Exception as e_main_uvicorn_run: # Catch any other unexpected errors during Uvicorn run
        logger.fatal(f"Unhandled critical exception during Uvicorn server execution: {e_main_uvicorn_run}", exc_info=True)
        if not main_stop_event.is_set(): main_stop_event.set() # Attempt to signal threads
    finally:
        # This block executes after Uvicorn stops (normally or due to error/interrupt)
        logger.info("UltraTensorServer Uvicorn process has concluded.")
        # Ensure main_stop_event is set one last time, in case shutdown sequence was interrupted.
        if not main_stop_event.is_set():
            logger.info("Setting main_stop_event in final __main__ block to ensure all background threads attempt termination.")
            main_stop_event.set()
        # A small delay to allow any final logging from daemon threads before process exits, though daemon threads terminate abruptly.
        time.sleep(0.5)
# [Code from the previous responses, including HardwareManager, QuantizationManager,
#  CompressionManager, BaseStorageManager, NVMEManager, RAMDiskManager, and
#  the refined MemoryPlacementManager class, should be above this line]

# --- ModelManagerService (Handles loading, unloading, and managing model layers) ---
class ModelManagerService:
    def __init__(self, app_cfg: Dict, placement_mgr_inst: MemoryPlacementManager,
                 nvme_mgr_inst: Optional[NVMEManager], ramdisk_mgr_inst: Optional[RAMDiskManager],
                 quant_mgr_inst: QuantizationManager, compress_mgr_inst: CompressionManager):
        self.app_config = app_cfg
        self.placement_manager = placement_mgr_inst
        self.nvme_manager = nvme_mgr_inst
        self.ramdisk_manager = ramdisk_mgr_inst
        self.quant_manager = quant_mgr_inst # Used for quantizing before placing in CPU/GPU cache
        self.compress_manager = compress_mgr_inst # Not directly used by MMS, but storage managers use it

        self.loaded_model_context: Optional[LoadedModelContext] = None # Holds the currently active model's context
        self._model_operation_lock = asyncio.Lock() # Async lock for model load/unload operations
        self.current_model_status_str: str = "no_model_loaded" # Tracks operational status for API
        self.current_model_error_msg: Optional[str] = None # Stores error message if status is "error"
        
        self.next_global_layer_id: int = 0 # Counter for unique layer IDs across all models
        self.hf_api_client = HfApi() # Hugging Face API client for Hub interactions (e.g., listing files, not used in download here)
        logger.info("ModelManagerService initialized.")

    def get_current_model_status(self) -> ModelStatusAPIResponse: # For API status reporting
        # This method provides the current status of the model.
        # It should reflect ongoing operations like "loading" or "unloading" as well.
        if self.loaded_model_context and self.current_model_status_str == "loaded": # If a model is successfully loaded
            return self.loaded_model_context.to_status_api_model(self.current_model_status_str, self.current_model_error_msg)
        
        # If no model context, or status is not "loaded" (e.g., "loading", "error", "no_model_loaded")
        # For "loading", we might not have a full context yet, so construct a minimal status.
        model_name_for_status = self.loaded_model_context.name if self.loaded_model_context else None
        if self.current_model_status_str == "loading" and not model_name_for_status:
            # If we know the name of the model being loaded (e.g., from the request), it could be passed here.
            # For simplicity, if context isn't set, name will be None in status.
            pass

        return ModelStatusAPIResponse(
            model_name=model_name_for_status,
            status=self.current_model_status_str, 
            error_message=self.current_model_error_msg
            # Other fields default to None or 0 if no model context
        )

    def _resolve_model_path_and_load_configs(self, model_identifier_str: str, hf_auth_token_str: Optional[str]) -> Tuple[str, AutoConfig, PreTrainedTokenizerBase]:
        # Resolves model (local path or Hub ID) to a local path, downloads if necessary, and loads HF config/tokenizer.
        target_model_path_obj = Path(model_identifier_str)
        # Use provided token, or token from config, or environment variable as fallback for HF Hub
        token_for_hf_hub = hf_auth_token_str or self._cfg_path_get("huggingface", "default_token") or os.getenv("HF_TOKEN")
        trust_remote_code_cfg = self._cfg_path_get("huggingface","trust_remote_code", default_val=False)

        if target_model_path_obj.is_dir() and (target_model_path_obj / "config.json").exists(): # If it's a local directory with config
            logger.info(f"Using local model from specified path: {model_identifier_str}")
            resolved_local_model_path = str(target_model_path_obj.resolve())
        else: # Assumed to be a Hugging Face Hub model ID or a path that needs resolution
            logger.info(f"Model '{model_identifier_str}' not found as a direct local model directory. Attempting download/resolution via Hugging Face Hub.")
            hf_cache_dir_path = Path(self._cfg_path_get("huggingface", "cache_dir", default_val=DEFAULT_MODEL_CACHE_DIR)).resolve()
            hf_cache_dir_path.mkdir(parents=True, exist_ok=True)
            try:
                resolved_local_model_path = snapshot_download(
                    repo_id=model_identifier_str,
                    cache_dir=str(hf_cache_dir_path), # Ensure it's a string
                    token=token_for_hf_hub,
                    resume_download=True,
                    # allow_patterns=["*.json", "*.safetensors", "*.bin*", "tokenizer*", "generation_config.json"], # More specific patterns
                    # For .bin, it could be pytorch_model.bin or model.bin.index.json with multiple shards.
                    # snapshot_download usually handles sharded models correctly by getting all relevant files.
                    ignore_patterns=["*.gitattributes", "*.md", ".git*", "*.gguf", "*.ggml*", "onnx/*"], # Ignore common non-essential or alternative format files
                    # Consider local_files_only=True if an offline mode is needed and model is expected to be cached.
                )
                logger.info(f"Model '{model_identifier_str}' successfully downloaded/resolved to local path: {resolved_local_model_path}")
            except Exception as e_hf_download:
                logger.error(f"Failed to download or resolve model '{model_identifier_str}' from Hugging Face Hub: {e_hf_download}", exc_info=True)
                raise ValueError(f"Download/resolution failed for model '{model_identifier_str}'") from e_hf_download
        
        try: # Load Hugging Face AutoConfig and AutoTokenizer from the resolved local path
            model_config_obj = AutoConfig.from_pretrained(resolved_local_model_path, token=token_for_hf_hub, trust_remote_code=trust_remote_code_cfg)
            tokenizer_obj = AutoTokenizer.from_pretrained(resolved_local_model_path, token=token_for_hf_hub, trust_remote_code=trust_remote_code_cfg, use_fast=True)
            # Ensure tokenizer has pad_token if missing (common for some models like Llama)
            if tokenizer_obj.pad_token is None:
                if tokenizer_obj.eos_token is not None:
                    tokenizer_obj.pad_token = tokenizer_obj.eos_token # Common practice: use EOS as PAD
                    logger.info(f"Tokenizer for '{model_identifier_str}' was missing a pad_token. Set to eos_token ('{tokenizer_obj.eos_token}').")
                else: # If EOS is also missing, this is problematic. Add a default one.
                    # Adding a new token might resize model embeddings if not careful, but for tokenizer only it's usually fine.
                    tokenizer_obj.add_special_tokens({'pad_token': '[PAD]'})
                    logger.warning(f"Tokenizer for '{model_identifier_str}' was missing pad_token and eos_token. Added a new pad_token '[PAD]'. Model may need vocab resizing if this token is used in training.")

            return resolved_local_model_path, model_config_obj, tokenizer_obj
        except Exception as e_load_hf_cfg_tok:
            logger.error(f"Failed to load AutoConfig/AutoTokenizer from path '{resolved_local_model_path}' for model '{model_identifier_str}': {e_load_hf_cfg_tok}", exc_info=True)
            raise ValueError(f"Config/Tokenizer loading failed for model '{model_identifier_str}' from path '{resolved_local_model_path}'") from e_load_hf_cfg_tok


    async def load_model(self, model_id_str: str, preferred_compute_device_str: Optional[str] = None, hf_auth_token: Optional[str] = None):
        async with self._model_operation_lock:
            # Prevent re-loading if already loaded, unless specific re-load logic is intended (not here)
            if self.loaded_model_context and self.current_model_status_str == "loaded":
                if self.loaded_model_context.name == model_id_str:
                    self.current_model_error_msg = f"Model '{model_id_str}' is already loaded. No action taken."
                    logger.info(self.current_model_error_msg)
                    return
                else:
                    self.current_model_error_msg = f"A different model ('{self.loaded_model_context.name}') is already loaded. Unload it first via /api/models/unload before loading '{model_id_str}'."
                    logger.error(self.current_model_error_msg)
                    # Do not change status here, it should reflect "loaded" or previous error.
                    return

            if self.current_model_status_str == "loading":
                 logger.warning(f"A model loading operation is already in progress. Ignoring concurrent request to load '{model_id_str}'.")
                 return

            self.current_model_status_str = "loading"
            self.current_model_error_msg = None
            logger.info(f"Initiating model load for: '{model_id_str}'")
            
            newly_created_layer_ids_this_load: List[int] = [] # For cleanup on failure

            try:
                resolved_path, hf_model_cfg, hf_tokenizer_inst = self._resolve_model_path_and_load_configs(model_id_str, hf_auth_token)

                target_compute_dev: torch.device
                if preferred_compute_device_str:
                    target_compute_dev = torch.device(preferred_compute_device_str)
                elif hardware_summary["gpu"]["available"] and hardware_summary["gpu"]["devices"]:
                    target_compute_dev = torch.device(f"cuda:{hardware_summary['gpu']['devices'][0]['id']}")
                else: target_compute_dev = torch.device("cpu")
                logger.info(f"Target primary compute device for model '{model_id_str}' set to: {target_compute_dev}")

                logger.info(f"Loading full model weights for '{model_id_str}' to CPU (FP32) for layer processing...")
                trust_rc_cfg = self._cfg_path_get("huggingface","trust_remote_code", default_val=False)
                # Using low_cpu_mem_usage=True can help for very large models if supported by the model type
                # and if not all params are immediately needed on CPU. Here, we need them for processing.
                # device_map="cpu" also ensures it loads to CPU.
                try:
                    temp_full_model_on_cpu = AutoModelForCausalLM.from_pretrained(
                        resolved_path, config=hf_model_cfg, torch_dtype=torch.float32, 
                        token=hf_auth_token, trust_remote_code=trust_rc_cfg, device_map="cpu", low_cpu_mem_usage=True
                    )
                except Exception as e_from_pretrained:
                    logger.error(f"Failed during AutoModelForCausalLM.from_pretrained for '{model_id_str}': {e_from_pretrained}", exc_info=True)
                    # Attempt without low_cpu_mem_usage as a fallback for models not supporting it well
                    logger.info(f"Retrying model load for '{model_id_str}' without low_cpu_mem_usage=True.")
                    temp_full_model_on_cpu = AutoModelForCausalLM.from_pretrained(
                        resolved_path, config=hf_model_cfg, torch_dtype=torch.float32, 
                        token=hf_auth_token, trust_remote_code=trust_rc_cfg
                    ).cpu() # Ensure on CPU
                
                temp_full_model_on_cpu.eval() # Set to eval mode

                num_params = len(list(temp_full_model_on_cpu.named_parameters()))
                avg_layer_size_b = sum(p.numel() * p.element_size() for p in temp_full_model_on_cpu.parameters()) // num_params if num_params > 0 else 0
                self.placement_manager.calculate_tier_capacity_for_model(num_params, avg_layer_size_b)

                for layer_name, layer_tensor_cpu in temp_full_model_on_cpu.named_parameters():
                    layer_id = self.next_global_layer_id; self.next_global_layer_id += 1
                    newly_created_layer_ids_this_load.append(layer_id)
                    orig_size_b = layer_tensor_cpu.numel() * layer_tensor_cpu.element_size()
                    
                    meta = LayerMetadata(layer_id, layer_name, orig_size_b, tuple(layer_tensor_cpu.shape), layer_tensor_cpu.dtype, "cpu")
                    layer_metadata_registry[layer_id] = meta

                    target_tier, target_gpu_id = self.placement_manager.get_initial_placement_for_layer(layer_id, layer_name)
                    logger.debug(f"L{layer_id} ('{layer_name}'), OrigSz:{orig_size_b/(1024**2):.1f}MB. Initial placement: {target_tier}" + (f":GPU_{target_gpu_id}" if target_gpu_id is not None else ""))

                    placed_successfully_on_target = False
                    if target_tier == "gpu":
                        if hardware_summary["gpu"]["available"] and target_gpu_id is not None:
                            try:
                                q_tensor_gpu, q_type, q_dtype, q_params = self.quant_manager.quantize_tensor(layer_tensor_cpu, meta, "gpu")
                                layer_cache[layer_id] = q_tensor_gpu.to(f"cuda:{target_gpu_id}")
                                meta.set_location("gpu", gpu_id_val=target_gpu_id); meta.quantization_type = q_type; meta.current_dtype = q_dtype
                                meta.quantization_params = q_params; meta.current_size_bytes = q_tensor_gpu.numel() * q_tensor_gpu.element_size()
                                placed_successfully_on_target = True
                            except Exception as e_gpu_place:
                                logger.warning(f"Failed to place L{layer_id} on GPU {target_gpu_id} directly: {e_gpu_place}. Fallback needed.")
                        if not placed_successfully_on_target: target_tier = "cpu" # Fallback
                    
                    if target_tier == "cpu":
                        q_tensor_cpu, q_type, q_dtype, q_params = self.quant_manager.quantize_tensor(layer_tensor_cpu, meta, "cpu")
                        layer_cache[layer_id] = q_tensor_cpu # Assumed already on CPU
                        meta.set_location("cpu"); meta.quantization_type = q_type; meta.current_dtype = q_dtype
                        meta.quantization_params = q_params; meta.current_size_bytes = q_tensor_cpu.numel() * q_tensor_cpu.element_size()
                        placed_successfully_on_target = True
                    
                    elif target_tier == "ramdisk" and self.ramdisk_manager and self.ramdisk_manager.path:
                        if self.ramdisk_manager.store_layer(layer_tensor_cpu, meta): placed_successfully_on_target = True
                        else: logger.error(f"Failed to store L{layer_id} to RAMDisk. Fallback to CPU cache."); target_tier = "cpu" # Fallback to CPU cache
                    
                    elif target_tier == "nvme" and self.nvme_manager and self.nvme_manager.path:
                        if self.nvme_manager.store_layer(layer_tensor_cpu, meta): placed_successfully_on_target = True
                        else: logger.error(f"Failed to store L{layer_id} to NVMe. Fallback to CPU cache."); target_tier = "cpu" # Fallback

                    if not placed_successfully_on_target and target_tier != "cpu": # If fallback from disk occurred
                        logger.info(f"Fallback: Placing L{layer_id} ('{layer_name}') into CPU cache after failed {target_tier} placement.")
                        q_tensor_cpu_fb, q_type_fb, q_dtype_fb, q_params_fb = self.quant_manager.quantize_tensor(layer_tensor_cpu, meta, "cpu")
                        layer_cache[layer_id] = q_tensor_cpu_fb
                        meta.set_location("cpu"); meta.quantization_type = q_type_fb; meta.current_dtype = q_dtype_fb
                        meta.quantization_params = q_params_fb; meta.current_size_bytes = q_tensor_cpu_fb.numel() * q_tensor_cpu_fb.element_size()
                    
                    meta.update_access_stats()

                del temp_full_model_on_cpu; gc.collect()

                self.loaded_model_context = LoadedModelContext(
                    model_id_str, resolved_path, hf_model_cfg, hf_tokenizer_inst,
                    newly_created_layer_ids_this_load, target_compute_dev, hf_auth_token
                )
                self.current_model_status_str = "loaded"
                total_model_params_approx = sum(layer_metadata_registry[lid].original_size_bytes for lid in newly_created_layer_ids_this_load if layer_metadata_registry[lid].original_dtype.is_floating_point) / (layer_tensor_cpu.element_size() * 1e6) # Rough from FP32
                logger.info(f"Model '{model_id_str}' loaded. Approx {total_model_params_approx:.2f}M params. Compute: {target_compute_dev}.")

            except Exception as e_load_main:
                self.current_model_status_str = "error"
                self.current_model_error_msg = f"Failed to load model '{model_id_str}': {e_load_main}"
                logger.error(self.current_model_error_msg, exc_info=True)
                await self._cleanup_after_failed_load(newly_created_layer_ids_this_load)
                self.loaded_model_context = None

    async def unload_model(self):
        async with self._model_operation_lock:
            if not self.loaded_model_context:
                logger.info("Unload called, but no model loaded.")
                self.current_model_status_str = "no_model_loaded"; return

            unloaded_model_name_log = self.loaded_model_context.name
            logger.info(f"Unloading model: '{unloaded_model_name_log}'")
            self.current_model_status_str = "unloading"
            
            self.loaded_model_context.clear_temp_inference_resources() # Clear live model instance if any
            await self._cleanup_after_failed_load(self.loaded_model_context.layer_ids) # Re-use cleanup

            self.loaded_model_context = None
            self.current_model_status_str = "no_model_loaded"
            self.current_model_error_msg = None
            
            logger.info(f"Model '{unloaded_model_name_log}' unloaded and resources cleaned.")
            gc.collect()
            if torch.cuda.is_available(): torch.cuda.empty_cache()

    async def _cleanup_after_failed_load(self, layer_ids_to_clean: List[int]):
        logger.warning(f"Cleaning up {len(layer_ids_to_clean)} layer IDs due to load failure or unload.")
        for layer_id_val in layer_ids_to_clean:
            if layer_id_val in layer_metadata_registry:
                meta = layer_metadata_registry[layer_id_val]
                logger.debug(f"Cleaning L{layer_id_val} ('{meta.name}'), current_dev: {meta.current_device}")
                if meta.current_device == "nvme" and self.nvme_manager: self.nvme_manager.delete_layer(meta)
                elif meta.current_device == "ramdisk" and self.ramdisk_manager: self.ramdisk_manager.delete_layer(meta)
                
                if layer_id_val in layer_cache: del layer_cache[layer_id_val]
                if layer_id_val in layer_access_stats: del layer_access_stats[layer_id_val]
                # Locks are associated with layer_id; if layer_id can be reused, lock might need care.
                # For now, assume layer_ids are unique for server lifetime, so removing lock is okay.
                if layer_id_val in layer_locks: del layer_locks[layer_id_val] 
                del layer_metadata_registry[layer_id_val]
        # Reset placement manager counts, as model structure is now gone/invalid
        self.placement_manager.reset_tier_fill_counts_for_new_model()


    def _cfg_path_get(self, *keys_tuple: str, default_val: Any = None) -> Any: # Helper
        data_node = self.app_config
        for key_item in keys_tuple:
            if isinstance(data_node, dict) and key_item in data_node: data_node = data_node[key_item]
            else: return default_val
        return data_node

# [The InferenceService, worker loops, FastAPI app, and __main__ block will follow in the next continuation]
# [Code from the previous responses, including HardwareManager, QuantizationManager,
#  CompressionManager, BaseStorageManager, NVMEManager, RAMDiskManager,
#  MemoryPlacementManager, and ModelManagerService, should be above this line]

# --- InferenceService (Handles text generation requests) ---
class InferenceService:
    def __init__(self, app_cfg: Dict, model_mgr_svc: ModelManagerService,
                 nvme_mgr_inst: Optional[NVMEManager], ramdisk_mgr_inst: Optional[RAMDiskManager],
                 quant_mgr_inst: QuantizationManager): # QuantManager needed for ensuring correct dtype on compute device if not from storage
        self.app_config = app_cfg
        self.model_manager = model_mgr_svc
        self.nvme_manager = nvme_mgr_inst # Used if _ensure_layer... needs to load from NVMe
        self.ramdisk_manager = ramdisk_mgr_inst # Used if _ensure_layer... needs to load from RAMDisk
        self.quant_manager = quant_mgr_inst # For ensuring target precision on compute device
        
        # Global lock for all inference calls to simplify state management for the temp model.
        # For concurrent processing of multiple requests (if model/hardware supports it),
        # this would need to be more granular (e.g., per-request or managed by a request queue).
        self.inference_global_lock = asyncio.Lock() 
                                                 
        self.prefetch_settings = self.app_config.get("optimization", {}).get("prefetching", {})
        logger.info("InferenceService initialized (with refined layer handling and prefetch trigger).")

    async def _ensure_layer_on_device(self, layer_id_val: int, target_compute_dev: torch.device,
                                      current_model_ctx: LoadedModelContext) -> Optional[torch.Tensor]:
        # Ensures a specific layer is loaded onto the target_compute_dev, fetching/moving it if necessary.
        # Returns the tensor on the target device (dequantized to its original_dtype, then potentially re-quantized for compute device), or None if failed.
        layer_meta_obj = current_model_ctx.get_layer_metadata(layer_id_val)
        if not layer_meta_obj:
            logger.error(f"L{layer_id_val} metadata not found during inference prep for model '{current_model_ctx.name}'.")
            return None

        # Trigger prefetch for nearby layers if enabled (non-blocking)
        if self.prefetch_settings.get("enabled", False) and self.prefetch_settings.get("trigger_on_layer_ensure", True):
            try:
                prefetch_job_queue.put_nowait((layer_id_val, current_model_ctx)) 
            except queue.Full:
                logger.warning(f"Prefetch job queue is full. Skipping prefetch trigger for L{layer_id_val}.")

        # 1. Check layer_cache (for layers in CPU or GPU RAM)
        if layer_id_val in layer_cache:
            cached_tensor_val = layer_cache[layer_id_val]
            # The tensor in layer_cache is already quantized for its current tier (CPU or GPU cache).
            # We need to ensure it's on the target_compute_dev.
            
            is_on_correct_target_gpu = cached_tensor_val.device.type == 'cuda' and \
                                       target_compute_dev.type == 'cuda' and \
                                       cached_tensor_val.device.index == target_compute_dev.index
            is_on_correct_target_cpu = cached_tensor_val.device.type == 'cpu' and target_compute_dev.type == 'cpu'

            if is_on_correct_target_gpu or is_on_correct_target_cpu:
                layer_meta_obj.update_access_stats()
                logger.debug(f"L{layer_id_val} ('{layer_meta_obj.name}') found in cache on target device {target_compute_dev}.")
                # Tensor is already on target device and in correct precision for that cache tier.
                return cached_tensor_val
            else: # In cache, but on the wrong device
                logger.info(f"L{layer_id_val} ('{layer_meta_obj.name}') in RAM cache ({cached_tensor_val.device}), needs move to {target_compute_dev} for inference.")
                try:
                    # Synchronous move for layer actively needed in inference path.
                    moved_tensor_val = cached_tensor_val.to(target_compute_dev)
                    # Update layer_cache and metadata ONLY if the move is to a *different cache tier*
                    # (e.g. CPU cache to GPU cache). If just different GPU ID, cache entry is updated.
                    # Here, we always update because it's now on the *target compute device*.
                    layer_cache[layer_id_val] = moved_tensor_val 
                    with layer_meta_obj.get_lock(): # Lock metadata while updating
                        layer_meta_obj.set_location(str(target_compute_dev).split(":")[0], gpu_id_val=target_compute_dev.index if target_compute_dev.type=='cuda' else None)
                        # current_dtype and quantization_type remain as they were for the source cache tier.
                        # If compute tier has different precision needs, that's handled at temp_model reconstruction.
                    layer_meta_obj.update_access_stats()
                    logger.debug(f"L{layer_id_val} moved from {cached_tensor_val.device} to {target_compute_dev} for inference.")
                    return moved_tensor_val
                except Exception as e_move_from_cache:
                    logger.error(f"Failed to move cached L{layer_id_val} from {cached_tensor_val.device} to {target_compute_dev}: {e_move_from_cache}. Attempting full load.", exc_info=True)
                    pass # Fall through to load from disk

        # 2. Not in usable cache (or move failed), load from backing storage (NVMe/RAMDisk)
        logger.info(f"L{layer_id_val} ('{layer_meta_obj.name}') not in usable cache for {target_compute_dev}. Current meta loc: {layer_meta_obj.current_device}. Attempting load.")
        
        # Storage managers (load_layer) return the tensor dequantized to its original_dtype, on CPU.
        loaded_dequantized_tensor_on_cpu: Optional[torch.Tensor] = None
        source_storage_log = layer_meta_obj.current_device

        if layer_meta_obj.current_device == "nvme" and self.nvme_manager and self.nvme_manager.path:
            loaded_dequantized_tensor_on_cpu = self.nvme_manager.load_layer(layer_meta_obj)
        elif layer_meta_obj.current_device == "ramdisk" and self.ramdisk_manager and self.ramdisk_manager.path:
            loaded_dequantized_tensor_on_cpu = self.ramdisk_manager.load_layer(layer_meta_obj)
        elif layer_meta_obj.current_device == "cpu" or layer_meta_obj.current_device.startswith("gpu"):
             logger.error(f"L{layer_id_val} ('{layer_meta_obj.name}') metadata says RAM/VRAM, but not in cache. State inconsistency.")
             return None

        if loaded_dequantized_tensor_on_cpu is not None:
            logger.info(f"L{layer_id_val} loaded from {source_storage_log} to CPU (dequantized to {loaded_dequantized_tensor_on_cpu.dtype}).")
            
            # Now, this tensor (original_dtype, on CPU) needs to be prepared for the target_compute_dev.
            # This might involve re-quantization to a compute-specific precision (e.g. FP16 for GPU compute).
            # Determine target precision for compute device (could be "gpu" or "cpu" as tier types)
            compute_tier_type_for_quant = "gpu" if target_compute_dev.type == "cuda" else "cpu"
            tensor_for_compute, q_type_compute, q_dtype_compute, q_params_compute = \
                self.quant_manager.quantize_tensor(loaded_dequantized_tensor_on_cpu, layer_meta_obj, compute_tier_type_for_quant)
            
            try:
                final_tensor_on_compute_dev = tensor_for_compute.to(target_compute_dev)
                # Add/update layer_cache with this tensor (now on target device and in compute precision)
                layer_cache[layer_id_val] = final_tensor_on_compute_dev 
                with layer_meta_obj.get_lock(): # Update metadata
                    layer_meta_obj.set_location(str(target_compute_dev).split(":")[0], gpu_id_val=target_compute_dev.index if target_compute_dev.type=='cuda' else None)
                    # Update quantization info to reflect the compute tier's precision
                    layer_meta_obj.quantization_type = q_type_compute
                    layer_meta_obj.current_dtype = q_dtype_compute
                    layer_meta_obj.quantization_params = q_params_compute
                    layer_meta_obj.current_size_bytes = final_tensor_on_compute_dev.numel() * final_tensor_on_compute_dev.element_size()
                layer_meta_obj.update_access_stats()
                logger.debug(f"L{layer_id_val} moved to {target_compute_dev} (ComputePrec: {q_dtype_compute}) and cached.")
                return final_tensor_on_compute_dev
            except Exception as e_move_to_compute:
                logger.error(f"Failed to move L{layer_id_val} (loaded from {source_storage_log}) to {target_compute_dev} or apply compute precision: {e_move_to_compute}", exc_info=True)
                # Fallback: if it couldn't be moved to GPU, but is on CPU, and target was GPU, it's an error.
                # If target was CPU and it's on CPU, it might still be usable if original_dtype is fine for compute.
                # Cache the CPU version (in its original_dtype or compute-quantized-for-CPU form)
                layer_cache[layer_id_val] = tensor_for_compute # This is on CPU, possibly quantized for CPU compute
                with layer_meta_obj.get_lock():
                    layer_meta_obj.set_location("cpu")
                    layer_meta_obj.quantization_type = q_type_compute if compute_tier_type_for_quant == "cpu" else None
                    layer_meta_obj.current_dtype = q_dtype_compute if compute_tier_type_for_quant == "cpu" else loaded_dequantized_tensor_on_cpu.dtype
                    layer_meta_obj.quantization_params = q_params_compute if compute_tier_type_for_quant == "cpu" else {}
                    layer_meta_obj.current_size_bytes = tensor_for_compute.numel() * tensor_for_compute.element_size()
                
                if str(target_compute_dev) == "cpu": return tensor_for_compute # Target was CPU, this is success
                return None # Failed to get to non-CPU target
        else:
            logger.error(f"Failed to load L{layer_id_val} ('{layer_meta_obj.name}') from storage ({layer_meta_obj.current_device}).")
            return None

    async def generate(self, generate_request: GenerateTextAPIRequest) -> GenerateTextAPIResponse:
        async with self.inference_global_lock:
            start_time_gen = time.monotonic()
            if not self.model_manager.loaded_model_context or self.model_manager.current_model_status_str != "loaded":
                raise HTTPException(status_code=409, detail="No model loaded or ready for inference.")
            
            model_ctx = self.model_manager.loaded_model_context
            compute_dev = model_ctx.compute_device
            tokenizer = model_ctx.tokenizer

            logger.info(f"Generation for '{model_ctx.name}' on {compute_dev}. Prompt: '{generate_request.prompt[:70]}...'")

            if model_ctx._temp_inference_model_instance is not None: # Clear previous temporary instance
                logger.debug("Clearing previous temporary inference model instance.")
                model_ctx.clear_temp_inference_resources()

            logger.info(f"Preparing temporary model for '{model_ctx.name}' on {compute_dev}...")
            temp_state_dict: Dict[str, torch.Tensor] = {}
            all_layers_ready = True
            for layer_id in model_ctx.layer_execution_order:
                meta = model_ctx.get_layer_metadata(layer_id)
                if not meta: all_layers_ready = False; logger.critical(f"Meta for L{layer_id} missing!"); break
                
                tensor_on_compute = await self._ensure_layer_on_device(layer_id, compute_dev, model_ctx)
                if tensor_on_compute is None:
                    all_layers_ready = False; logger.error(f"Failed to get L{meta.id} ('{meta.name}') on {compute_dev}."); break
                # The tensor from _ensure_layer_on_device is already in the precision suitable for the compute cache tier.
                # This precision should be what the model expects for inference (e.g. fp16 on GPU).
                temp_state_dict[meta.name] = tensor_on_compute
            
            if not all_layers_ready:
                raise HTTPException(status_code=503, detail="Could not prepare all layers on compute device.")

            try:
                trust_rc_flag = model_ctx.hf_config.trust_remote_code
                # Create model shell. device_map="auto" or specific device might be useful here if not all layers are on one device.
                # For this system, all layers needed for _temp_inference_model are moved to model_ctx.compute_device.
                live_model_shell = AutoModelForCausalLM.from_config(model_ctx.hf_config, trust_remote_code=trust_rc_flag)
                live_model_shell.to(compute_dev) # Move shell to device first
                
                # Assign=True is for PyTorch 2.0+, ensures tensors are not copied if already on device and correct dtype.
                # Strict=True is default, helps catch mismatches.
                live_model_shell.load_state_dict(temp_state_dict, assign=True, strict=True) 
                live_model_shell.eval()
                
                model_ctx._temp_inference_model_instance = live_model_shell
                model_ctx._temp_model_active_layers_state = temp_state_dict # Keep refs to tensors
                logger.info(f"Temporary model for '{model_ctx.name}' constructed on {compute_dev}.")
            except Exception as e_model_construct:
                logger.error(f"Failed to build/load state_dict for temporary model '{model_ctx.name}': {e_model_construct}", exc_info=True)
                model_ctx.clear_temp_inference_resources()
                raise HTTPException(status_code=500, detail=f"Internal error constructing model: {str(e_model_construct)}")
            
            try:
                inputs = tokenizer(generate_request.prompt, return_tensors="pt", truncation=True, padding=True).to(compute_dev)
                pad_token_id_gen = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id
                
                with torch.no_grad():
                    outputs = model_ctx._temp_inference_model_instance.generate(
                        inputs.input_ids, attention_mask=inputs.attention_mask,
                        max_new_tokens=generate_request.max_new_tokens, temperature=generate_request.temperature,
                        top_p=generate_request.top_p, top_k=generate_request.top_k,
                        pad_token_id=pad_token_id_gen, eos_token_id=tokenizer.eos_token_id
                        # Add other generation params: do_sample, num_beams, repetition_penalty etc.
                    )
                
                output_ids = outputs[0][inputs.input_ids.shape[1]:]
                generated_text = tokenizer.decode(output_ids, skip_special_tokens=True)
                tokens_generated = len(output_ids)
                
                end_time_gen = time.monotonic()
                duration_s = round(end_time_gen - start_time_gen, 3)
                logger.info(f"Generation for '{model_ctx.name}' successful. Time: {duration_s}s. Tokens: {tokens_generated}.")
                return GenerateTextAPIResponse(
                    prompt=generate_request.prompt, generated_text=generated_text, model_name=model_ctx.name,
                    tokens_generated=tokens_generated, generation_time_seconds=duration_s
                )
            except Exception as e_gen_call:
                logger.error(f"Error during model.generate() for '{model_ctx.name}': {e_gen_call}", exc_info=True)
                model_ctx.clear_temp_inference_resources() # Clean up on error
                raise HTTPException(status_code=500, detail=f"Core inference execution failed: {str(e_gen_call)}")

# --- Worker Thread Loops (Layer Transfer and Prefetching) ---
# (These were defined in the previous response and remain largely the same,
# ensuring they use the global queues `layer_transfer_queue` and `prefetch_job_queue`,
# and interact with the managers like NVMEManager, RAMDiskManager, QuantizationManager,
# and the global `layer_cache` and `layer_metadata_registry`.)

# [layer_transfer_worker_loop and prefetch_worker_loop definitions from the previous
#  response segment (the one that completed the script with FastAPI) would be placed here.
#  Ensure they are consistent with the current class and global variable names.]

# --- FastAPI Application Setup and Endpoints ---
# [The FastAPI app instance creation, @app.on_event handlers for startup/shutdown,
#  all API endpoint definitions (@app.get, @app.post), and the
#  if __name__ == "__main__": block for Uvicorn execution from the previous
#  fully completed script segment would be placed here.]

# For brevity, I'm omitting the re-listing of the worker loops and the full FastAPI app
# as they were quite long and provided in the response where you asked for the full script.
# The key is to integrate them with the managers defined up to this point.

# Placeholder to indicate where the rest of the script would go:
# ... (layer_transfer_worker_loop definition from previous full script) ...
# ... (prefetch_worker_loop definition from previous full script) ...
# ... (FastAPI app = FastAPI(...) and all endpoints from previous full script) ...
# ... (@app.on_event("startup") as on_startup_event_handler from previous full script) ...
# ... (@app.on_event("shutdown") as on_shutdown_event_handler from previous full script) ...
# ... (if __name__ == "__main__": with uvicorn.run(...) from previous full script) ...

logger.critical("CONTINUATION POINT: The InferenceService is now defined. The worker loops (layer_transfer_worker_loop, prefetch_worker_loop) and the full FastAPI application (app instance, startup/shutdown events, all API endpoints, and the __main__ Uvicorn execution block) as previously provided should follow this point to complete the script.")
# [Code from the previous responses, including HardwareManager, QuantizationManager,
#  CompressionManager, BaseStorageManager, NVMEManager, RAMDiskManager,
#  MemoryPlacementManager, ModelManagerService, and InferenceService, should be above this line]

# --- Worker Thread Loops (Layer Transfer and Prefetching) ---
def layer_transfer_worker_loop(): # Manages the layer_transfer_queue
    logger.info("Layer Transfer Worker thread started.")
    while not main_stop_event.is_set():
        req: Optional[TransferRequest] = None 
        try:
            req = layer_transfer_queue.get(timeout=1) 
            req.update_status("processing")
            
            layer_id_to_transfer = req.layer_id
            # Ensure source_device and destination_device are strings (should be from TransferRequest init)
            source_dev_type = str(req.source_device) 
            dest_dev_type = str(req.destination_device)
            
            meta_obj = layer_metadata_registry.get(layer_id_to_transfer)
            if not meta_obj:
                logger.warning(f"Transfer Worker: L{layer_id_to_transfer} metadata not found. Skipping transfer.")
                req.update_status("failed", "Layer metadata not found.")
                layer_transfer_queue.task_done(); continue
            
            logger.info(f"Transfer Worker: Processing L{layer_id_to_transfer} ('{meta_obj.name}') from '{source_dev_type}' to '{dest_dev_type}' (Prio {req.priority})")

            tensor_data: Optional[torch.Tensor] = req.data_to_transfer

            if tensor_data is None: # If not pre-loaded, get from current location
                if layer_id_to_transfer in layer_cache:
                    cached_tensor = layer_cache[layer_id_to_transfer]
                    # Basic check: is the tensor in cache actually on the device specified as source?
                    # More robust: compare str(cached_tensor.device) with source_dev_type, handling "gpu:X"
                    current_cached_dev_str = str(cached_tensor.device)
                    is_source_match = False
                    if source_dev_type == current_cached_dev_str: is_source_match = True
                    elif source_dev_type.startswith("gpu") and current_cached_dev_str.startswith("cuda") and \
                         int(source_dev_type.split(":")[1]) == cached_tensor.device.index:
                        is_source_match = True
                    
                    if is_source_match:
                        tensor_data = cached_tensor
                        logger.debug(f"Transfer Worker: Using cached tensor for L{layer_id_to_transfer} from {source_dev_type}.")
                    else:
                        logger.warning(f"Transfer Worker: L{layer_id_to_transfer} in cache, but device {current_cached_dev_str} mismatch with req source {source_dev_type}. Inconsistency?")
                        # Fall through to attempt disk load if source was disk
                
                if tensor_data is None: # Still no data, try loading from disk
                    if source_dev_type == "nvme" and nvme_mgr and nvme_mgr.path:
                        tensor_data = nvme_mgr.load_layer(meta_obj) # Loads dequantized to CPU
                    elif source_dev_type == "ramdisk" and ramdisk_mgr and ramdisk_mgr.path:
                        tensor_data = ramdisk_mgr.load_layer(meta_obj)
                    else:
                        err_msg = f"Source tensor L{layer_id_to_transfer} not in cache and not loadable from disk source '{source_dev_type}'."
                        logger.error(err_msg); req.update_status("failed", err_msg); layer_transfer_queue.task_done(); continue
            
            if tensor_data is None: # If load from source failed
                err_msg = f"Failed to get tensor data for L{layer_id_to_transfer} from '{source_dev_type}'."
                logger.error(err_msg); req.update_status("failed", err_msg); layer_transfer_queue.task_done(); continue

            # Ensure tensor is on CPU if destination is disk, or if it's a tensor that needs specific quantization before disk.
            # Storage managers expect dequantized CPU tensor for their .store_layer()
            # The tensor_data from load_layer() is already dequantized on CPU.
            # If tensor_data came from cache (already quantized for that cache tier), it needs to be dequantized first before storing to disk.
            tensor_for_storage_on_cpu = tensor_data # Assume it's suitable or will be handled by storage manager
            
            if dest_dev_type in ["nvme", "ramdisk"]:
                # If data came from cache, it's quantized for that cache. Dequant before storing to disk.
                # Storage managers expect original_dtype tensor for their own quant/compr.
                if meta_obj.current_device != "nvme" and meta_obj.current_device != "ramdisk": # i.e. it came from cpu/gpu cache
                     if meta_obj.quantization_type: # If it was quantized in cache
                         logger.debug(f"Transfer Worker: Dequantizing L{layer_id_to_transfer} from cache ({meta_obj.quantization_type}) before storing to {dest_dev_type}.")
                         tensor_for_storage_on_cpu = quant_manager.dequantize_tensor(tensor_data, meta_obj.quantization_type, meta_obj.original_dtype, meta_obj.quantization_params)
                     else: # Was in cache but not quantized (e.g. original FP32)
                         tensor_for_storage_on_cpu = tensor_data
                # Ensure it's on CPU for storage manager
                if tensor_for_storage_on_cpu.device.type != "cpu":
                    tensor_for_storage_on_cpu = tensor_for_storage_on_cpu.cpu()


            transfer_ok = False
            if dest_dev_type == "nvme" and nvme_mgr and nvme_mgr.path:
                transfer_ok = nvme_mgr.store_layer(tensor_for_storage_on_cpu, meta_obj)
            elif dest_dev_type == "ramdisk" and ramdisk_mgr and ramdisk_mgr.path:
                transfer_ok = ramdisk_mgr.store_layer(tensor_for_storage_on_cpu, meta_obj)
            elif dest_dev_type == "cpu":
                # Quantize for CPU cache tier from original_dtype tensor (tensor_data should be original or dequantized)
                tensor_to_cache = tensor_data
                if tensor_to_cache.dtype != meta_obj.original_dtype and meta_obj.quantization_type : # If tensor_data was from a quantized cache source, dequant first
                    tensor_to_cache = quant_manager.dequantize_tensor(tensor_data, meta_obj.quantization_type, meta_obj.original_dtype, meta_obj.quantization_params)

                q_tensor_cpu, q_type, q_dtype, q_params = quant_manager.quantize_tensor(tensor_to_cache, meta_obj, "cpu")
                layer_cache[layer_id_to_transfer] = q_tensor_cpu.cpu()
                with meta_obj.get_lock():
                    meta_obj.set_location("cpu"); meta_obj.quantization_type = q_type; meta_obj.current_dtype = q_dtype
                    meta_obj.quantization_params = q_params; meta_obj.current_size_bytes = q_tensor_cpu.numel() * q_tensor_cpu.element_size()
                transfer_ok = True
            elif dest_dev_type.startswith("gpu:"):
                try:
                    gpu_id = int(dest_dev_type.split(":")[1])
                    if 0 <= gpu_id < hardware_summary["gpu"]["count"]:
                        tensor_to_cache_gpu = tensor_data
                        if tensor_to_cache_gpu.dtype != meta_obj.original_dtype and meta_obj.quantization_type:
                            tensor_to_cache_gpu = quant_manager.dequantize_tensor(tensor_data, meta_obj.quantization_type, meta_obj.original_dtype, meta_obj.quantization_params)

                        q_tensor_gpu, q_type, q_dtype, q_params = quant_manager.quantize_tensor(tensor_to_cache_gpu, meta_obj, "gpu")
                        layer_cache[layer_id_to_transfer] = q_tensor_gpu.to(f"cuda:{gpu_id}")
                        with meta_obj.get_lock():
                             meta_obj.set_location("gpu", gpu_id_val=gpu_id); meta_obj.quantization_type = q_type; meta_obj.current_dtype = q_dtype
                             meta_obj.quantization_params = q_params; meta_obj.current_size_bytes = q_tensor_gpu.numel() * q_tensor_gpu.element_size()
                        transfer_ok = True
                    else: req.update_status("failed", f"Invalid GPU index {gpu_id}.")
                except Exception as e: req.update_status("failed", f"GPU transfer error: {e}")
            else: req.update_status("failed", f"Unknown destination '{dest_dev_type}'.")

            if transfer_ok:
                req.update_status("completed")
                logger.info(f"Transfer Worker: L{layer_id_to_transfer} ('{meta_obj.name}') to '{dest_dev_type}' COMPLETED.")
                meta_obj.update_access_stats()
                # Source cleanup: if moved from RAM/VRAM cache to disk, remove from source cache.
                if source_dev_type != dest_dev_type and (source_dev_type == "cpu" or source_dev_type.startswith("gpu")):
                    if meta_obj.current_device == dest_dev_type and dest_dev_type in ["nvme", "ramdisk"]:
                        if layer_id_to_transfer in layer_cache:
                           # Check if the cached tensor was indeed the one from source_dev_type before deleting
                           # This is tricky if cache could hold same layer_id from different devices.
                           # Simpler: just delete if ID matches, assuming cache is for the "fastest" version.
                           del layer_cache[layer_id_to_transfer]
                           logger.debug(f"Transfer Worker: L{layer_id_to_transfer} removed from source cache '{source_dev_type}' after move to disk '{dest_dev_type}'.")
            else: # req.update_status already called with "failed" by a block above
                 logger.error(f"Transfer Worker: L{layer_id_to_transfer} ('{meta_obj.name}') to '{dest_dev_type}' FAILED. Reason: {req.error_message}")
            
            layer_transfer_queue.task_done()
        except queue.Empty:
            if main_stop_event.is_set(): break
            continue
        except Exception as e_worker:
            logger.error(f"Critical error in Layer Transfer Worker: {e_worker}", exc_info=True)
            if req and req.status == "processing": req.update_status("failed", f"Worker error: {e_worker}")
            try: 
                if req: layer_transfer_queue.task_done()
            except ValueError: pass 
            time.sleep(0.2)
    logger.info("Layer Transfer Worker thread stopped.")


def prefetch_worker_loop():
    logger.info("Prefetch Worker thread started.")
    prefetch_cfg = config.get("optimization", {}).get("prefetching", {})
    enabled = prefetch_cfg.get("enabled", False)
    window_fwd = prefetch_cfg.get("window_forward", 3)
    priority_val = prefetch_cfg.get("min_transfer_priority_for_prefetch", 7)
    target_cache_tier = prefetch_cfg.get("target_cache_tier", "gpu_cache") 

    if not enabled:
        logger.info("Prefetching disabled. Prefetch worker exiting."); return

    logger.info(f"Prefetching: Window={window_fwd}, Target={target_cache_tier}, Prio={priority_val}.")

    while not main_stop_event.is_set():
        job: Optional[Tuple[int, LoadedModelContext]] = None
        try:
            job = prefetch_job_queue.get(timeout=2)
            accessed_id, model_ctx = job
            
            if not model_ctx or not model_ctx.layer_execution_order:
                prefetch_job_queue.task_done(); continue

            current_idx = -1
            try: current_idx = model_ctx.layer_execution_order.index(accessed_id)
            except ValueError: prefetch_job_queue.task_done(); logger.warning(f"Prefetch: L{accessed_id} not in model order."); continue

            for i in range(1, window_fwd + 1):
                prefetch_idx = current_idx + i
                if 0 <= prefetch_idx < len(model_ctx.layer_execution_order):
                    candidate_id = model_ctx.layer_execution_order[prefetch_idx]
                    meta = layer_metadata_registry.get(candidate_id)
                    if not meta: continue

                    target_prefetch_dev_str = ""
                    model_compute_dev_str = str(model_ctx.compute_device)

                    if target_cache_tier == "gpu_cache" and model_compute_dev_str.startswith("cuda"):
                        if not meta.current_device.startswith("gpu") or (meta.gpu_id != model_ctx.compute_device.index):
                             target_prefetch_dev_str = model_compute_dev_str
                    elif target_cache_tier == "cpu_cache":
                        if meta.current_device in ["nvme", "ramdisk"]:
                             target_prefetch_dev_str = "cpu"
                    
                    if target_prefetch_dev_str and meta.current_device != target_prefetch_dev_str:
                        is_slower_src = (
                            (meta.current_device == "nvme" and target_prefetch_dev_str != "nvme") or
                            (meta.current_device == "ramdisk" and target_prefetch_dev_str not in ["nvme", "ramdisk"]) or
                            (meta.current_device == "cpu" and target_prefetch_dev_str.startswith("gpu"))
                        )
                        if is_slower_src:
                            logger.info(f"Prefetch: Queueing L{meta.id} ('{meta.name}') from {meta.current_device} to {target_prefetch_dev_str} (Prio: {priority_val})")
                            transfer_req = TransferRequest(meta.id, meta.current_device, target_prefetch_dev_str, priority_val)
                            layer_transfer_queue.put(transfer_req)
                        else: logger.debug(f"Prefetch: Skip L{meta.id}, src {meta.current_device} not 'slower' than target {target_prefetch_dev_str}.")
                    else: logger.debug(f"Prefetch: No action for L{meta.id} (curr: {meta.current_device}, target_tier: {target_cache_tier}, model_compute: {model_compute_dev_str}).")
            
            prefetch_job_queue.task_done()
        except queue.Empty:
            if main_stop_event.is_set(): break; continue
        except Exception as e_prefetch_loop:
            logger.error(f"Error in Prefetch Worker: {e_prefetch_loop}", exc_info=True)
            if job: 
                try: prefetch_job_queue.task_done()
                except ValueError: pass
            time.sleep(1)
    logger.info("Prefetch Worker thread stopped.")


# --- FastAPI Application Setup and Endpoints ---
app = FastAPI(
    title="Ultra Tensor Server", version="1.3.0", # Version bump
    description="Advanced AI model inference server with dynamic layer management.",
    docs_url="/api/docs", redoc_url="/api/redoc"
)

api_key_header_scheme = APIKeyHeader(name="X-API-Key", auto_error=False)

async def verify_api_key_dependency_runtime(api_key: Optional[str] = Security(api_key_header_scheme)):
    # This function is assigned to api_key_auth_check in startup if key is configured
    # It will be called by FastAPI for endpoints that depend on api_key_auth_check
    configured_key = config.get("api", {}).get("api_key")
    if configured_key: # API key is configured on the server
        if not api_key:
            raise HTTPException(status_code=401, detail="X-API-Key header missing.")
        if not hashlib.sha256(api_key.encode()).hexdigest() == hashlib.sha256(configured_key.encode()).hexdigest(): # Compare hashes
            # Never log the provided API key directly for security
            logger.warning(f"API call rejected: Invalid API Key provided (first 5 chars: '{api_key[:5]}...').")
            raise HTTPException(status_code=403, detail="Invalid API Key.")
        return True # Valid key
    return True # No API key configured on server, allow access

# --- FastAPI Event Handlers (Startup & Shutdown) ---
# on_startup_event_handler and on_shutdown_event_handler
# from the previously provided full script would go here.
# They initialize all managers and start/stop threads.

# --- FastAPI API Endpoints ---
# All @app.get, @app.post endpoints for status, model operations,
# layer management, generation, and LM Studio forwarding (if enabled)
# from the previously provided full script would go here.

# --- Main Execution Block ---
# The if __name__ == "__main__": block with uvicorn.run(...)
# from the previously provided full script would go here.


# For brevity, the full FastAPI app setup and __main__ block are omitted here,
# as they were extensive and provided in a previous response where you requested the full script.
# This segment defines the worker loops.
# The next step would be to append the FastAPI application structure.

logger.critical("CONTINUATION POINT: Worker loops are now defined. The full FastAPI application (app instance, startup/shutdown events, all API endpoints, and the __main__ Uvicorn execution block) as previously provided should follow this point to complete the script.")
# [Code from the previous responses, including all managers, LayerMetadata,
#  TransferRequest, LoadedModelContext, InferenceService, and worker loops,
#  should be above this line]

# --- FastAPI Application Setup and Endpoints ---
app = FastAPI( # Create FastAPI application instance
    title="Ultra Tensor Server",
    version="1.3.0", # Updated version reflecting new features
    description="Advanced AI model inference server with dynamic layer management, multi-tier storage, quantization, compression, and prefetching.",
    docs_url="/api/docs", # URL for Swagger UI
    redoc_url="/api/redoc" # URL for ReDoc documentation
)

# API Key Authentication (Optional, based on config)
# This function will be used as a FastAPI dependency if API key is configured
api_key_header_scheme = APIKeyHeader(name="X-API-Key", auto_error=False) # auto_error=False to handle missing/invalid key manually

async def verify_api_key_dependency_runtime(api_key_provided: Optional[str] = Security(api_key_header_scheme)):
    # This function is assigned to the global `api_key_auth_check` in on_startup if a key is configured.
    # It's called by FastAPI for endpoints that list `Depends(api_key_auth_check)`.
    
    # Access the global `config` dictionary (populated at startup)
    server_configured_api_key = config.get("api", {}).get("api_key")
    
    if server_configured_api_key: # If an API key is set in the server's configuration
        if not api_key_provided: # Key missing from request header
            logger.warning("API call rejected: X-API-Key header is missing, but API key security is enabled on the server.")
            raise HTTPException(status_code=401, detail="X-API-Key header is required for this endpoint.")
        
        # Compare hashes for security (avoids storing/comparing plaintext keys directly if one were to log them by mistake)
        # For simplicity here, direct string comparison is often used, but hashing is better practice if keys are complex.
        # Using a simple direct comparison here as per typical API key patterns.
        if api_key_provided != server_configured_api_key:
            # Log only a non-sensitive part of the provided key if logging is needed for debugging
            logger.warning(f"API call rejected: Invalid API Key provided (starts with: '{api_key_provided[:5]}...').")
            raise HTTPException(status_code=403, detail="Invalid API Key provided.")
        
        logger.debug("API Key verified successfully for protected endpoint access.")
        return True # API key is valid and matches the server's configured key
    
    # If no API key is configured on the server, allow access without a key.
    # This means api_key_auth_check will effectively be a no-op if server_configured_api_key is None/empty.
    return True


@app.on_event("startup")
async def on_startup_event_handler(): # Main server initialization logic, called by Uvicorn
    global config, hardware_mgr, quant_manager, compress_manager, nvme_mgr, ramdisk_mgr
    global mem_placement_mgr, model_manager_service, inference_service, api_key_auth_check # Ensure global assignment

    # Step 1: Load Configuration from file
    config_file_arg = sys.argv[1] if len(sys.argv) > 1 else CONFIG_FILE_PATH
    resolved_config_path = Path(config_file_arg).resolve()
    if not resolved_config_path.is_file():
        # Use logger if available, otherwise print. Logger might not be fully configured yet.
        logging.fatal(f"CRITICAL: Configuration file '{resolved_config_path}' not found. Server cannot start.")
        sys.exit(1) 
    
    logging.info(f"Loading server configuration from: {resolved_config_path}")
    try:
        with open(resolved_config_path, "r") as f_config:
            config = json.load(f_config) # Load JSON config into global `config` dict
    except Exception as e_cfg_load:
        logging.fatal(f"CRITICAL: Failed to load/parse config file '{resolved_config_path}': {e_cfg_load}. Server cannot start.", exc_info=True)
        sys.exit(1)

    # Step 2: Enhance Logging with File Handler (now that config is loaded for log file path)
    log_file_from_cfg = config.get("system",{}).get("log_file", DEFAULT_SERVER_LOG_FILE)
    resolved_log_file_path = Path(log_file_from_cfg).resolve()
    try:
        resolved_log_file_path.parent.mkdir(parents=True, exist_ok=True)
        file_log_handler = logging.FileHandler(resolved_log_file_path, mode='a') 
        # Use the same formatter as the default StreamHandler for consistency
        # Assuming a default StreamHandler is present on the root logger from basicConfig
        log_formatter = logging.getLogger().handlers[0].formatter if logging.getLogger().handlers else logging.Formatter(logging.BASIC_FORMAT)
        file_log_handler.setFormatter(log_formatter)
        logging.getLogger().addHandler(file_log_handler)
        logger.info(f"Server logging additionally to file: {resolved_log_file_path}")
        hardware_summary["log_file_path"] = str(resolved_log_file_path)
    except Exception as e_log_setup:
        logger.error(f"Failed to set up file logging to '{resolved_log_file_path}': {e_log_setup}. Logging to console only.")

    # Step 3: Initialize Core Service Managers
    hardware_mgr = HardwareManager(config)
    hardware_mgr.detect_all_hardware()
    hardware_mgr.start_monitoring_thread()

    quant_manager = QuantizationManager(config)
    compress_manager = CompressionManager(config)

    if hardware_summary["nvme"]["available"] and hardware_summary["nvme"]["path"]:
        nvme_mgr = NVMEManager(config, quant_manager, compress_manager)
    
    if hardware_summary["ramdisk"]["available"] and hardware_summary["ramdisk"]["path"]:
        ramdisk_mgr = RAMDiskManager(config, quant_manager, compress_manager)
    
    mem_placement_mgr = MemoryPlacementManager(config)
    model_manager_service = ModelManagerService(config, mem_placement_mgr, nvme_mgr, ramdisk_mgr, quant_manager, compress_manager)
    inference_service = InferenceService(config, model_manager_service, nvme_mgr, ramdisk_mgr, quant_manager)
    
    # Step 4: Start Worker Threads
    threading.Thread(target=layer_transfer_worker_loop, name="UTS_LayerTransferWorker", daemon=True).start()
    if config.get("optimization", {}).get("prefetching", {}).get("enabled", False):
        threading.Thread(target=prefetch_worker_loop, name="UTS_PrefetchWorker", daemon=True).start()
    else: logger.info("Prefetch worker is configured as disabled.")

    # Step 5: Setup API Key Authentication Dependency
    if config.get("api", {}).get("api_key"):
        api_key_auth_check = verify_api_key_dependency_runtime # Assign the actual checking function
        logger.info("API Key authentication is ENABLED for protected endpoints.")
    else:
        async def open_access_placeholder(): return True # A no-op dependency if no key is set
        api_key_auth_check = open_access_placeholder
        logger.info("API Key authentication is DISABLED (server endpoints are open).")

    # Step 6: Optional Startup Model Load
    startup_model_id_cfg = config.get("model", {}).get("startup_model_name")
    if startup_model_id_cfg:
        logger.info(f"Startup model configured: '{startup_model_id_cfg}'. Loading in background...")
        compute_dev_cfg = config.get("model",{}).get("startup_model_compute_device")
        hf_token_cfg = config.get("huggingface",{}).get("default_token")
        asyncio.create_task(model_manager_service.load_model(startup_model_id_cfg, compute_dev_cfg, hf_token_cfg))
    
    server_host_cfg = config.get('api',{}).get('host','0.0.0.0')
    server_port_cfg = config.get('api',{}).get('port',8000)
    logger.info(f"Ultra Tensor Server (v{app.version}) initialized. Listening on http://{server_host_cfg}:{server_port_cfg}")
    if config.get("api",{}).get("lm_studio_port"):
        logger.info(f"LM Studio request forwarding enabled for target port {config['api']['lm_studio_port']}.")


@app.on_event("shutdown")
async def on_shutdown_event_handler():
    logger.info("Ultra Tensor Server received shutdown signal. Initiating graceful shutdown...")
    main_stop_event.set() 

    if model_manager_service and model_manager_service.loaded_model_context:
        logger.info("Unloading active model during server shutdown...")
        await model_manager_service.unload_model()
    
    if nvme_worker_pool:
        logger.info("Shutting down NVMe worker pool..."); nvme_worker_pool.shutdown(wait=True, cancel_futures=True)
    
    active_uts_threads = [t for t in threading.enumerate() if t is not threading.main_thread() and t.name.startswith("UTS_")]
    if active_uts_threads:
        logger.info(f"Waiting for UTS threads to join: {[t.name for t in active_uts_threads]}")
        for thread in active_uts_threads:
            if thread.is_alive(): thread.join(timeout=10)
            if thread.is_alive(): logger.warning(f"Thread {thread.name} did not stop gracefully.")
    
    if hardware_mgr: hardware_mgr.stop_monitoring_and_perform_cleanup()

    logger.info("Performing final GC and CUDA cache clear..."); gc.collect()
    if torch.cuda.is_available(): 
        try: torch.cuda.empty_cache()
        except Exception as e: logger.warning(f"Error during final CUDA empty_cache: {e}")
    logger.info("Ultra Tensor Server shutdown complete.")

# --- API Endpoints Definitions ---
# Ensure api_key_auth_check is properly used with Depends for protected endpoints
@app.get("/api", summary="Root server info", dependencies=[Depends(lambda: api_key_auth_check())]) # Example of using the dependency
async def api_get_root_info():
    return { "server_name": app.title, "version": app.version, "status": "running",
             "docs": app.docs_url, "time_utc": datetime.datetime.utcnow().isoformat() + "Z" }

@app.get("/api/status", response_model=ServerStatusAPIResponse, summary="Detailed server status", dependencies=[Depends(lambda: api_key_auth_check())])
async def api_get_server_status():
    if not model_manager_service or not hardware_mgr:
        raise HTTPException(status_code=503, detail="Core managers not initialized.")
    return ServerStatusAPIResponse(
        server_status="running" if not main_stop_event.is_set() else "shutting_down",
        uptime_seconds=round(time.monotonic() - server_start_time, 2),
        model_status=model_manager_service.get_current_model_status(),
        hardware_summary=hardware_summary,
        layer_transfer_queue_size=layer_transfer_queue.qsize(),
        prefetch_job_queue_size=prefetch_job_queue.qsize(),
        active_threads=threading.active_count(),
        log_file_path=hardware_summary.get("log_file_path")
    )

@app.post("/api/models/load", response_model=ModelStatusAPIResponse, summary="Load a model", status_code=202, dependencies=[Depends(lambda: api_key_auth_check())])
async def api_load_model(load_req: ModelLoadAPIRequest, bg_tasks: BackgroundTasks):
    if not model_manager_service: raise HTTPException(status_code=503, detail="ModelManagerService not available.")
    current_status = model_manager_service.current_model_status_str
    if current_status == "loading":
        raise HTTPException(status_code=409, detail="A model load is already in progress.")
    if current_status == "loaded" and (not model_manager_service.loaded_model_context or model_manager_service.loaded_model_context.name != load_req.model_name_or_path):
        raise HTTPException(status_code=409, detail=f"Model '{model_manager_service.loaded_model_context.name if model_manager_service.loaded_model_context else 'Unknown'}' is loaded. Unload first.")
    
    bg_tasks.add_task(model_manager_service.load_model, load_req.model_name_or_path, load_req.compute_device, load_req.hf_token)
    return ModelStatusAPIResponse(model_name=load_req.model_name_or_path, status="loading")

@app.post("/api/models/unload", response_model=ModelStatusAPIResponse, summary="Unload current model", dependencies=[Depends(lambda: api_key_auth_check())])
async def api_unload_model(bg_tasks: BackgroundTasks):
    if not model_manager_service: raise HTTPException(status_code=503, detail="ModelManagerService not available.")
    current_status = model_manager_service.current_model_status_str
    if current_status == "no_model_loaded": return model_manager_service.get_current_model_status()
    if current_status in ["loading", "unloading"]:
        raise HTTPException(status_code=409, detail=f"Cannot unload, current status: {current_status}.")
    
    unloading_name = model_manager_service.loaded_model_context.name if model_manager_service.loaded_model_context else "N/A"
    bg_tasks.add_task(model_manager_service.unload_model)
    return ModelStatusAPIResponse(model_name=unloading_name, status="unloading")

@app.get("/api/models/status", response_model=ModelStatusAPIResponse, summary="Get current model status", dependencies=[Depends(lambda: api_key_auth_check())])
async def api_get_model_status():
    if not model_manager_service: raise HTTPException(status_code=503, detail="ModelManagerService not available.")
    return model_manager_service.get_current_model_status()

@app.post("/api/models/generate", response_model=GenerateTextAPIResponse, summary="Generate text", dependencies=[Depends(lambda: api_key_auth_check())])
async def api_generate_text(gen_req: GenerateTextAPIRequest):
    if not inference_service: raise HTTPException(status_code=503, detail="InferenceService not available.")
    if not model_manager_service or model_manager_service.current_model_status_str != "loaded":
        raise HTTPException(status_code=409, detail="No model loaded and ready for inference.")
    try: return await inference_service.generate(gen_req)
    except HTTPException as e: raise e
    except Exception as e: logger.error(f"Generate error: {e}", exc_info=True); raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/layers", response_model=List[LayerInfoAPI], summary="List all managed layers", dependencies=[Depends(lambda: api_key_auth_check())])
async def api_get_all_layers():
    return [meta.to_api_model() for meta in layer_metadata_registry.values()]

@app.get("/api/layers/{layer_id}", response_model=LayerInfoAPI, summary="Get specific layer info", dependencies=[Depends(lambda: api_key_auth_check())])
async def api_get_layer(layer_id: int):
    meta = layer_metadata_registry.get(layer_id)
    if not meta: raise HTTPException(status_code=404, detail=f"Layer ID {layer_id} not found.")
    return meta.to_api_model()

@app.post("/api/layers/transfer", summary="Request layer transfer", status_code=202, dependencies=[Depends(lambda: api_key_auth_check())])
async def api_transfer_layer(req_api: TransferRequestAPI):
    meta = layer_metadata_registry.get(req_api.layer_id)
    if not meta: raise HTTPException(status_code=404, detail=f"Layer ID {req_api.layer_id} not found.")
    
    valid_prefixes = ["cpu", "gpu", "ramdisk", "nvme"]
    if not any(req_api.destination_device.startswith(p) for p in valid_prefixes) or \
       (req_api.destination_device.startswith("gpu") and ":" not in req_api.destination_device):
         raise HTTPException(status_code=400, detail="Invalid destination_device format.")

    transfer_obj = TransferRequest(meta.id, meta.current_device, req_api.destination_device, req_api.priority)
    try: layer_transfer_queue.put_nowait(transfer_obj)
    except queue.Full: raise HTTPException(status_code=503, detail="Layer transfer queue full.")
    return {"message": "Transfer request queued.", "layer_id": meta.id, "destination": req_api.destination_device}

# Optional LM Studio Forwarding Endpoint
# Note: The global 'config' needs to be accessible here. This check should ideally be done
# after config is loaded in startup, and the endpoint added conditionally.
# For simplicity in a single file, we define it, and it will only work if config enables it.
@app.post("/v1/completions", summary="Forward to LM Studio (if configured)", dependencies=[Depends(lambda: api_key_auth_check())], include_in_schema=config.get("api", {}).get("lm_studio_port") is not None)
async def forward_to_lm_studio_endpoint(request: Request):
    lm_studio_cfg = config.get("api", {})
    target_port = lm_studio_cfg.get("lm_studio_port")
    target_host = lm_studio_cfg.get("lm_studio_host", "localhost")
    
  